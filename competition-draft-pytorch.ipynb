{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import (Callback, EarlyStopping,\n",
    "                                         LearningRateMonitor, ModelCheckpoint,\n",
    "                                         ProgressBar)\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, Subset\n",
    "from torchmetrics import Metric\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "class PartialAUROC(Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_tpr: float = 0.80,\n",
    "        dist_sync_on_step: bool = False,\n",
    "    ):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.min_tpr = min_tpr\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        self.preds.append(preds)\n",
    "        self.target.append(target)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds)\n",
    "        target = torch.cat(self.target)\n",
    "        return self._partial_auroc(target, preds, self.min_tpr)\n",
    "\n",
    "    def _partial_auroc(\n",
    "        self, y_true: torch.Tensor, y_score: torch.Tensor, min_tpr: float\n",
    "    ) -> float:\n",
    "        y_true = torch.abs(y_true - 1)\n",
    "        y_score = -y_score\n",
    "\n",
    "        fpr, tpr, _ = self._roc_curve(y_true, y_score)\n",
    "        max_fpr = 1.0 - min_tpr\n",
    "\n",
    "        # print(f\"Computed FPR: {fpr}\")\n",
    "        # print(f\"Computed TPR: {tpr}\")\n",
    "\n",
    "        if max_fpr == 1:\n",
    "            return self._auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "\n",
    "        stop = torch.searchsorted(fpr, torch.tensor(max_fpr), right=True)\n",
    "        x_interp = fpr[stop - 1 : stop + 1]\n",
    "        y_interp = tpr[stop - 1 : stop + 1]\n",
    "\n",
    "        # print(f\"x_interp: {x_interp}\")\n",
    "        # print(f\"y_interp: {y_interp}\")\n",
    "\n",
    "        if len(x_interp) == 1:\n",
    "            interp_tpr = y_interp[0]\n",
    "        else:\n",
    "            interp_tpr = y_interp[0] + (max_fpr - x_interp[0]) * (\n",
    "                y_interp[1] - y_interp[0]\n",
    "            ) / (x_interp[1] - x_interp[0])\n",
    "\n",
    "        tpr = torch.cat([tpr[:stop], torch.tensor([interp_tpr])])\n",
    "        fpr = torch.cat([fpr[:stop], torch.tensor([max_fpr])])\n",
    "\n",
    "        partial_auc = self._auc(fpr, tpr)\n",
    "        return partial_auc\n",
    "\n",
    "    def _roc_curve(self, y_true: torch.Tensor, y_score: torch.Tensor):\n",
    "        desc_score_indices = torch.argsort(y_score, descending=True)\n",
    "        y_score = y_score[desc_score_indices]\n",
    "        y_true = y_true[desc_score_indices]\n",
    "\n",
    "        distinct_value_indices = torch.where(torch.diff(y_score))[0]\n",
    "        threshold_idxs = torch.cat(\n",
    "            [distinct_value_indices, torch.tensor([y_true.numel() - 1])]\n",
    "        )\n",
    "\n",
    "        tps = torch.cumsum(y_true, dim=0)[threshold_idxs]\n",
    "        fps = 1 + threshold_idxs - tps\n",
    "        \n",
    "        # Handle the case where there are no positive samples\n",
    "        if tps[-1] == 0:\n",
    "            tpr = torch.zeros_like(tps)\n",
    "        else:\n",
    "            tpr = tps / tps[-1]\n",
    "        \n",
    "        fpr = fps / fps[-1]\n",
    "        thresholds = y_score[threshold_idxs]\n",
    "\n",
    "        # print(f\"tps: {tps}\")\n",
    "        # print(f\"fps: {fps}\")\n",
    "        # print(f\"tpr: {tpr}\")\n",
    "        # print(f\"fpr: {fpr}\")\n",
    "        # print(f\"thresholds: {thresholds}\")\n",
    "\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def _auc(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        if torch.all(y == 0):\n",
    "            print(\"Warning: All TPR values are zero. AUC is undefined.\")\n",
    "            return 0.0\n",
    "\n",
    "        direction = 1\n",
    "        dx = torch.diff(x)\n",
    "        if torch.any(dx < 0):\n",
    "            if torch.all(dx <= 0):\n",
    "                direction = -1\n",
    "            else:\n",
    "                raise ValueError(\"x is neither increasing nor decreasing\")\n",
    "        auc_value = direction * torch.trapz(y, x).item()\n",
    "        # print(f\"Computed AUC: {auc_value}\")\n",
    "        return auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNActivation(in_channels, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBNActivation(\n",
    "                    hidden_dim, hidden_dim, stride=stride, groups=hidden_dim\n",
    "                ),\n",
    "                nn.Conv2d(hidden_dim, out_channels, 1, bias=True),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNActivation, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseLayer(in_channels + i * growth_rate, growth_rate, dropout_rate)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.cat([x, layer(x)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate):\n",
    "        super(DenseLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, 4 * growth_rate, 1, bias=True),\n",
    "            nn.BatchNorm2d(4 * growth_rate),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(4 * growth_rate, growth_rate, 3, padding=1, bias=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "        super(TransitionLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=True),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        att = nn.Hardswish()(g + x)\n",
    "        att = nn.Sigmoid()(self.bn3(self.conv3(att)))\n",
    "        return x * att\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "        self.branch1 = ConvBNActivation(in_channels, f1, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f2[0], kernel_size=1),\n",
    "            ConvBNActivation(f2[0], f2[1], kernel_size=3),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f3[0], kernel_size=1),\n",
    "            ConvBNActivation(f3[0], f3[1], kernel_size=5),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNActivation(in_channels, f1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides):\n",
    "        super(GatedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=strides,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "        # Add a shortcut connection if input and output dimensions don't match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=strides, bias=True\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        gate = nn.Sigmoid()(self.bn3(self.conv3(x)))\n",
    "        x = x * gate\n",
    "        x += residual\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class GuruNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=(139, 139, 3),\n",
    "        metadata_shape=None,\n",
    "        classes=2,\n",
    "    ):\n",
    "        super(GuruNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.metadata_shape = metadata_shape\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.activation = nn.Hardswish()\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        self.inv_res_blocks = nn.ModuleList()\n",
    "        block_params = [\n",
    "            # expand_ratio, filters, strides, repeats\n",
    "            (6, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 40, 2, 2),\n",
    "            (6, 80, 2, 3),\n",
    "            (6, 112, 1, 3),\n",
    "            (6, 128, 2, 4),\n",
    "            (6, 196, 1, 1),\n",
    "        ]\n",
    "\n",
    "        in_channels = 256\n",
    "        for i, (expand_ratio, filters, strides, repeats) in enumerate(block_params):\n",
    "            for j in range(repeats):\n",
    "                if j > 0:\n",
    "                    strides = 1\n",
    "                self.inv_res_blocks.append(\n",
    "                    InvertedResidualBlock(in_channels, filters, expand_ratio, strides)\n",
    "                )\n",
    "                in_channels = filters\n",
    "\n",
    "        # Dense Block\n",
    "        self.dense_block = DenseBlock(in_channels, num_layers=20, growth_rate=32)\n",
    "        in_channels += 20 * 32  # Update in_channels after dense block\n",
    "\n",
    "        # Transition Layer\n",
    "        self.transition = TransitionLayer(in_channels, compression_factor=0.5)\n",
    "        in_channels = int(in_channels * 0.5)\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Inception Block\n",
    "        self.inception = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        self.inception2 = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention2 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Gated Residual Block\n",
    "        self.gated_res = GatedResidualBlock(in_channels, 512, kernel_size=3, strides=2)\n",
    "        in_channels = 512\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention3 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_channels, 4096)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(2048)\n",
    "        self.fc3 = nn.Linear(2048, 512)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 128)\n",
    "        self.bn_fc4 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.metadata_fc1 = nn.Linear(41, 4096)\n",
    "        self.metadata_bn1 = nn.BatchNorm1d(4096)\n",
    "        self.metadata_fc2 = nn.Linear(4096, 1024)\n",
    "        self.metadata_bn2 = nn.BatchNorm1d(1024)\n",
    "        self.metadata_fc3 = nn.Linear(1024, 512)\n",
    "        self.metadata_bn3 = nn.BatchNorm1d(512)\n",
    "        self.metadata_fc4 = nn.Linear(512, 128)\n",
    "        self.metadata_bn4 = nn.BatchNorm1d(128)\n",
    "        self.final_fc = nn.Linear(128 + 128, classes)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        self.scaler = GradScaler()\n",
    "        self.loss = self.loss = nn.CrossEntropyLoss()\n",
    "        self.auroc = PartialAUROC(min_tpr=0.8)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for block in self.inv_res_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Dense Block\n",
    "        x = self.dense_block(x)\n",
    "\n",
    "        # Transition Layer\n",
    "        x = self.transition(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # Inception Block\n",
    "        x = self.inception(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        # Gated Residual Block\n",
    "        x = self.gated_res(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc4(self.fc4(x)))\n",
    "\n",
    "        metadata = self.activation(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn2(self.metadata_fc2(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn3(self.metadata_fc3(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn4(self.metadata_fc4(metadata)))\n",
    "\n",
    "        x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.final_fc(x)\n",
    "        # Apply sigmoid to ensure output is between 0 and 1\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)  # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.NAdam(\n",
    "            self.parameters(), lr=0.001, momentum_decay=0.5, weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def prepare_df(\n",
    "    df: pd.DataFrame,\n",
    "    is_training=True,\n",
    "):\n",
    "    print(\"Preparing DataFrame...\")\n",
    "    df_hash = hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    cache_dir = \"./cache\"\n",
    "    param_string = f\"{is_training}\"\n",
    "    cache_file = os.path.join(cache_dir, f\"prepared_df_{df_hash}_{param_string}.joblib\")\n",
    "\n",
    "    # Check if cached version exists\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading cached prepared DataFrame...\")\n",
    "        return joblib.load(cache_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    drop_columns_train = [\n",
    "        \"lesion_id\",\n",
    "        \"iddx_full\",\n",
    "        \"iddx_1\",\n",
    "        \"iddx_2\",\n",
    "        \"iddx_3\",\n",
    "        \"iddx_4\",\n",
    "        \"iddx_5\",\n",
    "        \"mel_mitotic_index\",\n",
    "        \"mel_thick_mm\",\n",
    "        \"tbp_lv_dnn_lesion_confidence\",\n",
    "    ]\n",
    "    drop_columns_test = [\"attribution\", \"copyright_license\"]\n",
    "\n",
    "    if is_training:\n",
    "        df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    target_columns = [\"target\"] if is_training else []\n",
    "    X = df.drop(target_columns + [\"isic_id\"], axis=1)\n",
    "    y = torch.tensor(df[\"target\"].values, dtype=torch.int8) if is_training else None\n",
    "\n",
    "    # Separate features by type\n",
    "    integer_features = X.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns\n",
    "    float_features = X.select_dtypes(include=[\"float64\", \"float32\", \"float16\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Handle NaN values and type conversions\n",
    "    for feature in float_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].mean()).astype(\"float32\")\n",
    "\n",
    "    for feature in integer_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median()).astype(\"int32\")\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        X[feature] = X[feature].astype(str).fillna(\"Unknown\")\n",
    "        X[feature] = pd.Categorical(X[feature]).codes\n",
    "\n",
    "    # Standardize all numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "    X_final = X_scaled\n",
    "\n",
    "    # Final check for any remaining NaN values\n",
    "    assert (\n",
    "        not X_final.isnull().any().any()\n",
    "    ), \"There are still NaN values in the processed data\"\n",
    "\n",
    "    print(\"Data shape after preprocessing:\", X_final.shape)\n",
    "    print(\"Number of NaN values after preprocessing:\", X_final.isnull().sum().sum())\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"DataFrame prepared in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Metadata Shape: {X_final.shape}\")\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    joblib.dump((X_final, y, df[\"isic_id\"]), cache_file)\n",
    "    return X_final, y, df[\"isic_id\"]\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, metadata_df, is_training=True, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.metadata_df = metadata_df\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.X, self.y, self.image_names = prepare_df(metadata_df, is_training)\n",
    "        self.metadata_shape = self.X.shape\n",
    "        self.train_transform = get_transforms(is_training=True)\n",
    "        self.test_transform = get_transforms(is_training=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, tuple):\n",
    "            idx, augment = idx\n",
    "        else:\n",
    "            augment = False\n",
    "\n",
    "        isic_id = self.image_names[idx]\n",
    "        metadata = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf:\n",
    "            image_data = hdf[str(isic_id)][()]\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.is_training and augment:\n",
    "            image = self.train_transform(image)\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            target = self.y[idx]\n",
    "            target_long = target.long()\n",
    "            del target\n",
    "            target_one_hot = nn.functional.one_hot(target_long, num_classes=2).float()\n",
    "            return (image, metadata), target_one_hot\n",
    "        else:\n",
    "            return (image, metadata), isic_id\n",
    "\n",
    "\n",
    "# Create separate transforms for training and validation\n",
    "def get_transforms(is_training=True):\n",
    "    # Define augmentation parameters\n",
    "    ROTATION_RANGE = 90\n",
    "    BRIGHTNESS_RANGE = (0.9, 1.1)\n",
    "    CONTRAST_RANGE = (0.9, 1.1)\n",
    "    SATURATION_RANGE = (0.9, 1.1)\n",
    "    HUE_RANGE = (-0.001, 0.001)\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((139, 139), antialias=True),\n",
    "    ]\n",
    "\n",
    "    if is_training:\n",
    "        train_transforms = [\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=(139, 139), scale=(0.9, 1.1), antialias=True\n",
    "            ),\n",
    "            transforms.RandomRotation(\n",
    "                degrees=ROTATION_RANGE,\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=BRIGHTNESS_RANGE,\n",
    "                contrast=CONTRAST_RANGE,\n",
    "                saturation=SATURATION_RANGE,\n",
    "                hue=HUE_RANGE,\n",
    "            ),\n",
    "        ]\n",
    "        return transforms.Compose(train_transforms + base_transforms)\n",
    "    else:\n",
    "        return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_hdf5_path: str,\n",
    "        test_hdf5_path: str,\n",
    "        train_metadata_df: pd.DataFrame,\n",
    "        test_metadata_df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_hdf5_path = train_hdf5_path\n",
    "        self.test_hdf5_path = test_hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_metadata_df = train_metadata_df\n",
    "        self.test_metadata_df = test_metadata_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ISICDataset(\n",
    "            self.train_hdf5_path,\n",
    "            self.train_metadata_df,\n",
    "            True,\n",
    "            transform=get_transforms(is_training=True),\n",
    "        )\n",
    "        self.metadata_shape = full_dataset.metadata_shape\n",
    "        \n",
    "        # Get targets for stratification\n",
    "        targets = self.train_metadata_df[\"target\"].values\n",
    "        balanced_indices = self.balance_dataset(np.arange(len(full_dataset)), targets)\n",
    "        \n",
    "        # Extract actual indices and augmentation flags\n",
    "        balanced_indices, augmentation_flags = zip(*balanced_indices)\n",
    "        balanced_indices = np.array(balanced_indices)\n",
    "        \n",
    "        augmentation_flags = np.array(augmentation_flags)\n",
    "        balanced_targets = targets[balanced_indices]\n",
    "\n",
    "        print(f\"Unique indices: {np.unique(balanced_indices)}\")\n",
    "        print(f\"Unique targets: {np.unique(balanced_targets)}\")\n",
    "        \n",
    "        print(len(balanced_indices))\n",
    "        print(len(balanced_targets))\n",
    "        \n",
    "        unique, counts = np.unique(balanced_targets, return_counts=True)\n",
    "        print(f\"Setup Count: {dict(zip(unique, counts))}\")\n",
    "        # Perform stratified split\n",
    "        train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "            balanced_indices,\n",
    "            balanced_targets,\n",
    "            test_size=0.2,\n",
    "            # stratify=balanced_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        val_indices, test_indices, val_targets, test_targets = train_test_split(\n",
    "            temp_indices,\n",
    "            temp_targets,\n",
    "            test_size=0.5,\n",
    "            # stratify=temp_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Create subset datasets\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            self.train_dataset = Subset(full_dataset, train_indices)\n",
    "            self.val_dataset = Subset(full_dataset, val_indices)\n",
    "            self.test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "        # Check for class balance\n",
    "        self._check_class_balance(train_targets.flatten(), \"Train\")\n",
    "        self._check_class_balance(val_targets.flatten(), \"Validation\")\n",
    "        self._check_class_balance(test_targets.flatten(), \"Test\")\n",
    "\n",
    "        print(f\"Length of full_dataset: {len(full_dataset)}\")\n",
    "        print(\n",
    "            f\"Length of train_indices: {len(train_indices)}, max index: {max(train_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of val_indices: {len(val_indices)}, max index: {max(val_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of test_indices: {len(test_indices)}, max index: {max(test_indices)}\"\n",
    "        )\n",
    "\n",
    "    def balance_dataset(self, indices, targets):\n",
    "        np.random.seed(42)\n",
    "        positive_indices = indices[targets == 1]\n",
    "        negative_indices = indices[targets == 0]\n",
    "\n",
    "        num_positive_samples = len(positive_indices)\n",
    "        num_negative_samples = len(negative_indices)\n",
    "\n",
    "        print(f\"Number of Positive Samples: {num_positive_samples}\")\n",
    "        print(f\"Number of Negative Samples: {num_negative_samples}\")\n",
    "\n",
    "        # Determine the number of samples for each class\n",
    "        num_samples = int(np.mean([num_positive_samples, num_negative_samples]))//10\n",
    "\n",
    "        # Upsample positive indices\n",
    "        upsampled_positive_indices = np.random.choice(\n",
    "            positive_indices, size=num_samples // 2, replace=True\n",
    "        )\n",
    "\n",
    "        # Downsample negative indices\n",
    "        downsampled_negative_indices = np.random.choice(\n",
    "            negative_indices, size=num_samples // 2, replace=False\n",
    "        )\n",
    "\n",
    "        # Add augmentation flag\n",
    "        balanced_indices = [(idx, True) for idx in upsampled_positive_indices] + [\n",
    "            (idx, False) for idx in downsampled_negative_indices\n",
    "        ]\n",
    "\n",
    "        np.random.shuffle(balanced_indices)\n",
    "\n",
    "        print(f\"Length of Balanced Positive Indices: {len(upsampled_positive_indices)}\")\n",
    "        print(\n",
    "            f\"Length of Balanced Negative Indices: {len(downsampled_negative_indices)}\"\n",
    "        )\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def _check_class_balance(self, targets, split_name):\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(\n",
    "            f\"{split_name} class distribution: {class_counts / len(targets)}, {len(targets)}\"\n",
    "        )\n",
    "        if len(class_counts) < 2 or min(class_counts) == 0:\n",
    "            raise ValueError(f\"Imbalanced classes in {split_name} split\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in train_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in val_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in test_loader: {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150614/2539756025.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 139, 139\n",
    "\n",
    "# Load metadata\n",
    "train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(\"test-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached prepared DataFrame...\n",
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 10026\n",
      "Length of Balanced Negative Indices: 10026\n",
      "Unique indices: [    81    237    261 ... 400922 400982 400984]\n",
      "Unique targets: [0 1]\n",
      "20052\n",
      "20052\n",
      "Setup Count: {0: 10026, 1: 10026}\n",
      "Train class distribution: [0.50090393 0.49909607], 16041\n",
      "Validation class distribution: [0.50573566 0.49426434], 2005\n",
      "Test class distribution: [0.48703888 0.51296112], 2006\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 16041, max index: 400982\n",
      "Length of val_indices: 2005, max index: 400922\n",
      "Length of test_indices: 2006, max index: 400984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name             | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0  | conv1            | Conv2d             | 19.5 K\n",
      "1  | bn1              | BatchNorm2d        | 512   \n",
      "2  | activation       | Hardswish          | 0     \n",
      "3  | inv_res_blocks   | ModuleList         | 2.2 M \n",
      "4  | dense_block      | DenseBlock         | 2.0 M \n",
      "5  | transition       | TransitionLayer    | 351 K \n",
      "6  | attention        | AttentionBlock     | 215 K \n",
      "7  | avg_pool         | AvgPool2d          | 0     \n",
      "8  | inception        | InceptionBlock     | 406 K \n",
      "9  | inception2       | InceptionBlock     | 526 K \n",
      "10 | attention2       | AttentionBlock     | 280 K \n",
      "11 | gated_res        | GatedResidualBlock | 1.8 M \n",
      "12 | attention3       | AttentionBlock     | 263 K \n",
      "13 | global_avg_pool  | AdaptiveAvgPool2d  | 0     \n",
      "14 | flatten          | Flatten            | 0     \n",
      "15 | fc1              | Linear             | 1.1 M \n",
      "16 | bn_fc1           | BatchNorm1d        | 8.2 K \n",
      "17 | fc2              | Linear             | 8.4 M \n",
      "18 | bn_fc2           | BatchNorm1d        | 4.1 K \n",
      "19 | fc3              | Linear             | 1.0 M \n",
      "20 | bn_fc3           | BatchNorm1d        | 1.0 K \n",
      "21 | fc4              | Linear             | 65.7 K\n",
      "22 | bn_fc4           | BatchNorm1d        | 256   \n",
      "23 | dropout          | Dropout            | 0     \n",
      "24 | metadata_fc1     | Linear             | 172 K \n",
      "25 | metadata_bn1     | BatchNorm1d        | 8.2 K \n",
      "26 | metadata_fc2     | Linear             | 4.2 M \n",
      "27 | metadata_bn2     | BatchNorm1d        | 2.0 K \n",
      "28 | metadata_fc3     | Linear             | 524 K \n",
      "29 | metadata_bn3     | BatchNorm1d        | 1.0 K \n",
      "30 | metadata_fc4     | Linear             | 65.7 K\n",
      "31 | metadata_bn4     | BatchNorm1d        | 256   \n",
      "32 | final_fc         | Linear             | 514   \n",
      "33 | final_activation | Sigmoid            | 0     \n",
      "34 | loss             | CrossEntropyLoss   | 0     \n",
      "35 | auroc            | PartialAUROC       | 0     \n",
      "---------------------------------------------------------\n",
      "23.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 M    Total params\n",
      "94.654    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Number of batches in val_loader: 56\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Warning: All TPR values are zero. AUC is undefined.\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 446\n",
      "Epoch 0: 100%|██████████| 502/502 [02:13<00:00,  3.78it/s, loss=0.445, v_num=107, train_loss_step=0.379, train_pAUC_step=0.178, val_loss=0.420, val_pAUC=0.166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 111: val_loss reached 0.42038 (best 0.42038), saving model to \"checkpoints/version_107/gurunet-epoch=00-val_loss=0.42.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 502/502 [02:08<00:00,  3.92it/s, loss=0.416, v_num=107, train_loss_step=0.442, train_pAUC_step=0.164, val_loss=0.403, val_pAUC=0.171, train_loss_epoch=0.468, train_pAUC_epoch=0.153]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 223: val_loss reached 0.40297 (best 0.40297), saving model to \"checkpoints/version_107/gurunet-epoch=01-val_loss=0.40.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 502/502 [02:03<00:00,  4.07it/s, loss=0.401, v_num=107, train_loss_step=0.474, train_pAUC_step=0.175, val_loss=0.398, val_pAUC=0.173, train_loss_epoch=0.424, train_pAUC_epoch=0.169] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 335: val_loss reached 0.39825 (best 0.39825), saving model to \"checkpoints/version_107/gurunet-epoch=02-val_loss=0.40.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 502/502 [02:04<00:00,  4.03it/s, loss=0.418, v_num=107, train_loss_step=0.429, train_pAUC_step=0.200, val_loss=0.388, val_pAUC=0.175, train_loss_epoch=0.413, train_pAUC_epoch=0.173] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 447: val_loss reached 0.38751 (best 0.38751), saving model to \"checkpoints/version_107/gurunet-epoch=03-val_loss=0.39.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 502/502 [02:05<00:00,  4.02it/s, loss=0.397, v_num=107, train_loss_step=0.424, train_pAUC_step=0.200, val_loss=0.377, val_pAUC=0.180, train_loss_epoch=0.408, train_pAUC_epoch=0.174] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 559: val_loss reached 0.37711 (best 0.37711), saving model to \"checkpoints/version_107/gurunet-epoch=04-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 502/502 [02:04<00:00,  4.05it/s, loss=0.405, v_num=107, train_loss_step=0.460, train_pAUC_step=0.171, val_loss=0.379, val_pAUC=0.179, train_loss_epoch=0.402, train_pAUC_epoch=0.176] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 671: val_loss reached 0.37908 (best 0.37711), saving model to \"checkpoints/version_107/gurunet-epoch=05-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 502/502 [02:05<00:00,  4.00it/s, loss=0.409, v_num=107, train_loss_step=0.487, train_pAUC_step=0.144, val_loss=0.377, val_pAUC=0.181, train_loss_epoch=0.399, train_pAUC_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 783: val_loss reached 0.37662 (best 0.37662), saving model to \"checkpoints/version_107/gurunet-epoch=06-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 502/502 [02:04<00:00,  4.04it/s, loss=0.399, v_num=107, train_loss_step=0.381, train_pAUC_step=0.200, val_loss=0.376, val_pAUC=0.181, train_loss_epoch=0.399, train_pAUC_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 895: val_loss reached 0.37573 (best 0.37573), saving model to \"checkpoints/version_107/gurunet-epoch=07-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 502/502 [02:04<00:00,  4.03it/s, loss=0.395, v_num=107, train_loss_step=0.487, train_pAUC_step=0.188, val_loss=0.377, val_pAUC=0.179, train_loss_epoch=0.395, train_pAUC_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1007: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 502/502 [02:05<00:00,  4.00it/s, loss=0.37, v_num=107, train_loss_step=0.374, train_pAUC_step=0.200, val_loss=0.375, val_pAUC=0.180, train_loss_epoch=0.394, train_pAUC_epoch=0.179]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1119: val_loss reached 0.37471 (best 0.37471), saving model to \"checkpoints/version_107/gurunet-epoch=09-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 502/502 [02:05<00:00,  4.00it/s, loss=0.383, v_num=107, train_loss_step=0.315, train_pAUC_step=0.200, val_loss=0.370, val_pAUC=0.183, train_loss_epoch=0.393, train_pAUC_epoch=0.179] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1231: val_loss reached 0.37010 (best 0.37010), saving model to \"checkpoints/version_107/gurunet-epoch=10-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 502/502 [02:04<00:00,  4.03it/s, loss=0.384, v_num=107, train_loss_step=0.344, train_pAUC_step=0.200, val_loss=0.378, val_pAUC=0.180, train_loss_epoch=0.391, train_pAUC_epoch=0.179] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1343: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 502/502 [01:56<00:00,  4.31it/s, loss=0.377, v_num=107, train_loss_step=0.322, train_pAUC_step=0.200, val_loss=0.371, val_pAUC=0.182, train_loss_epoch=0.391, train_pAUC_epoch=0.179] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 1455: val_loss reached 0.37086 (best 0.37010), saving model to \"checkpoints/version_107/gurunet-epoch=12-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 502/502 [01:53<00:00,  4.45it/s, loss=0.388, v_num=107, train_loss_step=0.361, train_pAUC_step=0.200, val_loss=0.376, val_pAUC=0.179, train_loss_epoch=0.383, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 1567: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 502/502 [01:53<00:00,  4.41it/s, loss=0.397, v_num=107, train_loss_step=0.403, train_pAUC_step=0.182, val_loss=0.385, val_pAUC=0.165, train_loss_epoch=0.384, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 1679: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 502/502 [01:53<00:00,  4.45it/s, loss=0.395, v_num=107, train_loss_step=0.474, train_pAUC_step=0.150, val_loss=0.369, val_pAUC=0.180, train_loss_epoch=0.383, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 1791: val_loss reached 0.36874 (best 0.36874), saving model to \"checkpoints/version_107/gurunet-epoch=15-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 16: 100%|██████████| 502/502 [01:53<00:00,  4.44it/s, loss=0.373, v_num=107, train_loss_step=0.407, train_pAUC_step=0.191, val_loss=0.358, val_pAUC=0.184, train_loss_epoch=0.384, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 1903: val_loss reached 0.35824 (best 0.35824), saving model to \"checkpoints/version_107/gurunet-epoch=16-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.362, v_num=107, train_loss_step=0.313, train_pAUC_step=0.200, val_loss=0.360, val_pAUC=0.184, train_loss_epoch=0.376, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 2015: val_loss reached 0.35966 (best 0.35824), saving model to \"checkpoints/version_107/gurunet-epoch=17-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.366, v_num=107, train_loss_step=0.346, train_pAUC_step=0.200, val_loss=0.359, val_pAUC=0.184, train_loss_epoch=0.373, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 2127: val_loss reached 0.35893 (best 0.35824), saving model to \"checkpoints/version_107/gurunet-epoch=18-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 502/502 [01:53<00:00,  4.43it/s, loss=0.362, v_num=107, train_loss_step=0.332, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.186, train_loss_epoch=0.375, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 2239: val_loss reached 0.35628 (best 0.35628), saving model to \"checkpoints/version_107/gurunet-epoch=19-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 502/502 [01:53<00:00,  4.42it/s, loss=0.363, v_num=107, train_loss_step=0.389, train_pAUC_step=0.175, val_loss=0.358, val_pAUC=0.185, train_loss_epoch=0.374, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 2351: val_loss reached 0.35785 (best 0.35628), saving model to \"checkpoints/version_107/gurunet-epoch=20-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.372, v_num=107, train_loss_step=0.315, train_pAUC_step=0.200, val_loss=0.355, val_pAUC=0.185, train_loss_epoch=0.371, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 2463: val_loss reached 0.35515 (best 0.35515), saving model to \"checkpoints/version_107/gurunet-epoch=21-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.364, v_num=107, train_loss_step=0.313, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.186, train_loss_epoch=0.372, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 2575: val_loss reached 0.35561 (best 0.35515), saving model to \"checkpoints/version_107/gurunet-epoch=22-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.388, v_num=107, train_loss_step=0.501, train_pAUC_step=0.151, val_loss=0.350, val_pAUC=0.187, train_loss_epoch=0.372, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 2687: val_loss reached 0.35046 (best 0.35046), saving model to \"checkpoints/version_107/gurunet-epoch=23-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 24: 100%|██████████| 502/502 [01:54<00:00,  4.39it/s, loss=0.354, v_num=107, train_loss_step=0.379, train_pAUC_step=0.191, val_loss=0.354, val_pAUC=0.186, train_loss_epoch=0.371, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 2799: val_loss reached 0.35388 (best 0.35046), saving model to \"checkpoints/version_107/gurunet-epoch=24-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 502/502 [01:53<00:00,  4.42it/s, loss=0.366, v_num=107, train_loss_step=0.339, train_pAUC_step=0.200, val_loss=0.357, val_pAUC=0.186, train_loss_epoch=0.369, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 2911: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.374, v_num=107, train_loss_step=0.361, train_pAUC_step=0.191, val_loss=0.352, val_pAUC=0.187, train_loss_epoch=0.368, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 3023: val_loss reached 0.35189 (best 0.35046), saving model to \"checkpoints/version_107/gurunet-epoch=26-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 502/502 [01:54<00:00,  4.40it/s, loss=0.36, v_num=107, train_loss_step=0.329, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.186, train_loss_epoch=0.370, train_pAUC_epoch=0.185]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 3135: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 502/502 [01:55<00:00,  4.37it/s, loss=0.356, v_num=107, train_loss_step=0.378, train_pAUC_step=0.191, val_loss=0.357, val_pAUC=0.186, train_loss_epoch=0.370, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 3247: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 29: 100%|██████████| 502/502 [01:54<00:00,  4.39it/s, loss=0.363, v_num=107, train_loss_step=0.415, train_pAUC_step=0.175, val_loss=0.353, val_pAUC=0.186, train_loss_epoch=0.369, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 3359: val_loss reached 0.35257 (best 0.35046), saving model to \"checkpoints/version_107/gurunet-epoch=29-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 502/502 [01:54<00:00,  4.38it/s, loss=0.371, v_num=107, train_loss_step=0.409, train_pAUC_step=0.133, val_loss=0.355, val_pAUC=0.186, train_loss_epoch=0.369, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 3471: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 502/502 [01:54<00:00,  4.39it/s, loss=0.377, v_num=107, train_loss_step=0.354, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.186, train_loss_epoch=0.369, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 3583: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 502/502 [01:54<00:00,  4.38it/s, loss=0.378, v_num=107, train_loss_step=0.355, train_pAUC_step=0.200, val_loss=0.357, val_pAUC=0.186, train_loss_epoch=0.368, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 3695: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 502/502 [01:55<00:00,  4.37it/s, loss=0.362, v_num=107, train_loss_step=0.339, train_pAUC_step=0.200, val_loss=0.354, val_pAUC=0.187, train_loss_epoch=0.367, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 3807: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 502/502 [01:55<00:00,  4.37it/s, loss=0.362, v_num=107, train_loss_step=0.339, train_pAUC_step=0.200, val_loss=0.354, val_pAUC=0.187, train_loss_epoch=0.367, train_pAUC_epoch=0.185]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "torch.cuda.memory.empty_cache()\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"gurunet_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/version_{logger.version}\",\n",
    "    filename=\"gurunet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=5,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize your data module\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached prepared DataFrame...\n",
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 10026\n",
      "Length of Balanced Negative Indices: 10026\n",
      "Unique indices: [    81    237    261 ... 400922 400982 400984]\n",
      "Unique targets: [0 1]\n",
      "20052\n",
      "20052\n",
      "Setup Count: {0: 10026, 1: 10026}\n",
      "Train class distribution: [0.50090393 0.49909607], 16041\n",
      "Validation class distribution: [0.50573566 0.49426434], 2005\n",
      "Test class distribution: [0.48703888 0.51296112], 2006\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 16041, max index: 400982\n",
      "Length of val_indices: 2005, max index: 400922\n",
      "Length of test_indices: 2006, max index: 400984\n",
      "Number of batches in test_loader: 56\n",
      "Testing: 0it [00:00, ?it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:   2%|▏         | 1/56 [00:01<00:59,  1.08s/it]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  12%|█▎        | 7/56 [00:01<00:08,  6.01it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  23%|██▎       | 13/56 [00:02<00:04,  8.87it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  30%|███       | 17/56 [00:02<00:04,  9.75it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  38%|███▊      | 21/56 [00:02<00:03,  9.84it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  48%|████▊     | 27/56 [00:03<00:02,  9.89it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  52%|█████▏    | 29/56 [00:03<00:02, 10.03it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  55%|█████▌    | 31/56 [00:03<00:02, 10.02it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  59%|█████▉    | 33/56 [00:04<00:02, 10.09it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  62%|██████▎   | 35/56 [00:04<00:02, 10.11it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  66%|██████▌   | 37/56 [00:04<00:01, 10.14it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  77%|███████▋  | 43/56 [00:05<00:01, 10.26it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  84%|████████▍ | 47/56 [00:05<00:00, 10.17it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  88%|████████▊ | 49/56 [00:05<00:00, 10.07it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  91%|█████████ | 51/56 [00:05<00:00, 10.25it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Testing:  98%|█████████▊| 55/56 [00:06<00:00, 10.35it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.6932480335235596,\n",
      " 'test_loss_epoch': 0.6932480335235596,\n",
      " 'test_pAUC': 0.00917116366326809,\n",
      " 'test_pAUC_epoch': 0.00917116366326809}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 56/56 [00:06<00:00,  8.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6932480335235596,\n",
       "  'test_loss_epoch': 0.6932480335235596,\n",
       "  'test_pAUC': 0.00917116366326809,\n",
       "  'test_pAUC_epoch': 0.00917116366326809}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=32,\n",
    "    deterministic=True,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "trainer.test(\n",
    "    model=model,\n",
    "    ckpt_path=\"/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_101/gurunet-epoch=27-val_loss=0.34.ckpt\",\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeoned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
