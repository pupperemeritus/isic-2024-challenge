{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "from torch.utils.data import Sampler\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import time\n",
    "from pytorch_lightning.callbacks import (\n",
    "    Callback,\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ProgressBar\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision.transforms import transforms\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "class PartialAUROC(Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_tpr: float = 0.80,\n",
    "        dist_sync_on_step: bool = False,\n",
    "    ):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.min_tpr = min_tpr\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        self.preds.append(preds)\n",
    "        self.target.append(target)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds)\n",
    "        target = torch.cat(self.target)\n",
    "        return self._partial_auroc(target, preds, self.min_tpr)\n",
    "\n",
    "    def _partial_auroc(\n",
    "        self, y_true: torch.Tensor, y_score: torch.Tensor, min_tpr: float\n",
    "    ) -> float:\n",
    "        y_true = torch.abs(y_true - 1)\n",
    "        y_score = -y_score\n",
    "\n",
    "        fpr, tpr, _ = self._roc_curve(y_true, y_score)\n",
    "        max_fpr = 1.0 - min_tpr\n",
    "\n",
    "        # print(f\"Computed FPR: {fpr}\")\n",
    "        # print(f\"Computed TPR: {tpr}\")\n",
    "\n",
    "        if max_fpr == 1:\n",
    "            return self._auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "\n",
    "        stop = torch.searchsorted(fpr, torch.tensor(max_fpr), right=True)\n",
    "        x_interp = fpr[stop - 1 : stop + 1]\n",
    "        y_interp = tpr[stop - 1 : stop + 1]\n",
    "\n",
    "        # print(f\"x_interp: {x_interp}\")\n",
    "        # print(f\"y_interp: {y_interp}\")\n",
    "\n",
    "        if len(x_interp) == 1:\n",
    "            interp_tpr = y_interp[0]\n",
    "        else:\n",
    "            interp_tpr = y_interp[0] + (max_fpr - x_interp[0]) * (\n",
    "                y_interp[1] - y_interp[0]\n",
    "            ) / (x_interp[1] - x_interp[0])\n",
    "\n",
    "        tpr = torch.cat([tpr[:stop], torch.tensor([interp_tpr])])\n",
    "        fpr = torch.cat([fpr[:stop], torch.tensor([max_fpr])])\n",
    "\n",
    "        partial_auc = self._auc(fpr, tpr)\n",
    "        return partial_auc\n",
    "\n",
    "    def _roc_curve(self, y_true: torch.Tensor, y_score: torch.Tensor):\n",
    "        desc_score_indices = torch.argsort(y_score, descending=True)\n",
    "        y_score = y_score[desc_score_indices]\n",
    "        y_true = y_true[desc_score_indices]\n",
    "\n",
    "        distinct_value_indices = torch.where(torch.diff(y_score))[0]\n",
    "        threshold_idxs = torch.cat(\n",
    "            [distinct_value_indices, torch.tensor([y_true.numel() - 1])]\n",
    "        )\n",
    "\n",
    "        tps = torch.cumsum(y_true, dim=0)[threshold_idxs]\n",
    "        fps = 1 + threshold_idxs - tps\n",
    "        \n",
    "        # Handle the case where there are no positive samples\n",
    "        if tps[-1] == 0:\n",
    "            tpr = torch.zeros_like(tps)\n",
    "        else:\n",
    "            tpr = tps / tps[-1]\n",
    "        \n",
    "        fpr = fps / fps[-1]\n",
    "        thresholds = y_score[threshold_idxs]\n",
    "\n",
    "        # print(f\"tps: {tps}\")\n",
    "        # print(f\"fps: {fps}\")\n",
    "        # print(f\"tpr: {tpr}\")\n",
    "        # print(f\"fpr: {fpr}\")\n",
    "        # print(f\"thresholds: {thresholds}\")\n",
    "\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def _auc(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        if torch.all(y == 0):\n",
    "            print(\"Warning: All TPR values are zero. AUC is undefined.\")\n",
    "            return 0.0\n",
    "\n",
    "        direction = 1\n",
    "        dx = torch.diff(x)\n",
    "        if torch.any(dx < 0):\n",
    "            if torch.all(dx <= 0):\n",
    "                direction = -1\n",
    "            else:\n",
    "                raise ValueError(\"x is neither increasing nor decreasing\")\n",
    "        auc_value = direction * torch.trapz(y, x).item()\n",
    "        # print(f\"Computed AUC: {auc_value}\")\n",
    "        return auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNActivation(in_channels, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBNActivation(\n",
    "                    hidden_dim, hidden_dim, stride=stride, groups=hidden_dim\n",
    "                ),\n",
    "                nn.Conv2d(hidden_dim, out_channels, 1, bias=True),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNActivation, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseLayer(in_channels + i * growth_rate, growth_rate, dropout_rate)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.cat([x, layer(x)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate):\n",
    "        super(DenseLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, 4 * growth_rate, 1, bias=True),\n",
    "            nn.BatchNorm2d(4 * growth_rate),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(4 * growth_rate, growth_rate, 3, padding=1, bias=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "        super(TransitionLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=True),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        att = nn.Hardswish()(g + x)\n",
    "        att = nn.Sigmoid()(self.bn3(self.conv3(att)))\n",
    "        return x * att\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "        self.branch1 = ConvBNActivation(in_channels, f1, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f2[0], kernel_size=1),\n",
    "            ConvBNActivation(f2[0], f2[1], kernel_size=3),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f3[0], kernel_size=1),\n",
    "            ConvBNActivation(f3[0], f3[1], kernel_size=5),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNActivation(in_channels, f1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides):\n",
    "        super(GatedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=strides,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "        # Add a shortcut connection if input and output dimensions don't match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=strides, bias=True\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        gate = nn.Sigmoid()(self.bn3(self.conv3(x)))\n",
    "        x = x * gate\n",
    "        x += residual\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class GuruNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=(139, 139, 3),\n",
    "        metadata_shape=None,\n",
    "        classes=2,\n",
    "    ):\n",
    "        super(GuruNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.metadata_shape = metadata_shape\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.activation = nn.Hardswish()\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        self.inv_res_blocks = nn.ModuleList()\n",
    "        block_params = [\n",
    "            # expand_ratio, filters, strides, repeats\n",
    "            (6, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 40, 2, 2),\n",
    "            (6, 80, 2, 3),\n",
    "            (6, 112, 1, 3),\n",
    "            (6, 128, 2, 4),\n",
    "            (6, 196, 1, 1),\n",
    "        ]\n",
    "\n",
    "        in_channels = 256\n",
    "        for i, (expand_ratio, filters, strides, repeats) in enumerate(block_params):\n",
    "            for j in range(repeats):\n",
    "                if j > 0:\n",
    "                    strides = 1\n",
    "                self.inv_res_blocks.append(\n",
    "                    InvertedResidualBlock(in_channels, filters, expand_ratio, strides)\n",
    "                )\n",
    "                in_channels = filters\n",
    "\n",
    "        # Dense Block\n",
    "        self.dense_block = DenseBlock(in_channels, num_layers=20, growth_rate=32)\n",
    "        in_channels += 20 * 32  # Update in_channels after dense block\n",
    "\n",
    "        # Transition Layer\n",
    "        self.transition = TransitionLayer(in_channels, compression_factor=0.5)\n",
    "        in_channels = int(in_channels * 0.5)\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Inception Block\n",
    "        self.inception = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        self.inception2 = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention2 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Gated Residual Block\n",
    "        self.gated_res = GatedResidualBlock(in_channels, 512, kernel_size=3, strides=2)\n",
    "        in_channels = 512\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention3 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_channels, 4096)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(2048)\n",
    "        self.fc3 = nn.Linear(2048, 512)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 128)\n",
    "        self.bn_fc4 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.metadata_fc1 = nn.Linear(41, 4096)\n",
    "        self.metadata_bn1 = nn.BatchNorm1d(4096)\n",
    "        self.metadata_fc2 = nn.Linear(4096, 1024)\n",
    "        self.metadata_bn2 = nn.BatchNorm1d(1024)\n",
    "        self.metadata_fc3 = nn.Linear(1024, 512)\n",
    "        self.metadata_bn3 = nn.BatchNorm1d(512)\n",
    "        self.metadata_fc4 = nn.Linear(512, 128)\n",
    "        self.metadata_bn4 = nn.BatchNorm1d(128)\n",
    "        self.final_fc = nn.Linear(128 + 128, classes)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        self.scaler = GradScaler()\n",
    "        self.loss = self.loss = nn.CrossEntropyLoss()\n",
    "        self.auroc = PartialAUROC(min_tpr=0.8)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for block in self.inv_res_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Dense Block\n",
    "        x = self.dense_block(x)\n",
    "\n",
    "        # Transition Layer\n",
    "        x = self.transition(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # Inception Block\n",
    "        x = self.inception(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        # Gated Residual Block\n",
    "        x = self.gated_res(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc4(self.fc4(x)))\n",
    "\n",
    "        metadata = self.activation(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn2(self.metadata_fc2(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn3(self.metadata_fc3(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn4(self.metadata_fc4(metadata)))\n",
    "\n",
    "        x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.final_fc(x)\n",
    "        # Apply sigmoid to ensure output is between 0 and 1\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)  # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.NAdam(\n",
    "            self.parameters(), lr=0.001, momentum_decay=0.5, weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def prepare_df(\n",
    "    df: pd.DataFrame,\n",
    "    is_training=True,\n",
    "):\n",
    "    print(\"Preparing DataFrame...\")\n",
    "    df_hash = hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    cache_dir = \"./cache\"\n",
    "    param_string = f\"{is_training}\"\n",
    "    cache_file = os.path.join(cache_dir, f\"prepared_df_{df_hash}_{param_string}.joblib\")\n",
    "\n",
    "    # Check if cached version exists\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading cached prepared DataFrame...\")\n",
    "        return joblib.load(cache_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    drop_columns_train = [\n",
    "        \"lesion_id\",\n",
    "        \"iddx_full\",\n",
    "        \"iddx_1\",\n",
    "        \"iddx_2\",\n",
    "        \"iddx_3\",\n",
    "        \"iddx_4\",\n",
    "        \"iddx_5\",\n",
    "        \"mel_mitotic_index\",\n",
    "        \"mel_thick_mm\",\n",
    "        \"tbp_lv_dnn_lesion_confidence\",\n",
    "    ]\n",
    "    drop_columns_test = [\"attribution\", \"copyright_license\"]\n",
    "\n",
    "    if is_training:\n",
    "        df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    target_columns = [\"target\"] if is_training else []\n",
    "    X = df.drop(target_columns + [\"isic_id\"], axis=1)\n",
    "    y = torch.tensor(df[\"target\"].values, dtype=torch.int8) if is_training else None\n",
    "\n",
    "    # Separate features by type\n",
    "    integer_features = X.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns\n",
    "    float_features = X.select_dtypes(include=[\"float64\", \"float32\", \"float16\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Handle NaN values and type conversions\n",
    "    for feature in float_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].mean()).astype(\"float32\")\n",
    "\n",
    "    for feature in integer_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median()).astype(\"int32\")\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        X[feature] = X[feature].astype(str).fillna(\"Unknown\")\n",
    "        X[feature] = pd.Categorical(X[feature]).codes\n",
    "\n",
    "    # Standardize all numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "    X_final = X_scaled\n",
    "\n",
    "    # Final check for any remaining NaN values\n",
    "    assert (\n",
    "        not X_final.isnull().any().any()\n",
    "    ), \"There are still NaN values in the processed data\"\n",
    "\n",
    "    print(\"Data shape after preprocessing:\", X_final.shape)\n",
    "    print(\"Number of NaN values after preprocessing:\", X_final.isnull().sum().sum())\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"DataFrame prepared in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Metadata Shape: {X_final.shape}\")\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    joblib.dump((X_final, y, df[\"isic_id\"]), cache_file)\n",
    "    return X_final, y, df[\"isic_id\"]\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, metadata_df, is_training=True, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.metadata_df = metadata_df\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.X, self.y, self.image_names = prepare_df(metadata_df, is_training)\n",
    "        self.metadata_shape = self.X.shape\n",
    "        self.train_transform = get_transforms(is_training=True)\n",
    "        self.test_transform = get_transforms(is_training=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, tuple):\n",
    "            idx, augment = idx\n",
    "        else:\n",
    "            augment = False\n",
    "\n",
    "        isic_id = self.image_names[idx]\n",
    "        metadata = torch.tensor(self.X.iloc[idx].values, dtype=torch.float16)\n",
    "\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf:\n",
    "            image_data = hdf[str(isic_id)][()]\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.is_training and augment:\n",
    "            image = self.train_transform(image)\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            target = self.y[idx]\n",
    "            target_long = target.long()\n",
    "            del target\n",
    "            target_one_hot = nn.functional.one_hot(target_long, num_classes=2).float()\n",
    "            return (image, metadata), target_one_hot\n",
    "        else:\n",
    "            return (image, metadata)\n",
    "\n",
    "\n",
    "# Create separate transforms for training and validation\n",
    "def get_transforms(is_training=True):\n",
    "    # Define augmentation parameters\n",
    "    ROTATION_RANGE = 90\n",
    "    BRIGHTNESS_RANGE = (0.9, 1.1)\n",
    "    CONTRAST_RANGE = (0.9, 1.1)\n",
    "    SATURATION_RANGE = (0.9, 1.1)\n",
    "    HUE_RANGE = (-0.001, 0.001)\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((139, 139), antialias=True),\n",
    "    ]\n",
    "\n",
    "    if is_training:\n",
    "        train_transforms = [\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=(139, 139), scale=(0.9, 1.1), antialias=True\n",
    "            ),\n",
    "            transforms.RandomRotation(\n",
    "                degrees=ROTATION_RANGE,\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=BRIGHTNESS_RANGE,\n",
    "                contrast=CONTRAST_RANGE,\n",
    "                saturation=SATURATION_RANGE,\n",
    "                hue=HUE_RANGE,\n",
    "            ),\n",
    "        ]\n",
    "        return transforms.Compose(train_transforms + base_transforms)\n",
    "    else:\n",
    "        return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_hdf5_path: str,\n",
    "        test_hdf5_path: str,\n",
    "        train_metadata_df: pd.DataFrame,\n",
    "        test_metadata_df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_hdf5_path = train_hdf5_path\n",
    "        self.test_hdf5_path = test_hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_metadata_df = train_metadata_df\n",
    "        self.test_metadata_df = test_metadata_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ISICDataset(\n",
    "            self.train_hdf5_path,\n",
    "            self.train_metadata_df,\n",
    "            True,\n",
    "            transform=get_transforms(is_training=True),\n",
    "        )\n",
    "        self.metadata_shape = full_dataset.metadata_shape\n",
    "        # Get targets for stratification\n",
    "        targets = self.train_metadata_df[\"target\"].values\n",
    "        balanced_indices = self.balance_dataset(np.arange(len(full_dataset)), targets)\n",
    "        # Extract actual indices and augmentation flags\n",
    "        balanced_indices, augmentation_flags = zip(*balanced_indices)\n",
    "        balanced_indices = np.array(balanced_indices)\n",
    "        augmentation_flags = np.array(augmentation_flags)\n",
    "        balanced_targets = targets[balanced_indices]\n",
    "        print(f\"Unique indices: {np.unique(balanced_indices)}\")\n",
    "        print(f\"Unique targets: {np.unique(balanced_targets)}\")\n",
    "        print(len(balanced_indices))\n",
    "        print(len(balanced_targets))\n",
    "        unique, counts = np.unique(balanced_targets, return_counts=True)\n",
    "        print(f\"Setup Count: {dict(zip(unique, counts))}\")\n",
    "        # Perform stratified split\n",
    "        train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "            balanced_indices,\n",
    "            balanced_targets,\n",
    "            test_size=0.2,\n",
    "            # stratify=balanced_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        val_indices, test_indices, val_targets, test_targets = train_test_split(\n",
    "            temp_indices,\n",
    "            temp_targets,\n",
    "            test_size=0.5,\n",
    "            # stratify=temp_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Create subset datasets\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            self.train_dataset = Subset(full_dataset, train_indices)\n",
    "            self.val_dataset = Subset(full_dataset, val_indices)\n",
    "            self.test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "        # Check for class balance\n",
    "        self._check_class_balance(train_targets.flatten(), \"Train\")\n",
    "        self._check_class_balance(val_targets.flatten(), \"Validation\")\n",
    "        self._check_class_balance(test_targets.flatten(), \"Test\")\n",
    "\n",
    "        print(f\"Length of full_dataset: {len(full_dataset)}\")\n",
    "        print(\n",
    "            f\"Length of train_indices: {len(train_indices)}, max index: {max(train_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of val_indices: {len(val_indices)}, max index: {max(val_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of test_indices: {len(test_indices)}, max index: {max(test_indices)}\"\n",
    "        )\n",
    "\n",
    "    def balance_dataset(self, indices, targets):\n",
    "        np.random.seed(42)\n",
    "        positive_indices = indices[targets == 1]\n",
    "        negative_indices = indices[targets == 0]\n",
    "\n",
    "        num_positive_samples = len(positive_indices)\n",
    "        num_negative_samples = len(negative_indices)\n",
    "\n",
    "        print(f\"Number of Positive Samples: {num_positive_samples}\")\n",
    "        print(f\"Number of Negative Samples: {num_negative_samples}\")\n",
    "\n",
    "        # Determine the number of samples for each class\n",
    "        num_samples = int(np.mean([num_positive_samples, num_negative_samples]))//3\n",
    "\n",
    "        # Upsample positive indices\n",
    "        upsampled_positive_indices = np.random.choice(\n",
    "            positive_indices, size=num_samples // 2, replace=True\n",
    "        )\n",
    "\n",
    "        # Downsample negative indices\n",
    "        downsampled_negative_indices = np.random.choice(\n",
    "            negative_indices, size=num_samples // 2, replace=False\n",
    "        )\n",
    "\n",
    "        # Add augmentation flag\n",
    "        balanced_indices = [(idx, True) for idx in upsampled_positive_indices] + [\n",
    "            (idx, False) for idx in downsampled_negative_indices\n",
    "        ]\n",
    "\n",
    "        np.random.shuffle(balanced_indices)\n",
    "\n",
    "        print(f\"Length of Balanced Positive Indices: {len(upsampled_positive_indices)}\")\n",
    "        print(\n",
    "            f\"Length of Balanced Negative Indices: {len(downsampled_negative_indices)}\"\n",
    "        )\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def _check_class_balance(self, targets, split_name):\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(\n",
    "            f\"{split_name} class distribution: {class_counts / len(targets)}, {len(targets)}\"\n",
    "        )\n",
    "        if len(class_counts) < 2 or min(class_counts) == 0:\n",
    "            raise ValueError(f\"Imbalanced classes in {split_name} split\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in train_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in val_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in test_loader: {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2505331/2539756025.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 139, 139\n",
    "\n",
    "# Load metadata\n",
    "train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(\"test-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached prepared DataFrame...\n",
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 33421\n",
      "Length of Balanced Negative Indices: 33421\n",
      "Unique indices: [    30     51     53 ... 401033 401052 401055]\n",
      "Unique targets: [0 1]\n",
      "66842\n",
      "66842\n",
      "Setup Count: {0: 33421, 1: 33421}\n",
      "Train class distribution: [0.50000935 0.49999065], 53473\n",
      "Validation class distribution: [0.48803112 0.51196888], 6684\n",
      "Test class distribution: [0.5118923 0.4881077], 6685\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 53473, max index: 401033\n",
      "Length of val_indices: 6684, max index: 401055\n",
      "Length of test_indices: 6685, max index: 401052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name             | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0  | conv1            | Conv2d             | 19.5 K\n",
      "1  | bn1              | BatchNorm2d        | 512   \n",
      "2  | activation       | Hardswish          | 0     \n",
      "3  | inv_res_blocks   | ModuleList         | 2.2 M \n",
      "4  | dense_block      | DenseBlock         | 2.0 M \n",
      "5  | transition       | TransitionLayer    | 351 K \n",
      "6  | attention        | AttentionBlock     | 215 K \n",
      "7  | avg_pool         | AvgPool2d          | 0     \n",
      "8  | inception        | InceptionBlock     | 406 K \n",
      "9  | inception2       | InceptionBlock     | 526 K \n",
      "10 | attention2       | AttentionBlock     | 280 K \n",
      "11 | gated_res        | GatedResidualBlock | 1.8 M \n",
      "12 | attention3       | AttentionBlock     | 263 K \n",
      "13 | global_avg_pool  | AdaptiveAvgPool2d  | 0     \n",
      "14 | flatten          | Flatten            | 0     \n",
      "15 | fc1              | Linear             | 1.1 M \n",
      "16 | bn_fc1           | BatchNorm1d        | 8.2 K \n",
      "17 | fc2              | Linear             | 8.4 M \n",
      "18 | bn_fc2           | BatchNorm1d        | 4.1 K \n",
      "19 | fc3              | Linear             | 1.0 M \n",
      "20 | bn_fc3           | BatchNorm1d        | 1.0 K \n",
      "21 | fc4              | Linear             | 65.7 K\n",
      "22 | bn_fc4           | BatchNorm1d        | 256   \n",
      "23 | dropout          | Dropout            | 0     \n",
      "24 | metadata_fc1     | Linear             | 172 K \n",
      "25 | metadata_bn1     | BatchNorm1d        | 8.2 K \n",
      "26 | metadata_fc2     | Linear             | 4.2 M \n",
      "27 | metadata_bn2     | BatchNorm1d        | 2.0 K \n",
      "28 | metadata_fc3     | Linear             | 524 K \n",
      "29 | metadata_bn3     | BatchNorm1d        | 1.0 K \n",
      "30 | metadata_fc4     | Linear             | 65.7 K\n",
      "31 | metadata_bn4     | BatchNorm1d        | 256   \n",
      "32 | final_fc         | Linear             | 514   \n",
      "33 | final_activation | Sigmoid            | 0     \n",
      "34 | loss             | CrossEntropyLoss   | 0     \n",
      "35 | auroc            | PartialAUROC       | 0     \n",
      "---------------------------------------------------------\n",
      "23.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 M    Total params\n",
      "94.654    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Number of batches in val_loader: 105\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 836\n",
      "Epoch 0: 100%|██████████| 941/941 [03:47<00:00,  4.15it/s, loss=0.392, v_num=98, train_loss_step=0.366, train_pAUC_step=0.193, val_loss=0.384, val_pAUC=0.179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 835: val_loss reached 0.38384 (best 0.38384), saving model to \"checkpoints/version_98/gurunet-epoch=00-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 941/941 [03:44<00:00,  4.19it/s, loss=0.398, v_num=98, train_loss_step=0.380, train_pAUC_step=0.166, val_loss=0.377, val_pAUC=0.179, train_loss_epoch=0.425, train_pAUC_epoch=0.165] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1671: val_loss reached 0.37698 (best 0.37698), saving model to \"checkpoints/version_98/gurunet-epoch=01-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 941/941 [03:47<00:00,  4.14it/s, loss=0.385, v_num=98, train_loss_step=0.383, train_pAUC_step=0.134, val_loss=0.378, val_pAUC=0.176, train_loss_epoch=0.401, train_pAUC_epoch=0.173] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2507: val_loss reached 0.37763 (best 0.37698), saving model to \"checkpoints/version_98/gurunet-epoch=02-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 941/941 [03:45<00:00,  4.18it/s, loss=0.371, v_num=98, train_loss_step=0.365, train_pAUC_step=0.193, val_loss=0.363, val_pAUC=0.179, train_loss_epoch=0.393, train_pAUC_epoch=0.175] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3343: val_loss reached 0.36335 (best 0.36335), saving model to \"checkpoints/version_98/gurunet-epoch=03-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 941/941 [03:47<00:00,  4.15it/s, loss=0.377, v_num=98, train_loss_step=0.433, train_pAUC_step=0.167, val_loss=0.363, val_pAUC=0.181, train_loss_epoch=0.388, train_pAUC_epoch=0.176] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4179: val_loss reached 0.36339 (best 0.36335), saving model to \"checkpoints/version_98/gurunet-epoch=04-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 941/941 [03:49<00:00,  4.11it/s, loss=0.402, v_num=98, train_loss_step=0.418, train_pAUC_step=0.184, val_loss=0.364, val_pAUC=0.183, train_loss_epoch=0.384, train_pAUC_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 5015: val_loss reached 0.36378 (best 0.36335), saving model to \"checkpoints/version_98/gurunet-epoch=05-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 941/941 [04:04<00:00,  3.85it/s, loss=0.381, v_num=98, train_loss_step=0.446, train_pAUC_step=0.187, val_loss=0.370, val_pAUC=0.177, train_loss_epoch=0.384, train_pAUC_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 5851: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 941/941 [04:01<00:00,  3.91it/s, loss=0.384, v_num=98, train_loss_step=0.430, train_pAUC_step=0.135, val_loss=0.359, val_pAUC=0.179, train_loss_epoch=0.380, train_pAUC_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 6687: val_loss reached 0.35887 (best 0.35887), saving model to \"checkpoints/version_98/gurunet-epoch=07-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 941/941 [04:00<00:00,  3.92it/s, loss=0.371, v_num=98, train_loss_step=0.419, train_pAUC_step=0.168, val_loss=0.358, val_pAUC=0.180, train_loss_epoch=0.377, train_pAUC_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 7523: val_loss reached 0.35757 (best 0.35757), saving model to \"checkpoints/version_98/gurunet-epoch=08-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 941/941 [04:00<00:00,  3.91it/s, loss=0.378, v_num=98, train_loss_step=0.345, train_pAUC_step=0.200, val_loss=0.352, val_pAUC=0.183, train_loss_epoch=0.376, train_pAUC_epoch=0.179] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 8359: val_loss reached 0.35193 (best 0.35193), saving model to \"checkpoints/version_98/gurunet-epoch=09-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 941/941 [04:00<00:00,  3.92it/s, loss=0.373, v_num=98, train_loss_step=0.358, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 9195: val_loss reached 0.35626 (best 0.35193), saving model to \"checkpoints/version_98/gurunet-epoch=10-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 941/941 [04:00<00:00,  3.92it/s, loss=0.369, v_num=98, train_loss_step=0.346, train_pAUC_step=0.193, val_loss=0.354, val_pAUC=0.184, train_loss_epoch=0.373, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 10031: val_loss reached 0.35365 (best 0.35193), saving model to \"checkpoints/version_98/gurunet-epoch=11-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 941/941 [04:06<00:00,  3.81it/s, loss=0.364, v_num=98, train_loss_step=0.377, train_pAUC_step=0.185, val_loss=0.350, val_pAUC=0.185, train_loss_epoch=0.371, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 10867: val_loss reached 0.35035 (best 0.35035), saving model to \"checkpoints/version_98/gurunet-epoch=12-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 941/941 [04:05<00:00,  3.84it/s, loss=0.367, v_num=98, train_loss_step=0.360, train_pAUC_step=0.200, val_loss=0.350, val_pAUC=0.185, train_loss_epoch=0.368, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 11703: val_loss reached 0.34994 (best 0.34994), saving model to \"checkpoints/version_98/gurunet-epoch=13-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 941/941 [04:04<00:00,  3.85it/s, loss=0.362, v_num=98, train_loss_step=0.333, train_pAUC_step=0.200, val_loss=0.352, val_pAUC=0.186, train_loss_epoch=0.368, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 12539: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 941/941 [03:57<00:00,  3.97it/s, loss=0.363, v_num=98, train_loss_step=0.402, train_pAUC_step=0.180, val_loss=0.347, val_pAUC=0.186, train_loss_epoch=0.366, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 13375: val_loss reached 0.34687 (best 0.34687), saving model to \"checkpoints/version_98/gurunet-epoch=15-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 941/941 [04:00<00:00,  3.91it/s, loss=0.357, v_num=98, train_loss_step=0.374, train_pAUC_step=0.172, val_loss=0.346, val_pAUC=0.184, train_loss_epoch=0.365, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 14211: val_loss reached 0.34601 (best 0.34601), saving model to \"checkpoints/version_98/gurunet-epoch=16-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 941/941 [03:58<00:00,  3.95it/s, loss=0.382, v_num=98, train_loss_step=0.378, train_pAUC_step=0.178, val_loss=0.351, val_pAUC=0.185, train_loss_epoch=0.365, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 15047: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 941/941 [03:59<00:00,  3.94it/s, loss=0.36, v_num=98, train_loss_step=0.344, train_pAUC_step=0.196, val_loss=0.350, val_pAUC=0.187, train_loss_epoch=0.364, train_pAUC_epoch=0.184]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 15883: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 941/941 [04:11<00:00,  3.74it/s, loss=0.358, v_num=98, train_loss_step=0.410, train_pAUC_step=0.187, val_loss=0.348, val_pAUC=0.187, train_loss_epoch=0.363, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 16719: val_loss reached 0.34787 (best 0.34601), saving model to \"checkpoints/version_98/gurunet-epoch=19-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 941/941 [03:59<00:00,  3.94it/s, loss=0.374, v_num=98, train_loss_step=0.403, train_pAUC_step=0.178, val_loss=0.344, val_pAUC=0.185, train_loss_epoch=0.360, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 17555: val_loss reached 0.34364 (best 0.34364), saving model to \"checkpoints/version_98/gurunet-epoch=20-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 941/941 [04:02<00:00,  3.89it/s, loss=0.364, v_num=98, train_loss_step=0.403, train_pAUC_step=0.188, val_loss=0.342, val_pAUC=0.187, train_loss_epoch=0.361, train_pAUC_epoch=0.184] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 18391: val_loss reached 0.34200 (best 0.34200), saving model to \"checkpoints/version_98/gurunet-epoch=21-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 941/941 [04:07<00:00,  3.80it/s, loss=0.357, v_num=98, train_loss_step=0.322, train_pAUC_step=0.200, val_loss=0.340, val_pAUC=0.187, train_loss_epoch=0.359, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 19227: val_loss reached 0.34011 (best 0.34011), saving model to \"checkpoints/version_98/gurunet-epoch=22-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 941/941 [03:54<00:00,  4.02it/s, loss=0.358, v_num=98, train_loss_step=0.403, train_pAUC_step=0.168, val_loss=0.345, val_pAUC=0.188, train_loss_epoch=0.359, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 20063: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 941/941 [03:47<00:00,  4.14it/s, loss=0.361, v_num=98, train_loss_step=0.468, train_pAUC_step=0.178, val_loss=0.342, val_pAUC=0.189, train_loss_epoch=0.357, train_pAUC_epoch=0.185] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 20899: val_loss reached 0.34177 (best 0.34011), saving model to \"checkpoints/version_98/gurunet-epoch=24-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 941/941 [03:53<00:00,  4.04it/s, loss=0.361, v_num=98, train_loss_step=0.344, train_pAUC_step=0.185, val_loss=0.347, val_pAUC=0.188, train_loss_epoch=0.356, train_pAUC_epoch=0.186] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 21735: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 941/941 [03:51<00:00,  4.07it/s, loss=0.355, v_num=98, train_loss_step=0.385, train_pAUC_step=0.185, val_loss=0.341, val_pAUC=0.189, train_loss_epoch=0.356, train_pAUC_epoch=0.186] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 22571: val_loss reached 0.34082 (best 0.34011), saving model to \"checkpoints/version_98/gurunet-epoch=26-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 941/941 [03:53<00:00,  4.04it/s, loss=0.35, v_num=98, train_loss_step=0.356, train_pAUC_step=0.192, val_loss=0.346, val_pAUC=0.188, train_loss_epoch=0.356, train_pAUC_epoch=0.186]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 23407: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 941/941 [03:55<00:00,  4.00it/s, loss=0.353, v_num=98, train_loss_step=0.364, train_pAUC_step=0.189, val_loss=0.344, val_pAUC=0.189, train_loss_epoch=0.355, train_pAUC_epoch=0.186]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 24243: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 941/941 [03:54<00:00,  4.02it/s, loss=0.35, v_num=98, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.341, val_pAUC=0.188, train_loss_epoch=0.355, train_pAUC_epoch=0.187]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 25079: val_loss reached 0.34137 (best 0.34011), saving model to \"checkpoints/version_98/gurunet-epoch=29-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 941/941 [03:54<00:00,  4.02it/s, loss=0.364, v_num=98, train_loss_step=0.443, train_pAUC_step=0.0963, val_loss=0.340, val_pAUC=0.189, train_loss_epoch=0.354, train_pAUC_epoch=0.187]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 25915: val_loss reached 0.33967 (best 0.33967), saving model to \"checkpoints/version_98/gurunet-epoch=30-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 941/941 [03:53<00:00,  4.03it/s, loss=0.351, v_num=98, train_loss_step=0.440, train_pAUC_step=0.162, val_loss=0.339, val_pAUC=0.188, train_loss_epoch=0.354, train_pAUC_epoch=0.187]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 26751: val_loss reached 0.33929 (best 0.33929), saving model to \"checkpoints/version_98/gurunet-epoch=31-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 941/941 [03:54<00:00,  4.02it/s, loss=0.352, v_num=98, train_loss_step=0.414, train_pAUC_step=0.172, val_loss=0.337, val_pAUC=0.189, train_loss_epoch=0.354, train_pAUC_epoch=0.187] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 27587: val_loss reached 0.33739 (best 0.33739), saving model to \"checkpoints/version_98/gurunet-epoch=32-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 941/941 [03:56<00:00,  3.98it/s, loss=0.35, v_num=98, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.338, val_pAUC=0.189, train_loss_epoch=0.352, train_pAUC_epoch=0.187]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 28423: val_loss reached 0.33751 (best 0.33739), saving model to \"checkpoints/version_98/gurunet-epoch=33-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 941/941 [03:55<00:00,  4.00it/s, loss=0.35, v_num=98, train_loss_step=0.344, train_pAUC_step=0.174, val_loss=0.337, val_pAUC=0.189, train_loss_epoch=0.352, train_pAUC_epoch=0.187] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 29259: val_loss reached 0.33718 (best 0.33718), saving model to \"checkpoints/version_98/gurunet-epoch=34-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 941/941 [03:54<00:00,  4.01it/s, loss=0.349, v_num=98, train_loss_step=0.315, train_pAUC_step=0.200, val_loss=0.341, val_pAUC=0.188, train_loss_epoch=0.352, train_pAUC_epoch=0.187]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 30095: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 941/941 [03:58<00:00,  3.95it/s, loss=0.349, v_num=98, train_loss_step=0.352, train_pAUC_step=0.196, val_loss=0.338, val_pAUC=0.190, train_loss_epoch=0.351, train_pAUC_epoch=0.187] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 30931: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 941/941 [03:53<00:00,  4.04it/s, loss=0.34, v_num=98, train_loss_step=0.323, train_pAUC_step=0.200, val_loss=0.339, val_pAUC=0.189, train_loss_epoch=0.350, train_pAUC_epoch=0.187]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 31767: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 941/941 [03:53<00:00,  4.04it/s, loss=0.349, v_num=98, train_loss_step=0.342, train_pAUC_step=0.200, val_loss=0.337, val_pAUC=0.189, train_loss_epoch=0.349, train_pAUC_epoch=0.188]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 32603: val_loss reached 0.33701 (best 0.33701), saving model to \"checkpoints/version_98/gurunet-epoch=38-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 941/941 [03:52<00:00,  4.05it/s, loss=0.353, v_num=98, train_loss_step=0.344, train_pAUC_step=0.155, val_loss=0.337, val_pAUC=0.189, train_loss_epoch=0.350, train_pAUC_epoch=0.187] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 33439: val_loss reached 0.33666 (best 0.33666), saving model to \"checkpoints/version_98/gurunet-epoch=39-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 941/941 [03:54<00:00,  4.01it/s, loss=0.349, v_num=98, train_loss_step=0.404, train_pAUC_step=0.148, val_loss=0.336, val_pAUC=0.190, train_loss_epoch=0.351, train_pAUC_epoch=0.187] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 34275: val_loss reached 0.33645 (best 0.33645), saving model to \"checkpoints/version_98/gurunet-epoch=40-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 41: 100%|██████████| 941/941 [03:54<00:00,  4.01it/s, loss=0.341, v_num=98, train_loss_step=0.313, train_pAUC_step=0.200, val_loss=0.334, val_pAUC=0.190, train_loss_epoch=0.350, train_pAUC_epoch=0.188] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 35111: val_loss reached 0.33413 (best 0.33413), saving model to \"checkpoints/version_98/gurunet-epoch=41-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 941/941 [03:56<00:00,  3.98it/s, loss=0.339, v_num=98, train_loss_step=0.404, train_pAUC_step=0.170, val_loss=0.334, val_pAUC=0.191, train_loss_epoch=0.345, train_pAUC_epoch=0.188] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 35947: val_loss reached 0.33361 (best 0.33361), saving model to \"checkpoints/version_98/gurunet-epoch=42-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 941/941 [04:15<00:00,  3.68it/s, loss=0.342, v_num=98, train_loss_step=0.344, train_pAUC_step=0.185, val_loss=0.332, val_pAUC=0.190, train_loss_epoch=0.344, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 36783: val_loss reached 0.33239 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=43-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 941/941 [03:55<00:00,  4.01it/s, loss=0.332, v_num=98, train_loss_step=0.339, train_pAUC_step=0.200, val_loss=0.334, val_pAUC=0.190, train_loss_epoch=0.343, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 37619: val_loss reached 0.33399 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=44-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 941/941 [03:55<00:00,  3.99it/s, loss=0.336, v_num=98, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.333, val_pAUC=0.190, train_loss_epoch=0.342, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 38455: val_loss reached 0.33328 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=45-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 941/941 [03:39<00:00,  4.28it/s, loss=0.337, v_num=98, train_loss_step=0.345, train_pAUC_step=0.189, val_loss=0.333, val_pAUC=0.190, train_loss_epoch=0.341, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 39291: val_loss reached 0.33267 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=46-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 941/941 [03:55<00:00,  4.00it/s, loss=0.331, v_num=98, train_loss_step=0.317, train_pAUC_step=0.200, val_loss=0.333, val_pAUC=0.191, train_loss_epoch=0.340, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 40127: val_loss reached 0.33318 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=47-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 941/941 [03:52<00:00,  4.06it/s, loss=0.34, v_num=98, train_loss_step=0.384, train_pAUC_step=0.187, val_loss=0.333, val_pAUC=0.191, train_loss_epoch=0.341, train_pAUC_epoch=0.190]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 40963: val_loss reached 0.33308 (best 0.33239), saving model to \"checkpoints/version_98/gurunet-epoch=48-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 941/941 [03:52<00:00,  4.05it/s, loss=0.339, v_num=98, train_loss_step=0.313, train_pAUC_step=0.200, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.341, train_pAUC_epoch=0.189]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 41799: val_loss reached 0.33155 (best 0.33155), saving model to \"checkpoints/version_98/gurunet-epoch=49-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 941/941 [03:51<00:00,  4.07it/s, loss=0.339, v_num=98, train_loss_step=0.321, train_pAUC_step=0.200, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.340, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, global step 42635: val_loss reached 0.33202 (best 0.33155), saving model to \"checkpoints/version_98/gurunet-epoch=50-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 941/941 [03:52<00:00,  4.06it/s, loss=0.34, v_num=98, train_loss_step=0.348, train_pAUC_step=0.200, val_loss=0.333, val_pAUC=0.191, train_loss_epoch=0.340, train_pAUC_epoch=0.190]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, global step 43471: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 941/941 [03:52<00:00,  4.05it/s, loss=0.339, v_num=98, train_loss_step=0.344, train_pAUC_step=0.200, val_loss=0.333, val_pAUC=0.191, train_loss_epoch=0.339, train_pAUC_epoch=0.190]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52, global step 44307: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 941/941 [03:48<00:00,  4.12it/s, loss=0.331, v_num=98, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.331, val_pAUC=0.191, train_loss_epoch=0.340, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 45143: val_loss reached 0.33141 (best 0.33141), saving model to \"checkpoints/version_98/gurunet-epoch=53-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 941/941 [03:43<00:00,  4.22it/s, loss=0.336, v_num=98, train_loss_step=0.325, train_pAUC_step=0.200, val_loss=0.330, val_pAUC=0.191, train_loss_epoch=0.339, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 45979: val_loss reached 0.33035 (best 0.33035), saving model to \"checkpoints/version_98/gurunet-epoch=54-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 941/941 [03:47<00:00,  4.15it/s, loss=0.342, v_num=98, train_loss_step=0.343, train_pAUC_step=0.200, val_loss=0.331, val_pAUC=0.190, train_loss_epoch=0.339, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 46815: val_loss reached 0.33140 (best 0.33035), saving model to \"checkpoints/version_98/gurunet-epoch=55-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 941/941 [04:00<00:00,  3.92it/s, loss=0.343, v_num=98, train_loss_step=0.374, train_pAUC_step=0.123, val_loss=0.333, val_pAUC=0.191, train_loss_epoch=0.339, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56, global step 47651: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 941/941 [03:59<00:00,  3.94it/s, loss=0.347, v_num=98, train_loss_step=0.345, train_pAUC_step=0.200, val_loss=0.334, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 48487: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 941/941 [03:52<00:00,  4.05it/s, loss=0.341, v_num=98, train_loss_step=0.340, train_pAUC_step=0.200, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 49323: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 941/941 [03:53<00:00,  4.03it/s, loss=0.337, v_num=98, train_loss_step=0.322, train_pAUC_step=0.200, val_loss=0.331, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59, global step 50159: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 941/941 [03:52<00:00,  4.06it/s, loss=0.344, v_num=98, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.331, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 50995: val_loss reached 0.33133 (best 0.33035), saving model to \"checkpoints/version_98/gurunet-epoch=60-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 941/941 [03:54<00:00,  4.02it/s, loss=0.333, v_num=98, train_loss_step=0.348, train_pAUC_step=0.190, val_loss=0.331, val_pAUC=0.191, train_loss_epoch=0.339, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 51831: val_loss reached 0.33065 (best 0.33035), saving model to \"checkpoints/version_98/gurunet-epoch=61-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 941/941 [03:50<00:00,  4.09it/s, loss=0.336, v_num=98, train_loss_step=0.346, train_pAUC_step=0.192, val_loss=0.331, val_pAUC=0.191, train_loss_epoch=0.337, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 52667: val_loss reached 0.33101 (best 0.33035), saving model to \"checkpoints/version_98/gurunet-epoch=62-val_loss=0.33.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 941/941 [03:56<00:00,  3.99it/s, loss=0.334, v_num=98, train_loss_step=0.344, train_pAUC_step=0.200, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 53503: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 941/941 [03:57<00:00,  3.97it/s, loss=0.336, v_num=98, train_loss_step=0.344, train_pAUC_step=0.191, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, global step 54339: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 64: 100%|██████████| 941/941 [03:57<00:00,  3.97it/s, loss=0.336, v_num=98, train_loss_step=0.344, train_pAUC_step=0.191, val_loss=0.332, val_pAUC=0.191, train_loss_epoch=0.338, train_pAUC_epoch=0.190]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "torch.cuda.memory.empty_cache()\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"gurunet_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/version_{logger.version}\",\n",
    "    filename=\"gurunet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize your data module\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    # accumulate_grad_batches=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Loading cached prepared DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 33421\n",
      "Length of Balanced Negative Indices: 33421\n",
      "Unique indices: [    30     51     53 ... 401033 401052 401055]\n",
      "Unique targets: [0 1]\n",
      "66842\n",
      "66842\n",
      "Setup Count: {0: 33421, 1: 33421}\n",
      "Train class distribution: [0.50000935 0.49999065], 53473\n",
      "Validation class distribution: [0.48803112 0.51196888], 6684\n",
      "Test class distribution: [0.5118923 0.4881077], 6685\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 53473, max index: 401033\n",
      "Length of val_indices: 6684, max index: 401055\n",
      "Length of test_indices: 6685, max index: 401052\n",
      "Number of batches in test_loader: 105\n",
      "Testing: 100%|██████████| 105/105 [00:18<00:00,  6.49it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.3328363001346588,\n",
      " 'test_loss_epoch': 0.3328363001346588,\n",
      " 'test_pAUC': 0.19089999794960022,\n",
      " 'test_pAUC_epoch': 0.19089999794960022}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 105/105 [00:18<00:00,  5.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3328363001346588,\n",
       "  'test_loss_epoch': 0.3328363001346588,\n",
       "  'test_pAUC': 0.19089999794960022,\n",
       "  'test_pAUC_epoch': 0.19089999794960022}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeoned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
