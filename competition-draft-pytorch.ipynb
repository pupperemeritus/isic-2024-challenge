{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Sampler\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import time\n",
    "from pytorch_lightning.callbacks import (\n",
    "    Callback,\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ProgressBar\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision.transforms import transforms\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "class PartialAUROC(Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_tpr: float = 0.80,\n",
    "        dist_sync_on_step: bool = False,\n",
    "    ):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.min_tpr = min_tpr\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        self.preds.append(preds)\n",
    "        self.target.append(target)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds)\n",
    "        target = torch.cat(self.target)\n",
    "        return self._partial_auroc(target, preds, self.min_tpr)\n",
    "\n",
    "    def _partial_auroc(\n",
    "        self, y_true: torch.Tensor, y_score: torch.Tensor, min_tpr: float\n",
    "    ) -> float:\n",
    "        y_true = torch.abs(y_true - 1)\n",
    "        y_score = -y_score\n",
    "\n",
    "        fpr, tpr, _ = self._roc_curve(y_true, y_score)\n",
    "        max_fpr = 1.0 - min_tpr\n",
    "\n",
    "        # print(f\"Computed FPR: {fpr}\")\n",
    "        # print(f\"Computed TPR: {tpr}\")\n",
    "\n",
    "        if max_fpr == 1:\n",
    "            return self._auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "\n",
    "        stop = torch.searchsorted(fpr, torch.tensor(max_fpr), right=True)\n",
    "        x_interp = fpr[stop - 1 : stop + 1]\n",
    "        y_interp = tpr[stop - 1 : stop + 1]\n",
    "\n",
    "        # print(f\"x_interp: {x_interp}\")\n",
    "        # print(f\"y_interp: {y_interp}\")\n",
    "\n",
    "        if len(x_interp) == 1:\n",
    "            interp_tpr = y_interp[0]\n",
    "        else:\n",
    "            interp_tpr = y_interp[0] + (max_fpr - x_interp[0]) * (\n",
    "                y_interp[1] - y_interp[0]\n",
    "            ) / (x_interp[1] - x_interp[0])\n",
    "\n",
    "        tpr = torch.cat([tpr[:stop], torch.tensor([interp_tpr])])\n",
    "        fpr = torch.cat([fpr[:stop], torch.tensor([max_fpr])])\n",
    "\n",
    "        partial_auc = self._auc(fpr, tpr)\n",
    "        return partial_auc\n",
    "\n",
    "    def _roc_curve(self, y_true: torch.Tensor, y_score: torch.Tensor):\n",
    "        desc_score_indices = torch.argsort(y_score, descending=True)\n",
    "        y_score = y_score[desc_score_indices]\n",
    "        y_true = y_true[desc_score_indices]\n",
    "\n",
    "        distinct_value_indices = torch.where(torch.diff(y_score))[0]\n",
    "        threshold_idxs = torch.cat(\n",
    "            [distinct_value_indices, torch.tensor([y_true.numel() - 1])]\n",
    "        )\n",
    "\n",
    "        tps = torch.cumsum(y_true, dim=0)[threshold_idxs]\n",
    "        fps = 1 + threshold_idxs - tps\n",
    "        \n",
    "        # Handle the case where there are no positive samples\n",
    "        if tps[-1] == 0:\n",
    "            tpr = torch.zeros_like(tps)\n",
    "        else:\n",
    "            tpr = tps / tps[-1]\n",
    "        \n",
    "        fpr = fps / fps[-1]\n",
    "        thresholds = y_score[threshold_idxs]\n",
    "\n",
    "        # print(f\"tps: {tps}\")\n",
    "        # print(f\"fps: {fps}\")\n",
    "        # print(f\"tpr: {tpr}\")\n",
    "        # print(f\"fpr: {fpr}\")\n",
    "        # print(f\"thresholds: {thresholds}\")\n",
    "\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def _auc(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        if torch.all(y == 0):\n",
    "            print(\"Warning: All TPR values are zero. AUC is undefined.\")\n",
    "            return 0.0\n",
    "\n",
    "        direction = 1\n",
    "        dx = torch.diff(x)\n",
    "        if torch.any(dx < 0):\n",
    "            if torch.all(dx <= 0):\n",
    "                direction = -1\n",
    "            else:\n",
    "                raise ValueError(\"x is neither increasing nor decreasing\")\n",
    "        auc_value = direction * torch.trapz(y, x).item()\n",
    "        # print(f\"Computed AUC: {auc_value}\")\n",
    "        return auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNActivation(in_channels, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBNActivation(\n",
    "                    hidden_dim, hidden_dim, stride=stride, groups=hidden_dim\n",
    "                ),\n",
    "                nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNActivation, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseLayer(in_channels + i * growth_rate, growth_rate, dropout_rate)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.cat([x, layer(x)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate):\n",
    "        super(DenseLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, 4 * growth_rate, 1, bias=False),\n",
    "            nn.BatchNorm2d(4 * growth_rate),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(4 * growth_rate, growth_rate, 3, padding=1, bias=False),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "        super(TransitionLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        att = nn.Hardswish()(g + x)\n",
    "        att = nn.Sigmoid()(self.bn3(self.conv3(att)))\n",
    "        return x * att\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "        self.branch1 = ConvBNActivation(in_channels, f1, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f2[0], kernel_size=1),\n",
    "            ConvBNActivation(f2[0], f2[1], kernel_size=3),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f3[0], kernel_size=1),\n",
    "            ConvBNActivation(f3[0], f3[1], kernel_size=5),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNActivation(in_channels, f1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides):\n",
    "        super(GatedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=strides,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "        # Add a shortcut connection if input and output dimensions don't match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=strides, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        gate = nn.Sigmoid()(self.bn3(self.conv3(x)))\n",
    "        x = x * gate\n",
    "        x += residual\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class GuruNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=(139, 139, 3),\n",
    "        metadata_shape=None,\n",
    "        classes=2,\n",
    "    ):\n",
    "        super(GuruNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.metadata_shape = metadata_shape\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.activation = nn.Hardswish()\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        self.inv_res_blocks = nn.ModuleList()\n",
    "        block_params = [\n",
    "            # expand_ratio, filters, strides, repeats\n",
    "            (6, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 40, 2, 2),\n",
    "            (6, 80, 2, 3),\n",
    "            (6, 112, 1, 3),\n",
    "            (6, 128, 2, 4),\n",
    "            (6, 196, 1, 1),\n",
    "        ]\n",
    "\n",
    "        in_channels = 128\n",
    "        for i, (expand_ratio, filters, strides, repeats) in enumerate(block_params):\n",
    "            for j in range(repeats):\n",
    "                if j > 0:\n",
    "                    strides = 1\n",
    "                self.inv_res_blocks.append(\n",
    "                    InvertedResidualBlock(in_channels, filters, expand_ratio, strides)\n",
    "                )\n",
    "                in_channels = filters\n",
    "\n",
    "        # Dense Block\n",
    "        self.dense_block = DenseBlock(in_channels, num_layers=16, growth_rate=32)\n",
    "        in_channels += 16 * 32  # Update in_channels after dense block\n",
    "\n",
    "        # Transition Layer\n",
    "        self.transition = TransitionLayer(in_channels, compression_factor=0.5)\n",
    "        in_channels = int(in_channels * 0.5)\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention = AttentionBlock(in_channels, 128)\n",
    "        in_channels = 128\n",
    "\n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Inception Block\n",
    "        self.inception = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention2 = AttentionBlock(in_channels, 128)\n",
    "        in_channels = 128\n",
    "\n",
    "        # Gated Residual Block\n",
    "        self.gated_res = GatedResidualBlock(in_channels, 256, kernel_size=3, strides=2)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention3 = AttentionBlock(in_channels, 128)\n",
    "        in_channels = 128\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_channels, 4096)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1024)\n",
    "        self.fc3 = nn.Linear(1024, 256)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn_fc4 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.metadata_fc1 = nn.Linear(metadata_shape[1] - 2, 4096)\n",
    "        self.metadata_bn1 = nn.BatchNorm1d(4096)\n",
    "        self.metadata_fc2 = nn.Linear(4096, 1024)\n",
    "        self.metadata_bn2 = nn.BatchNorm1d(1024)\n",
    "        self.metadata_fc3 = nn.Linear(1024, 512)\n",
    "        self.metadata_bn3 = nn.BatchNorm1d(512)\n",
    "        self.metadata_fc4 = nn.Linear(512, 128)\n",
    "        self.metadata_bn4 = nn.BatchNorm1d(128)\n",
    "        self.final_fc = nn.Linear(128 + 128, classes)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        self.scaler = GradScaler()\n",
    "        self.loss = self.loss = nn.CrossEntropyLoss()\n",
    "        self.auroc = PartialAUROC(min_tpr=0.8)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for block in self.inv_res_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Dense Block\n",
    "        x = self.dense_block(x)\n",
    "\n",
    "        # Transition Layer\n",
    "        x = self.transition(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # Inception Block\n",
    "        x = self.inception(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        # Gated Residual Block\n",
    "        x = self.gated_res(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc4(self.fc4(x)))\n",
    "\n",
    "        metadata = self.activation(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn2(self.metadata_fc2(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn3(self.metadata_fc3(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn4(self.metadata_fc4(metadata)))\n",
    "\n",
    "        x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.final_fc(x)\n",
    "        # Apply sigmoid to ensure output is between 0 and 1\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )  # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )\n",
    "\n",
    "        # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.NAdam(\n",
    "            self.parameters(), lr=0.001, momentum_decay=0.5, weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.2, patience=1, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "def prepare_df(\n",
    "    df: pd.DataFrame, is_training=True, n_components=0.95, variance_threshold=0.5\n",
    "):\n",
    "    print(\"Preparing DataFrame...\")\n",
    "    df_hash = hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    cache_dir = \"./cache\"\n",
    "    param_string = f\"{is_training}_{n_components}_{variance_threshold}\" \n",
    "    cache_file = os.path.join(cache_dir, f\"prepared_df_{df_hash}_{param_string}.joblib\")\n",
    "\n",
    "    # Check if cached version exists\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading cached prepared DataFrame...\")\n",
    "        return joblib.load(cache_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    drop_columns_train = [\n",
    "        \"lesion_id\",\n",
    "        \"iddx_full\",\n",
    "        \"iddx_1\",\n",
    "        \"iddx_2\",\n",
    "        \"iddx_3\",\n",
    "        \"iddx_4\",\n",
    "        \"iddx_5\",\n",
    "        \"mel_mitotic_index\",\n",
    "        \"mel_thick_mm\",\n",
    "        \"tbp_lv_dnn_lesion_confidence\",\n",
    "    ]\n",
    "    drop_columns_test = [\"attribution\", \"copyright_license\"]\n",
    "\n",
    "    # train_metadata_df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    # train_metadata_df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    # test_metadata_df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    if is_training:\n",
    "        df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    target_columns = [\"target\"] if is_training else []\n",
    "    X = df.drop(target_columns + [\"isic_id\"], axis=1)\n",
    "    y = torch.tensor(df[\"target\"].values, dtype=torch.int8) if is_training else None\n",
    "\n",
    "    # Separate features by type\n",
    "    integer_features = X.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns\n",
    "    float_features = X.select_dtypes(include=[\"float64\", \"float32\", \"float16\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Handle NaN values and type conversions\n",
    "    for feature in float_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].mean()).astype(\"float32\")\n",
    "\n",
    "    for feature in integer_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median()).astype(\"int32\")\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        X[feature] = X[feature].astype(str).fillna(\"Unknown\")\n",
    "        X[feature] = pd.Categorical(X[feature]).codes\n",
    "\n",
    "    # Feature Engineering\n",
    "    print(\"Performing feature engineering...\")\n",
    "\n",
    "    # 1. Polynomial features for numeric columns\n",
    "    numeric_features = list(float_features) + list(integer_features)\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly_features = poly.fit_transform(X[numeric_features])\n",
    "    poly_feature_names = poly.get_feature_names_out(numeric_features)\n",
    "    X_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=X.index)\n",
    "\n",
    "    # 2. Interaction terms between categorical and numeric features\n",
    "    for cat_feature in categorical_features:\n",
    "        for num_feature in numeric_features:\n",
    "            X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
    "                X[cat_feature] * X[num_feature]\n",
    "            )\n",
    "\n",
    "    # 3. Binning for numeric features\n",
    "    for feature in numeric_features:\n",
    "        X[f\"{feature}_binned\"] = pd.qcut(\n",
    "            X[feature], q=5, labels=False, duplicates=\"drop\"\n",
    "        )\n",
    "\n",
    "    # Combine all features\n",
    "    X = pd.concat([X, X_poly], axis=1)\n",
    "\n",
    "    # Standardize all numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "    # Dimensionality Reduction\n",
    "    print(\"Performing dimensionality reduction...\")\n",
    "\n",
    "    # 1. Variance Threshold\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_var_threshold = selector.fit_transform(X_scaled)\n",
    "\n",
    "    # 2. PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_var_threshold)\n",
    "\n",
    "    print(f\"Original number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of features after Variance Threshold: {X_var_threshold.shape[1]}\")\n",
    "    print(f\"Number of features after PCA: {X_pca.shape[1]}\")\n",
    "\n",
    "    # Final dataset\n",
    "    X_final = pd.DataFrame(X_pca, index=X.index)\n",
    "\n",
    "    # Final check for any remaining NaN values\n",
    "    assert (\n",
    "        not X_final.isnull().any().any()\n",
    "    ), \"There are still NaN values in the processed data\"\n",
    "\n",
    "    print(\"Data shape after preprocessing:\", X_final.shape)\n",
    "    print(\"Number of NaN values after preprocessing:\", X_final.isnull().sum().sum())\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"DataFrame prepared in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Metadata Shape: {X_final.shape}\")\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    joblib.dump((X_final, y, df[\"isic_id\"]), cache_file)\n",
    "    return X_final, y, df[\"isic_id\"]\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, metadata_df, is_training=True, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.metadata_df = metadata_df\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.X, self.y, self.image_names = prepare_df(metadata_df, is_training)\n",
    "        self.metadata_shape = self.X.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        isic_id = self.image_names[idx]\n",
    "        metadata = torch.tensor(self.X.iloc[idx].values, dtype=torch.float16)\n",
    "\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf:\n",
    "            image_data = hdf[str(isic_id)][()]\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            target = self.y[idx]\n",
    "            target_long = target.long()\n",
    "            del target\n",
    "            target_one_hot = nn.functional.one_hot(target_long, num_classes=2).float()\n",
    "            return (image, metadata), target_one_hot\n",
    "        else:\n",
    "            return (image, metadata)\n",
    "\n",
    "\n",
    "# Create separate transforms for training and validation\n",
    "def get_transforms(is_training=True):\n",
    "    # Define augmentation parameters\n",
    "    ROTATION_RANGE = 180\n",
    "    BRIGHTNESS_RANGE = (0.99, 1.01)\n",
    "    CONTRAST_RANGE = (0.9, 1.1)\n",
    "    SATURATION_RANGE = (0.99, 1.01)\n",
    "    HUE_RANGE = (-0.001, 0.001)\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((139, 139), antialias=True),\n",
    "    ]\n",
    "\n",
    "    if is_training:\n",
    "        train_transforms = [\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=(139, 139), scale=(0.9, 1.1), antialias=True\n",
    "            ),\n",
    "            transforms.RandomRotation(degrees=ROTATION_RANGE),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=BRIGHTNESS_RANGE,\n",
    "                contrast=CONTRAST_RANGE,\n",
    "                saturation=SATURATION_RANGE,\n",
    "                hue=HUE_RANGE,\n",
    "            ),\n",
    "        ]\n",
    "        return transforms.Compose(train_transforms + base_transforms)\n",
    "    else:\n",
    "        return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_hdf5_path: str,\n",
    "        test_hdf5_path: str,\n",
    "        train_metadata_df: pd.DataFrame,\n",
    "        test_metadata_df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_hdf5_path = train_hdf5_path\n",
    "        self.test_hdf5_path = test_hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_metadata_df = train_metadata_df\n",
    "        self.test_metadata_df = test_metadata_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ISICDataset(\n",
    "            self.train_hdf5_path,\n",
    "            self.train_metadata_df,\n",
    "            True,\n",
    "            transform=get_transforms(is_training=True),\n",
    "        )\n",
    "        self.metadata_shape = full_dataset.metadata_shape\n",
    "        # Get targets for stratification\n",
    "        targets = self.train_metadata_df[\"target\"].values\n",
    "        balanced_indices = self.balance_dataset(np.arange(len(full_dataset)), targets)\n",
    "        balanced_targets = targets[balanced_indices]\n",
    "        print(f\"Unique indices: {np.unique(balanced_indices)}\")\n",
    "        print(len(balanced_indices))\n",
    "        print(len(balanced_targets))\n",
    "        print(Counter(balanced_targets))\n",
    "        # Perform stratified split\n",
    "        train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "            balanced_indices,\n",
    "            balanced_targets,\n",
    "            test_size=0.2,\n",
    "            stratify=balanced_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        val_indices, test_indices, val_targets, test_targets = train_test_split(\n",
    "            temp_indices,\n",
    "            temp_targets,\n",
    "            test_size=0.5,\n",
    "            stratify=temp_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Create subset datasets\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            self.train_dataset = Subset(\n",
    "                full_dataset, train_indices\n",
    "            )\n",
    "            self.val_dataset = Subset(\n",
    "                full_dataset, val_indices\n",
    "            )\n",
    "            self.test_dataset = Subset(\n",
    "                full_dataset, test_indices\n",
    "            )\n",
    "\n",
    "        # Check for class balance\n",
    "        self._check_class_balance(train_targets, \"Train\")\n",
    "        self._check_class_balance(val_targets, \"Validation\")\n",
    "        self._check_class_balance(test_targets, \"Test\")\n",
    "\n",
    "        print(f\"Length of full_dataset: {len(full_dataset)}\")\n",
    "        print(\n",
    "            f\"Length of train_indices: {len(train_indices)}, max index: {max(train_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of val_indices: {len(val_indices)}, max index: {max(val_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of test_indices: {len(test_indices)}, max index: {max(test_indices)}\"\n",
    "        )\n",
    "\n",
    "    def balance_dataset(self, indices, targets):\n",
    "        # Determine class balance in the dataset\n",
    "        np.random.seed(42)\n",
    "        labels = targets\n",
    "        positive_indices = indices[np.where(labels[indices] == 1)[0]]\n",
    "        negative_indices = indices[np.where(labels[indices] == 0)[0]]\n",
    "\n",
    "        num_positive_samples = len(positive_indices)\n",
    "        num_negative_samples = len(negative_indices)\n",
    "\n",
    "        num_avg_samples = (num_negative_samples + num_positive_samples) // 2\n",
    "        # Upsample positive indices\n",
    "        balanced_indices = np.concatenate(\n",
    "            [\n",
    "                np.random.choice(positive_indices, size=num_avg_samples, replace=True),\n",
    "                np.random.choice(negative_indices, size=num_avg_samples, replace=False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def balance_dataset_parallel(self, dataset, indices, targets):\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(f\"Number of CPU cores available: {num_cores}\")\n",
    "\n",
    "        # Use joblib for parallel processing\n",
    "        balanced_indices = Parallel(n_jobs=num_cores)(\n",
    "            delayed(self.balance_indices)(dataset, indices[i], targets)\n",
    "            for i in range(len(indices))\n",
    "        )\n",
    "\n",
    "        return [idx for sublist in balanced_indices for idx in sublist]\n",
    "\n",
    "    def _check_class_balance(self, targets, split_name):\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(\n",
    "            f\"{split_name} class distribution: {class_counts / len(targets)}, {len(targets)}\"\n",
    "        )\n",
    "        if len(class_counts) < 2 or min(class_counts) == 0:\n",
    "            raise ValueError(f\"Imbalanced classes in {split_name} split\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in train_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in val_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in test_loader: {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1487702/2539756025.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 139, 139\n",
    "\n",
    "# Load metadata\n",
    "train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(\"test-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Performing feature engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{cat_feature}_{num_feature}_interaction\"] = (\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n",
      "/tmp/ipykernel_1487702/1106105853.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[f\"{feature}_binned\"] = pd.qcut(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction...\n",
      "Original number of features: 942\n",
      "Number of features after Variance Threshold: 907\n",
      "Number of features after PCA: 35\n",
      "Data shape after preprocessing: (401059, 35)\n",
      "Number of NaN values after preprocessing: 0\n",
      "Class distribution:\n",
      "target\n",
      "0    0.99902\n",
      "1    0.00098\n",
      "Name: proportion, dtype: float64\n",
      "DataFrame prepared in 75.91 seconds\n",
      "Metadata Shape: (401059, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique indices: [     0      2      3 ... 401052 401054 401056]\n",
      "401058\n",
      "401058\n",
      "Counter({1: 200529, 0: 200529})\n",
      "Train class distribution: [0.5 0.5], 320846\n",
      "Validation class distribution: [0.5 0.5], 40106\n",
      "Test class distribution: [0.5 0.5], 40106\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 320846, max index: 401056\n",
      "Length of val_indices: 40106, max index: 401022\n",
      "Length of test_indices: 40106, max index: 401050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name             | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0  | conv1            | Conv2d             | 3.6 K \n",
      "1  | bn1              | BatchNorm2d        | 256   \n",
      "2  | activation       | Hardswish          | 0     \n",
      "3  | inv_res_blocks   | ModuleList         | 1.8 M \n",
      "4  | dense_block      | DenseBlock         | 1.5 M \n",
      "5  | transition       | TransitionLayer    | 252 K \n",
      "6  | attention        | AttentionBlock     | 91.5 K\n",
      "7  | avg_pool         | AvgPool2d          | 0     \n",
      "8  | inception        | InceptionBlock     | 352 K \n",
      "9  | attention2       | AttentionBlock     | 140 K \n",
      "10 | gated_res        | GatedResidualBlock | 461 K \n",
      "11 | attention3       | AttentionBlock     | 66.4 K\n",
      "12 | global_avg_pool  | AdaptiveAvgPool2d  | 0     \n",
      "13 | flatten          | Flatten            | 0     \n",
      "14 | fc1              | Linear             | 528 K \n",
      "15 | bn_fc1           | BatchNorm1d        | 8.2 K \n",
      "16 | fc2              | Linear             | 4.2 M \n",
      "17 | bn_fc2           | BatchNorm1d        | 2.0 K \n",
      "18 | fc3              | Linear             | 262 K \n",
      "19 | bn_fc3           | BatchNorm1d        | 512   \n",
      "20 | fc4              | Linear             | 32.9 K\n",
      "21 | bn_fc4           | BatchNorm1d        | 256   \n",
      "22 | dropout          | Dropout            | 0     \n",
      "23 | metadata_fc1     | Linear             | 147 K \n",
      "24 | metadata_bn1     | BatchNorm1d        | 8.2 K \n",
      "25 | metadata_fc2     | Linear             | 4.2 M \n",
      "26 | metadata_bn2     | BatchNorm1d        | 2.0 K \n",
      "27 | metadata_fc3     | Linear             | 524 K \n",
      "28 | metadata_bn3     | BatchNorm1d        | 1.0 K \n",
      "29 | metadata_fc4     | Linear             | 65.7 K\n",
      "30 | metadata_bn4     | BatchNorm1d        | 256   \n",
      "31 | final_fc         | Linear             | 514   \n",
      "32 | final_activation | Sigmoid            | 0     \n",
      "33 | loss             | CrossEntropyLoss   | 0     \n",
      "34 | auroc            | PartialAUROC       | 0     \n",
      "---------------------------------------------------------\n",
      "14.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.7 M    Total params\n",
      "58.714    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Number of batches in val_loader: 279\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 2229\n",
      "Epoch 0:  24%|       | 610/2508 [03:11<09:53,  3.20it/s, loss=0.388, v_num=68, train_loss_step=0.404, train_pAUC_step=0.173] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "torch.cuda.memory.empty_cache()\n",
    "\n",
    "batch_size = 144\n",
    "epochs = 20\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"gurunet_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/version_{logger.version}\",\n",
    "    filename=\"gurunet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize your data module\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    # accumulate_grad_batches=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeoned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
