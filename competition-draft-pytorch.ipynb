{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Sampler\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import time\n",
    "from pytorch_lightning.callbacks import (\n",
    "    Callback,\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ProgressBar\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision.transforms import transforms\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "class PartialAUROC(Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_tpr: float = 0.80,\n",
    "        dist_sync_on_step: bool = False,\n",
    "    ):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.min_tpr = min_tpr\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        self.preds.append(preds)\n",
    "        self.target.append(target)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds)\n",
    "        target = torch.cat(self.target)\n",
    "        return self._partial_auroc(target, preds, self.min_tpr)\n",
    "\n",
    "    def _partial_auroc(\n",
    "        self, y_true: torch.Tensor, y_score: torch.Tensor, min_tpr: float\n",
    "    ) -> float:\n",
    "        y_true = torch.abs(y_true - 1)\n",
    "        y_score = -y_score\n",
    "\n",
    "        fpr, tpr, _ = self._roc_curve(y_true, y_score)\n",
    "        max_fpr = 1.0 - min_tpr\n",
    "\n",
    "        # print(f\"Computed FPR: {fpr}\")\n",
    "        # print(f\"Computed TPR: {tpr}\")\n",
    "\n",
    "        if max_fpr == 1:\n",
    "            return self._auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "\n",
    "        stop = torch.searchsorted(fpr, torch.tensor(max_fpr), right=True)\n",
    "        x_interp = fpr[stop - 1 : stop + 1]\n",
    "        y_interp = tpr[stop - 1 : stop + 1]\n",
    "\n",
    "        # print(f\"x_interp: {x_interp}\")\n",
    "        # print(f\"y_interp: {y_interp}\")\n",
    "\n",
    "        if len(x_interp) == 1:\n",
    "            interp_tpr = y_interp[0]\n",
    "        else:\n",
    "            interp_tpr = y_interp[0] + (max_fpr - x_interp[0]) * (\n",
    "                y_interp[1] - y_interp[0]\n",
    "            ) / (x_interp[1] - x_interp[0])\n",
    "\n",
    "        tpr = torch.cat([tpr[:stop], torch.tensor([interp_tpr])])\n",
    "        fpr = torch.cat([fpr[:stop], torch.tensor([max_fpr])])\n",
    "\n",
    "        partial_auc = self._auc(fpr, tpr)\n",
    "        return partial_auc\n",
    "\n",
    "    def _roc_curve(self, y_true: torch.Tensor, y_score: torch.Tensor):\n",
    "        desc_score_indices = torch.argsort(y_score, descending=True)\n",
    "        y_score = y_score[desc_score_indices]\n",
    "        y_true = y_true[desc_score_indices]\n",
    "\n",
    "        distinct_value_indices = torch.where(torch.diff(y_score))[0]\n",
    "        threshold_idxs = torch.cat(\n",
    "            [distinct_value_indices, torch.tensor([y_true.numel() - 1])]\n",
    "        )\n",
    "\n",
    "        tps = torch.cumsum(y_true, dim=0)[threshold_idxs]\n",
    "        fps = 1 + threshold_idxs - tps\n",
    "        \n",
    "        # Handle the case where there are no positive samples\n",
    "        if tps[-1] == 0:\n",
    "            tpr = torch.zeros_like(tps)\n",
    "        else:\n",
    "            tpr = tps / tps[-1]\n",
    "        \n",
    "        fpr = fps / fps[-1]\n",
    "        thresholds = y_score[threshold_idxs]\n",
    "\n",
    "        # print(f\"tps: {tps}\")\n",
    "        # print(f\"fps: {fps}\")\n",
    "        # print(f\"tpr: {tpr}\")\n",
    "        # print(f\"fpr: {fpr}\")\n",
    "        # print(f\"thresholds: {thresholds}\")\n",
    "\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def _auc(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        if torch.all(y == 0):\n",
    "            print(\"Warning: All TPR values are zero. AUC is undefined.\")\n",
    "            return 0.0\n",
    "\n",
    "        direction = 1\n",
    "        dx = torch.diff(x)\n",
    "        if torch.any(dx < 0):\n",
    "            if torch.all(dx <= 0):\n",
    "                direction = -1\n",
    "            else:\n",
    "                raise ValueError(\"x is neither increasing nor decreasing\")\n",
    "        auc_value = direction * torch.trapz(y, x).item()\n",
    "        # print(f\"Computed AUC: {auc_value}\")\n",
    "        return auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNActivation(in_channels, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBNActivation(\n",
    "                    hidden_dim, hidden_dim, stride=stride, groups=hidden_dim\n",
    "                ),\n",
    "                nn.Conv2d(hidden_dim, out_channels, 1, bias=True),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNActivation, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseLayer(in_channels + i * growth_rate, growth_rate, dropout_rate)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.cat([x, layer(x)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate):\n",
    "        super(DenseLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, 4 * growth_rate, 1, bias=True),\n",
    "            nn.BatchNorm2d(4 * growth_rate),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(4 * growth_rate, growth_rate, 3, padding=1, bias=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "        super(TransitionLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=True),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        att = nn.Hardswish()(g + x)\n",
    "        att = nn.Sigmoid()(self.bn3(self.conv3(att)))\n",
    "        return x * att\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "        self.branch1 = ConvBNActivation(in_channels, f1, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f2[0], kernel_size=1),\n",
    "            ConvBNActivation(f2[0], f2[1], kernel_size=3),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f3[0], kernel_size=1),\n",
    "            ConvBNActivation(f3[0], f3[1], kernel_size=5),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNActivation(in_channels, f1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides):\n",
    "        super(GatedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=strides,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "        # Add a shortcut connection if input and output dimensions don't match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=strides, bias=True\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        gate = nn.Sigmoid()(self.bn3(self.conv3(x)))\n",
    "        x = x * gate\n",
    "        x += residual\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class GuruNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=(139, 139, 3),\n",
    "        metadata_shape=None,\n",
    "        classes=2,\n",
    "    ):\n",
    "        super(GuruNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.metadata_shape = metadata_shape\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.activation = nn.Hardswish()\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        self.inv_res_blocks = nn.ModuleList()\n",
    "        block_params = [\n",
    "            # expand_ratio, filters, strides, repeats\n",
    "            (6, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 40, 2, 2),\n",
    "            (6, 80, 2, 3),\n",
    "            (6, 112, 1, 3),\n",
    "            (6, 128, 2, 4),\n",
    "            (6, 196, 1, 1),\n",
    "        ]\n",
    "\n",
    "        in_channels = 256\n",
    "        for i, (expand_ratio, filters, strides, repeats) in enumerate(block_params):\n",
    "            for j in range(repeats):\n",
    "                if j > 0:\n",
    "                    strides = 1\n",
    "                self.inv_res_blocks.append(\n",
    "                    InvertedResidualBlock(in_channels, filters, expand_ratio, strides)\n",
    "                )\n",
    "                in_channels = filters\n",
    "\n",
    "        # Dense Block\n",
    "        self.dense_block = DenseBlock(in_channels, num_layers=20, growth_rate=32)\n",
    "        in_channels += 20 * 32  # Update in_channels after dense block\n",
    "\n",
    "        # Transition Layer\n",
    "        self.transition = TransitionLayer(in_channels, compression_factor=0.5)\n",
    "        in_channels = int(in_channels * 0.5)\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Inception Block\n",
    "        self.inception = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        self.inception2 = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention2 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Gated Residual Block\n",
    "        self.gated_res = GatedResidualBlock(in_channels, 512, kernel_size=3, strides=2)\n",
    "        in_channels = 512\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention3 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_channels, 4096)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(2048)\n",
    "        self.fc3 = nn.Linear(2048, 512)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 128)\n",
    "        self.bn_fc4 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.metadata_fc1 = nn.Linear(41, 4096)\n",
    "        self.metadata_bn1 = nn.BatchNorm1d(4096)\n",
    "        self.metadata_fc2 = nn.Linear(4096, 1024)\n",
    "        self.metadata_bn2 = nn.BatchNorm1d(1024)\n",
    "        self.metadata_fc3 = nn.Linear(1024, 512)\n",
    "        self.metadata_bn3 = nn.BatchNorm1d(512)\n",
    "        self.metadata_fc4 = nn.Linear(512, 128)\n",
    "        self.metadata_bn4 = nn.BatchNorm1d(128)\n",
    "        self.final_fc = nn.Linear(128 + 128, classes)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        self.scaler = GradScaler()\n",
    "        self.loss = self.loss = nn.CrossEntropyLoss()\n",
    "        self.auroc = PartialAUROC(min_tpr=0.8)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for block in self.inv_res_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Dense Block\n",
    "        x = self.dense_block(x)\n",
    "\n",
    "        # Transition Layer\n",
    "        x = self.transition(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # Inception Block\n",
    "        x = self.inception(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        # Gated Residual Block\n",
    "        x = self.gated_res(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc4(self.fc4(x)))\n",
    "\n",
    "        metadata = self.activation(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn2(self.metadata_fc2(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn3(self.metadata_fc3(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn4(self.metadata_fc4(metadata)))\n",
    "\n",
    "        x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.final_fc(x)\n",
    "        # Apply sigmoid to ensure output is between 0 and 1\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )  # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )\n",
    "\n",
    "        # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(\n",
    "            pos_probs, targets_binary\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.NAdam(\n",
    "            self.parameters(), lr=0.001, momentum_decay=0.5, weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.2, patience=1, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def prepare_df(\n",
    "    df: pd.DataFrame,\n",
    "    is_training=True,\n",
    "):\n",
    "    print(\"Preparing DataFrame...\")\n",
    "    df_hash = hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    cache_dir = \"./cache\"\n",
    "    param_string = f\"{is_training}\"\n",
    "    cache_file = os.path.join(cache_dir, f\"prepared_df_{df_hash}_{param_string}.joblib\")\n",
    "\n",
    "    # Check if cached version exists\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading cached prepared DataFrame...\")\n",
    "        return joblib.load(cache_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    drop_columns_train = [\n",
    "        \"lesion_id\",\n",
    "        \"iddx_full\",\n",
    "        \"iddx_1\",\n",
    "        \"iddx_2\",\n",
    "        \"iddx_3\",\n",
    "        \"iddx_4\",\n",
    "        \"iddx_5\",\n",
    "        \"mel_mitotic_index\",\n",
    "        \"mel_thick_mm\",\n",
    "        \"tbp_lv_dnn_lesion_confidence\",\n",
    "    ]\n",
    "    drop_columns_test = [\"attribution\", \"copyright_license\"]\n",
    "\n",
    "    if is_training:\n",
    "        df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    target_columns = [\"target\"] if is_training else []\n",
    "    X = df.drop(target_columns + [\"isic_id\"], axis=1)\n",
    "    y = torch.tensor(df[\"target\"].values, dtype=torch.int8) if is_training else None\n",
    "\n",
    "    # Separate features by type\n",
    "    integer_features = X.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns\n",
    "    float_features = X.select_dtypes(include=[\"float64\", \"float32\", \"float16\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Handle NaN values and type conversions\n",
    "    for feature in float_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].mean()).astype(\"float32\")\n",
    "\n",
    "    for feature in integer_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median()).astype(\"int32\")\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        X[feature] = X[feature].astype(str).fillna(\"Unknown\")\n",
    "        X[feature] = pd.Categorical(X[feature]).codes\n",
    "\n",
    "    # Standardize all numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "    X_final = X_scaled\n",
    "\n",
    "    # Final check for any remaining NaN values\n",
    "    assert (\n",
    "        not X_final.isnull().any().any()\n",
    "    ), \"There are still NaN values in the processed data\"\n",
    "\n",
    "    print(\"Data shape after preprocessing:\", X_final.shape)\n",
    "    print(\"Number of NaN values after preprocessing:\", X_final.isnull().sum().sum())\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"DataFrame prepared in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Metadata Shape: {X_final.shape}\")\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    joblib.dump((X_final, y, df[\"isic_id\"]), cache_file)\n",
    "    return X_final, y, df[\"isic_id\"]\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, metadata_df, is_training=True, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.metadata_df = metadata_df\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.X, self.y, self.image_names = prepare_df(metadata_df, is_training)\n",
    "        self.metadata_shape = self.X.shape\n",
    "        self.train_transform = get_transforms(is_training=True)\n",
    "        self.test_transform = get_transforms(is_training=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, tuple):\n",
    "            idx, augment = idx\n",
    "        else:\n",
    "            augment = False\n",
    "\n",
    "        isic_id = self.image_names[idx]\n",
    "        metadata = torch.tensor(self.X.iloc[idx].values, dtype=torch.float16)\n",
    "\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf:\n",
    "            image_data = hdf[str(isic_id)][()]\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.is_training and augment:\n",
    "            image = self.train_transform(image)\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            target = self.y[idx]\n",
    "            target_long = target.long()\n",
    "            del target\n",
    "            target_one_hot = nn.functional.one_hot(target_long, num_classes=2).float()\n",
    "            return (image, metadata), target_one_hot\n",
    "        else:\n",
    "            return (image, metadata)\n",
    "\n",
    "\n",
    "# Create separate transforms for training and validation\n",
    "def get_transforms(is_training=True):\n",
    "    # Define augmentation parameters\n",
    "    ROTATION_RANGE = 90\n",
    "    BRIGHTNESS_RANGE = (0.9, 1.1)\n",
    "    CONTRAST_RANGE = (0.9, 1.1)\n",
    "    SATURATION_RANGE = (0.9, 1.1)\n",
    "    HUE_RANGE = (-0.001, 0.001)\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((139, 139), antialias=True),\n",
    "    ]\n",
    "\n",
    "    if is_training:\n",
    "        train_transforms = [\n",
    "            transforms.RandomResizedCrop(\n",
    "                size=(139, 139), scale=(0.9, 1.1), antialias=True\n",
    "            ),\n",
    "            transforms.RandomRotation(\n",
    "                degrees=ROTATION_RANGE,\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=BRIGHTNESS_RANGE,\n",
    "                contrast=CONTRAST_RANGE,\n",
    "                saturation=SATURATION_RANGE,\n",
    "                hue=HUE_RANGE,\n",
    "            ),\n",
    "        ]\n",
    "        return transforms.Compose(train_transforms + base_transforms)\n",
    "    else:\n",
    "        return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_hdf5_path: str,\n",
    "        test_hdf5_path: str,\n",
    "        train_metadata_df: pd.DataFrame,\n",
    "        test_metadata_df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_hdf5_path = train_hdf5_path\n",
    "        self.test_hdf5_path = test_hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_metadata_df = train_metadata_df\n",
    "        self.test_metadata_df = test_metadata_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ISICDataset(\n",
    "            self.train_hdf5_path,\n",
    "            self.train_metadata_df,\n",
    "            True,\n",
    "            transform=get_transforms(is_training=True),\n",
    "        )\n",
    "        self.metadata_shape = full_dataset.metadata_shape\n",
    "        # Get targets for stratification\n",
    "        targets = self.train_metadata_df[\"target\"].values\n",
    "        balanced_indices = self.balance_dataset(np.arange(len(full_dataset)), targets)\n",
    "        # Extract actual indices and augmentation flags\n",
    "        balanced_indices, augmentation_flags = zip(*balanced_indices)\n",
    "        balanced_indices = np.array(balanced_indices)\n",
    "        augmentation_flags = np.array(augmentation_flags)\n",
    "        balanced_targets = targets[balanced_indices]\n",
    "        print(f\"Unique indices: {np.unique(balanced_indices)}\")\n",
    "        print(f\"Unique targets: {np.unique(balanced_targets)}\")\n",
    "        print(len(balanced_indices))\n",
    "        print(len(balanced_targets))\n",
    "        unique, counts = np.unique(balanced_targets, return_counts=True)\n",
    "        print(f\"Setup Count: {dict(zip(unique, counts))}\")\n",
    "        # Perform stratified split\n",
    "        train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "            balanced_indices,\n",
    "            balanced_targets,\n",
    "            test_size=0.2,\n",
    "            # stratify=balanced_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        val_indices, test_indices, val_targets, test_targets = train_test_split(\n",
    "            temp_indices,\n",
    "            temp_targets,\n",
    "            test_size=0.5,\n",
    "            # stratify=temp_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Create subset datasets\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            self.train_dataset = Subset(full_dataset, train_indices)\n",
    "            self.val_dataset = Subset(full_dataset, val_indices)\n",
    "            self.test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "        # Check for class balance\n",
    "        self._check_class_balance(train_targets.flatten(), \"Train\")\n",
    "        self._check_class_balance(val_targets.flatten(), \"Validation\")\n",
    "        self._check_class_balance(test_targets.flatten(), \"Test\")\n",
    "\n",
    "        print(f\"Length of full_dataset: {len(full_dataset)}\")\n",
    "        print(\n",
    "            f\"Length of train_indices: {len(train_indices)}, max index: {max(train_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of val_indices: {len(val_indices)}, max index: {max(val_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of test_indices: {len(test_indices)}, max index: {max(test_indices)}\"\n",
    "        )\n",
    "\n",
    "    def balance_dataset(self, indices, targets):\n",
    "        np.random.seed(42)\n",
    "        positive_indices = indices[targets == 1]\n",
    "        negative_indices = indices[targets == 0]\n",
    "\n",
    "        num_positive_samples = len(positive_indices)\n",
    "        num_negative_samples = len(negative_indices)\n",
    "\n",
    "        print(f\"Number of Positive Samples: {num_positive_samples}\")\n",
    "        print(f\"Number of Negative Samples: {num_negative_samples}\")\n",
    "\n",
    "        # Determine the number of samples for each class\n",
    "        num_samples = int(np.mean([num_positive_samples, num_negative_samples]))//3\n",
    "\n",
    "        # Upsample positive indices\n",
    "        upsampled_positive_indices = np.random.choice(\n",
    "            positive_indices, size=num_samples // 2, replace=True\n",
    "        )\n",
    "\n",
    "        # Downsample negative indices\n",
    "        downsampled_negative_indices = np.random.choice(\n",
    "            negative_indices, size=num_samples // 2, replace=False\n",
    "        )\n",
    "\n",
    "        # Add augmentation flag\n",
    "        balanced_indices = [(idx, True) for idx in upsampled_positive_indices] + [\n",
    "            (idx, False) for idx in downsampled_negative_indices\n",
    "        ]\n",
    "\n",
    "        np.random.shuffle(balanced_indices)\n",
    "\n",
    "        print(f\"Length of Balanced Positive Indices: {len(upsampled_positive_indices)}\")\n",
    "        print(\n",
    "            f\"Length of Balanced Negative Indices: {len(downsampled_negative_indices)}\"\n",
    "        )\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def _check_class_balance(self, targets, split_name):\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(\n",
    "            f\"{split_name} class distribution: {class_counts / len(targets)}, {len(targets)}\"\n",
    "        )\n",
    "        if len(class_counts) < 2 or min(class_counts) == 0:\n",
    "            raise ValueError(f\"Imbalanced classes in {split_name} split\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in train_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in val_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in test_loader: {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2347321/2539756025.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 139, 139\n",
    "\n",
    "# Load metadata\n",
    "train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(\"test-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Loading cached prepared DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 20052\n",
      "Length of Balanced Negative Indices: 20052\n",
      "Unique indices: [    30     81     85 ... 401004 401026 401042]\n",
      "Unique targets: [0 1]\n",
      "40104\n",
      "40104\n",
      "Setup Count: {0: 20052, 1: 20052}\n",
      "Train class distribution: [0.49942337 0.50057663], 32083\n",
      "Validation class distribution: [0.50498753 0.49501247], 4010\n",
      "Test class distribution: [0.49962603 0.50037397], 4011\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 32083, max index: 401026\n",
      "Length of val_indices: 4010, max index: 400922\n",
      "Length of test_indices: 4011, max index: 401042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name             | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0  | conv1            | Conv2d             | 19.5 K\n",
      "1  | bn1              | BatchNorm2d        | 512   \n",
      "2  | activation       | Hardswish          | 0     \n",
      "3  | inv_res_blocks   | ModuleList         | 2.2 M \n",
      "4  | dense_block      | DenseBlock         | 2.0 M \n",
      "5  | transition       | TransitionLayer    | 351 K \n",
      "6  | attention        | AttentionBlock     | 215 K \n",
      "7  | avg_pool         | AvgPool2d          | 0     \n",
      "8  | inception        | InceptionBlock     | 406 K \n",
      "9  | inception2       | InceptionBlock     | 526 K \n",
      "10 | attention2       | AttentionBlock     | 280 K \n",
      "11 | gated_res        | GatedResidualBlock | 1.8 M \n",
      "12 | attention3       | AttentionBlock     | 263 K \n",
      "13 | global_avg_pool  | AdaptiveAvgPool2d  | 0     \n",
      "14 | flatten          | Flatten            | 0     \n",
      "15 | fc1              | Linear             | 1.1 M \n",
      "16 | bn_fc1           | BatchNorm1d        | 8.2 K \n",
      "17 | fc2              | Linear             | 8.4 M \n",
      "18 | bn_fc2           | BatchNorm1d        | 4.1 K \n",
      "19 | fc3              | Linear             | 1.0 M \n",
      "20 | bn_fc3           | BatchNorm1d        | 1.0 K \n",
      "21 | fc4              | Linear             | 65.7 K\n",
      "22 | bn_fc4           | BatchNorm1d        | 256   \n",
      "23 | dropout          | Dropout            | 0     \n",
      "24 | metadata_fc1     | Linear             | 172 K \n",
      "25 | metadata_bn1     | BatchNorm1d        | 8.2 K \n",
      "26 | metadata_fc2     | Linear             | 4.2 M \n",
      "27 | metadata_bn2     | BatchNorm1d        | 2.0 K \n",
      "28 | metadata_fc3     | Linear             | 524 K \n",
      "29 | metadata_bn3     | BatchNorm1d        | 1.0 K \n",
      "30 | metadata_fc4     | Linear             | 65.7 K\n",
      "31 | metadata_bn4     | BatchNorm1d        | 256   \n",
      "32 | final_fc         | Linear             | 514   \n",
      "33 | final_activation | Sigmoid            | 0     \n",
      "34 | loss             | CrossEntropyLoss   | 0     \n",
      "35 | auroc            | PartialAUROC       | 0     \n",
      "---------------------------------------------------------\n",
      "23.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 M    Total params\n",
      "94.654    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Number of batches in val_loader: 63\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 502\n",
      "Epoch 0: 100%|██████████| 565/565 [02:30<00:00,  3.75it/s, loss=0.417, v_num=97, train_loss_step=0.501, train_pAUC_step=0.0864, val_loss=0.393, val_pAUC=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 501: val_loss reached 0.39297 (best 0.39297), saving model to \"checkpoints/version_97/gurunet-epoch=00-val_loss=0.39.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 565/565 [02:30<00:00,  3.75it/s, loss=0.403, v_num=97, train_loss_step=0.504, train_pAUC_step=0.133, val_loss=0.379, val_pAUC=0.180, train_loss_epoch=0.434, train_pAUC_epoch=0.160] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1003: val_loss reached 0.37895 (best 0.37895), saving model to \"checkpoints/version_97/gurunet-epoch=01-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 565/565 [02:19<00:00,  4.05it/s, loss=0.398, v_num=97, train_loss_step=0.426, train_pAUC_step=0.109, val_loss=0.385, val_pAUC=0.179, train_loss_epoch=0.406, train_pAUC_epoch=0.171] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1505: val_loss reached 0.38513 (best 0.37895), saving model to \"checkpoints/version_97/gurunet-epoch=02-val_loss=0.39.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 565/565 [02:13<00:00,  4.25it/s, loss=0.391, v_num=97, train_loss_step=0.436, train_pAUC_step=0.177, val_loss=0.381, val_pAUC=0.179, train_loss_epoch=0.397, train_pAUC_epoch=0.173] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2007: val_loss reached 0.38100 (best 0.37895), saving model to \"checkpoints/version_97/gurunet-epoch=03-val_loss=0.38.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 565/565 [02:13<00:00,  4.25it/s, loss=0.382, v_num=97, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.371, val_pAUC=0.181, train_loss_epoch=0.395, train_pAUC_epoch=0.174] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2509: val_loss reached 0.37075 (best 0.37075), saving model to \"checkpoints/version_97/gurunet-epoch=04-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 565/565 [02:13<00:00,  4.24it/s, loss=0.393, v_num=97, train_loss_step=0.531, train_pAUC_step=0.134, val_loss=0.365, val_pAUC=0.181, train_loss_epoch=0.389, train_pAUC_epoch=0.176] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 3011: val_loss reached 0.36454 (best 0.36454), saving model to \"checkpoints/version_97/gurunet-epoch=05-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 565/565 [02:12<00:00,  4.26it/s, loss=0.38, v_num=97, train_loss_step=0.419, train_pAUC_step=0.200, val_loss=0.372, val_pAUC=0.180, train_loss_epoch=0.387, train_pAUC_epoch=0.176]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 3513: val_loss reached 0.37193 (best 0.36454), saving model to \"checkpoints/version_97/gurunet-epoch=06-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 565/565 [02:13<00:00,  4.24it/s, loss=0.377, v_num=97, train_loss_step=0.513, train_pAUC_step=0.172, val_loss=0.367, val_pAUC=0.181, train_loss_epoch=0.384, train_pAUC_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 4015: val_loss reached 0.36670 (best 0.36454), saving model to \"checkpoints/version_97/gurunet-epoch=07-val_loss=0.37.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 565/565 [02:13<00:00,  4.23it/s, loss=0.393, v_num=97, train_loss_step=0.444, train_pAUC_step=0.169, val_loss=0.382, val_pAUC=0.153, train_loss_epoch=0.384, train_pAUC_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 4517: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 565/565 [02:13<00:00,  4.24it/s, loss=0.379, v_num=97, train_loss_step=0.400, train_pAUC_step=0.200, val_loss=0.356, val_pAUC=0.182, train_loss_epoch=0.383, train_pAUC_epoch=0.176] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 5019: val_loss reached 0.35633 (best 0.35633), saving model to \"checkpoints/version_97/gurunet-epoch=09-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 565/565 [02:14<00:00,  4.22it/s, loss=0.384, v_num=97, train_loss_step=0.444, train_pAUC_step=0.160, val_loss=0.362, val_pAUC=0.184, train_loss_epoch=0.380, train_pAUC_epoch=0.178]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 5521: val_loss reached 0.36152 (best 0.35633), saving model to \"checkpoints/version_97/gurunet-epoch=10-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 565/565 [02:14<00:00,  4.20it/s, loss=0.363, v_num=97, train_loss_step=0.403, train_pAUC_step=0.182, val_loss=0.359, val_pAUC=0.182, train_loss_epoch=0.379, train_pAUC_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 6023: val_loss reached 0.35884 (best 0.35633), saving model to \"checkpoints/version_97/gurunet-epoch=11-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 565/565 [02:28<00:00,  3.82it/s, loss=0.37, v_num=97, train_loss_step=0.371, train_pAUC_step=0.188, val_loss=0.357, val_pAUC=0.181, train_loss_epoch=0.379, train_pAUC_epoch=0.177]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 6525: val_loss reached 0.35674 (best 0.35633), saving model to \"checkpoints/version_97/gurunet-epoch=12-val_loss=0.36.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 13: 100%|██████████| 565/565 [02:12<00:00,  4.27it/s, loss=0.365, v_num=97, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.348, val_pAUC=0.185, train_loss_epoch=0.379, train_pAUC_epoch=0.176] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 7027: val_loss reached 0.34759 (best 0.34759), saving model to \"checkpoints/version_97/gurunet-epoch=13-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 565/565 [02:17<00:00,  4.12it/s, loss=0.374, v_num=97, train_loss_step=0.624, train_pAUC_step=0.0682, val_loss=0.348, val_pAUC=0.185, train_loss_epoch=0.367, train_pAUC_epoch=0.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 7529: val_loss reached 0.34781 (best 0.34759), saving model to \"checkpoints/version_97/gurunet-epoch=14-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 565/565 [02:19<00:00,  4.05it/s, loss=0.365, v_num=97, train_loss_step=0.371, train_pAUC_step=0.177, val_loss=0.348, val_pAUC=0.184, train_loss_epoch=0.366, train_pAUC_epoch=0.180]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 8031: val_loss reached 0.34802 (best 0.34759), saving model to \"checkpoints/version_97/gurunet-epoch=15-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 565/565 [02:16<00:00,  4.16it/s, loss=0.357, v_num=97, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.343, val_pAUC=0.183, train_loss_epoch=0.363, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 8533: val_loss reached 0.34319 (best 0.34319), saving model to \"checkpoints/version_97/gurunet-epoch=16-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 565/565 [02:15<00:00,  4.18it/s, loss=0.361, v_num=97, train_loss_step=0.389, train_pAUC_step=0.188, val_loss=0.342, val_pAUC=0.184, train_loss_epoch=0.362, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 9035: val_loss reached 0.34244 (best 0.34244), saving model to \"checkpoints/version_97/gurunet-epoch=17-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 565/565 [02:17<00:00,  4.12it/s, loss=0.366, v_num=97, train_loss_step=0.444, train_pAUC_step=0.187, val_loss=0.348, val_pAUC=0.183, train_loss_epoch=0.362, train_pAUC_epoch=0.180] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 9537: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00019: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 19: 100%|██████████| 565/565 [02:20<00:00,  4.03it/s, loss=0.366, v_num=97, train_loss_step=0.376, train_pAUC_step=0.182, val_loss=0.346, val_pAUC=0.185, train_loss_epoch=0.363, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 10039: val_loss reached 0.34558 (best 0.34244), saving model to \"checkpoints/version_97/gurunet-epoch=19-val_loss=0.35.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 565/565 [02:20<00:00,  4.03it/s, loss=0.348, v_num=97, train_loss_step=0.318, train_pAUC_step=0.200, val_loss=0.346, val_pAUC=0.185, train_loss_epoch=0.361, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 10541: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 565/565 [02:22<00:00,  3.98it/s, loss=0.365, v_num=97, train_loss_step=0.459, train_pAUC_step=0.156, val_loss=0.342, val_pAUC=0.184, train_loss_epoch=0.358, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 11043: val_loss reached 0.34169 (best 0.34169), saving model to \"checkpoints/version_97/gurunet-epoch=21-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 565/565 [02:30<00:00,  3.75it/s, loss=0.364, v_num=97, train_loss_step=0.315, train_pAUC_step=0.200, val_loss=0.346, val_pAUC=0.186, train_loss_epoch=0.358, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 11545: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 565/565 [02:23<00:00,  3.94it/s, loss=0.358, v_num=97, train_loss_step=0.372, train_pAUC_step=0.200, val_loss=0.340, val_pAUC=0.184, train_loss_epoch=0.359, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 12047: val_loss reached 0.34021 (best 0.34021), saving model to \"checkpoints/version_97/gurunet-epoch=23-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 24: 100%|██████████| 565/565 [02:19<00:00,  4.06it/s, loss=0.357, v_num=97, train_loss_step=0.364, train_pAUC_step=0.200, val_loss=0.342, val_pAUC=0.185, train_loss_epoch=0.359, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 12549: val_loss reached 0.34154 (best 0.34021), saving model to \"checkpoints/version_97/gurunet-epoch=24-val_loss=0.34.ckpt\" as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 565/565 [02:18<00:00,  4.09it/s, loss=0.352, v_num=97, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.346, val_pAUC=0.185, train_loss_epoch=0.357, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 13051: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 565/565 [02:25<00:00,  3.88it/s, loss=0.366, v_num=97, train_loss_step=0.414, train_pAUC_step=0.183, val_loss=0.345, val_pAUC=0.184, train_loss_epoch=0.358, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 13553: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 27: 100%|██████████| 565/565 [02:23<00:00,  3.96it/s, loss=0.36, v_num=97, train_loss_step=0.359, train_pAUC_step=0.189, val_loss=0.346, val_pAUC=0.186, train_loss_epoch=0.358, train_pAUC_epoch=0.182]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 14055: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 565/565 [02:23<00:00,  3.96it/s, loss=0.362, v_num=97, train_loss_step=0.519, train_pAUC_step=0.156, val_loss=0.346, val_pAUC=0.186, train_loss_epoch=0.357, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 14557: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 565/565 [02:22<00:00,  3.97it/s, loss=0.371, v_num=97, train_loss_step=0.419, train_pAUC_step=0.0682, val_loss=0.347, val_pAUC=0.186, train_loss_epoch=0.357, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 15059: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00030: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 30: 100%|██████████| 565/565 [02:21<00:00,  3.99it/s, loss=0.357, v_num=97, train_loss_step=0.430, train_pAUC_step=0.177, val_loss=0.345, val_pAUC=0.185, train_loss_epoch=0.358, train_pAUC_epoch=0.182]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 15561: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 565/565 [02:20<00:00,  4.02it/s, loss=0.364, v_num=97, train_loss_step=0.410, train_pAUC_step=0.182, val_loss=0.343, val_pAUC=0.184, train_loss_epoch=0.357, train_pAUC_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 16063: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 565/565 [02:18<00:00,  4.08it/s, loss=0.358, v_num=97, train_loss_step=0.313, train_pAUC_step=0.200, val_loss=0.345, val_pAUC=0.185, train_loss_epoch=0.357, train_pAUC_epoch=0.181] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 16565: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 565/565 [02:28<00:00,  3.81it/s, loss=0.365, v_num=97, train_loss_step=0.506, train_pAUC_step=0.144, val_loss=0.343, val_pAUC=0.185, train_loss_epoch=0.356, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 17067: val_loss was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 565/565 [02:28<00:00,  3.80it/s, loss=0.365, v_num=97, train_loss_step=0.506, train_pAUC_step=0.144, val_loss=0.343, val_pAUC=0.185, train_loss_epoch=0.356, train_pAUC_epoch=0.183]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "torch.cuda.memory.empty_cache()\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"gurunet_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/version_{logger.version}\",\n",
    "    filename=\"gurunet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize your data module\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    # accumulate_grad_batches=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/easy-diffusion/installer_files/env/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Loading cached prepared DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 20052\n",
      "Length of Balanced Negative Indices: 20052\n",
      "Unique indices: [    30     81     85 ... 401004 401026 401042]\n",
      "Unique targets: [0 1]\n",
      "40104\n",
      "40104\n",
      "Setup Count: {0: 20052, 1: 20052}\n",
      "Train class distribution: [0.49942337 0.50057663], 32083\n",
      "Validation class distribution: [0.50498753 0.49501247], 4010\n",
      "Test class distribution: [0.49962603 0.50037397], 4011\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 32083, max index: 401026\n",
      "Length of val_indices: 4010, max index: 400922\n",
      "Length of test_indices: 4011, max index: 401042\n",
      "Number of batches in test_loader: 63\n",
      "Testing: 100%|██████████| 63/63 [00:10<00:00,  5.78it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.3482796549797058,\n",
      " 'test_loss_epoch': 0.3482796549797058,\n",
      " 'test_pAUC': 0.1820291429758072,\n",
      " 'test_pAUC_epoch': 0.1820291429758072}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 63/63 [00:11<00:00,  5.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3482796549797058,\n",
       "  'test_loss_epoch': 0.3482796549797058,\n",
       "  'test_pAUC': 0.1820291429758072,\n",
       "  'test_pAUC_epoch': 0.1820291429758072}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeoned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
