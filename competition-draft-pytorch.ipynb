{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import (Callback, EarlyStopping,\n",
    "                                         LearningRateMonitor, ModelCheckpoint,\n",
    "                                         ProgressBar)\n",
    "from torchmetrics import Precision, Recall, Specificity\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, Subset\n",
    "from torchmetrics import Metric\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "class PartialAUROC(Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_tpr: float = 0.80,\n",
    "        dist_sync_on_step: bool = False,\n",
    "    ):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.min_tpr = min_tpr\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        self.preds.append(preds)\n",
    "        self.target.append(target)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds)\n",
    "        target = torch.cat(self.target)\n",
    "        return self._partial_auroc(target, preds, self.min_tpr)\n",
    "\n",
    "    def _partial_auroc(\n",
    "        self, y_true: torch.Tensor, y_score: torch.Tensor, min_tpr: float\n",
    "    ) -> float:\n",
    "        y_true = torch.abs(y_true - 1)\n",
    "        y_score = -y_score\n",
    "\n",
    "        fpr, tpr, _ = self._roc_curve(y_true, y_score)\n",
    "        max_fpr = 1.0 - min_tpr\n",
    "\n",
    "        # print(f\"Computed FPR: {fpr}\")\n",
    "        # print(f\"Computed TPR: {tpr}\")\n",
    "\n",
    "        if max_fpr == 1:\n",
    "            return self._auc(fpr, tpr)\n",
    "        if max_fpr <= 0 or max_fpr > 1:\n",
    "            raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "\n",
    "        stop = torch.searchsorted(fpr, torch.tensor(max_fpr), right=True)\n",
    "        x_interp = fpr[stop - 1 : stop + 1]\n",
    "        y_interp = tpr[stop - 1 : stop + 1]\n",
    "\n",
    "        # print(f\"x_interp: {x_interp}\")\n",
    "        # print(f\"y_interp: {y_interp}\")\n",
    "\n",
    "        if len(x_interp) == 1:\n",
    "            interp_tpr = y_interp[0]\n",
    "        else:\n",
    "            interp_tpr = y_interp[0] + (max_fpr - x_interp[0]) * (\n",
    "                y_interp[1] - y_interp[0]\n",
    "            ) / (x_interp[1] - x_interp[0])\n",
    "\n",
    "        tpr = torch.cat([tpr[:stop], torch.tensor([interp_tpr])])\n",
    "        fpr = torch.cat([fpr[:stop], torch.tensor([max_fpr])])\n",
    "\n",
    "        partial_auc = self._auc(fpr, tpr)\n",
    "        return partial_auc\n",
    "\n",
    "    def _roc_curve(self, y_true: torch.Tensor, y_score: torch.Tensor):\n",
    "        desc_score_indices = torch.argsort(y_score, descending=True)\n",
    "        y_score = y_score[desc_score_indices]\n",
    "        y_true = y_true[desc_score_indices]\n",
    "\n",
    "        distinct_value_indices = torch.where(torch.diff(y_score))[0]\n",
    "        threshold_idxs = torch.cat(\n",
    "            [distinct_value_indices, torch.tensor([y_true.numel() - 1])]\n",
    "        )\n",
    "\n",
    "        tps = torch.cumsum(y_true, dim=0)[threshold_idxs]\n",
    "        fps = 1 + threshold_idxs - tps\n",
    "        \n",
    "        # Handle the case where there are no positive samples\n",
    "        if tps[-1] == 0:\n",
    "            tpr = torch.zeros_like(tps)\n",
    "        else:\n",
    "            tpr = tps / tps[-1]\n",
    "        \n",
    "        fpr = fps / fps[-1]\n",
    "        thresholds = y_score[threshold_idxs]\n",
    "\n",
    "        # print(f\"tps: {tps}\")\n",
    "        # print(f\"fps: {fps}\")\n",
    "        # print(f\"tpr: {tpr}\")\n",
    "        # print(f\"fpr: {fpr}\")\n",
    "        # print(f\"thresholds: {thresholds}\")\n",
    "\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def _auc(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        if torch.all(y == 0):\n",
    "            print(\"Warning: All TPR values are zero. AUC is undefined.\")\n",
    "            return 0.0\n",
    "\n",
    "        direction = 1\n",
    "        dx = torch.diff(x)\n",
    "        if torch.any(dx < 0):\n",
    "            if torch.all(dx <= 0):\n",
    "                direction = -1\n",
    "            else:\n",
    "                raise ValueError(\"x is neither increasing nor decreasing\")\n",
    "        auc_value = direction * torch.trapz(y, x).item()\n",
    "        # print(f\"Computed AUC: {auc_value}\")\n",
    "        return auc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n",
      "0.19.0+cu121\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers.append(ConvBNActivation(in_channels, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                ConvBNActivation(\n",
    "                    hidden_dim, hidden_dim, stride=stride, groups=hidden_dim\n",
    "                ),\n",
    "                nn.Conv2d(hidden_dim, out_channels, 1, bias=True),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNActivation, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, growth_rate, dropout_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DenseLayer(in_channels + i * growth_rate, growth_rate, dropout_rate)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.cat([x, layer(x)], 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate):\n",
    "        super(DenseLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, 4 * growth_rate, 1, bias=True),\n",
    "            nn.BatchNorm2d(4 * growth_rate),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(4 * growth_rate, growth_rate, 3, padding=1, bias=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "        super(TransitionLayer, self).__init__(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=True),\n",
    "            nn.AvgPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        att = nn.Hardswish()(g + x)\n",
    "        att = nn.Sigmoid()(self.bn3(self.conv3(att)))\n",
    "        return x * att\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "        self.branch1 = ConvBNActivation(in_channels, f1, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f2[0], kernel_size=1),\n",
    "            ConvBNActivation(f2[0], f2[1], kernel_size=3),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNActivation(in_channels, f3[0], kernel_size=1),\n",
    "            ConvBNActivation(f3[0], f3[1], kernel_size=5),\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBNActivation(in_channels, f1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "\n",
    "\n",
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, strides):\n",
    "        super(GatedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=strides,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "        # Add a shortcut connection if input and output dimensions don't match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if strides != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=strides, bias=True\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        gate = nn.Sigmoid()(self.bn3(self.conv3(x)))\n",
    "        x = x * gate\n",
    "        x += residual\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class GuruNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=(139, 139, 3),\n",
    "        metadata_shape=None,\n",
    "        classes=2,\n",
    "    ):\n",
    "        super(GuruNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.metadata_shape = metadata_shape\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.activation = nn.Hardswish()\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        self.inv_res_blocks = nn.ModuleList()\n",
    "        block_params = [\n",
    "            # expand_ratio, filters, strides, repeats\n",
    "            (6, 16, 1, 1),\n",
    "            (6, 24, 2, 2),\n",
    "            (6, 40, 2, 2),\n",
    "            (6, 80, 2, 3),\n",
    "            (6, 112, 1, 3),\n",
    "            (6, 128, 2, 4),\n",
    "            (6, 196, 1, 1),\n",
    "        ]\n",
    "\n",
    "        in_channels = 256\n",
    "        for i, (expand_ratio, filters, strides, repeats) in enumerate(block_params):\n",
    "            for j in range(repeats):\n",
    "                if j > 0:\n",
    "                    strides = 1\n",
    "                self.inv_res_blocks.append(\n",
    "                    InvertedResidualBlock(in_channels, filters, expand_ratio, strides)\n",
    "                )\n",
    "                in_channels = filters\n",
    "\n",
    "        # Dense Block\n",
    "        self.dense_block = DenseBlock(in_channels, num_layers=32, growth_rate=64)\n",
    "        in_channels += 64 * 32  # Update in_channels after dense block\n",
    "\n",
    "        # Transition Layer\n",
    "        self.transition = TransitionLayer(in_channels, compression_factor=0.5)\n",
    "        in_channels = int(in_channels * 0.5)\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention = AttentionBlock(in_channels, 1024)\n",
    "        in_channels = 1024\n",
    "\n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Inception Block\n",
    "        self.inception = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        self.inception2 = InceptionBlock(in_channels, [128, (128, 192), (32, 96)])\n",
    "        in_channels = 128 + 192 + 96 + 128\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention2 = AttentionBlock(in_channels, 512)\n",
    "        in_channels = 512\n",
    "\n",
    "        # Gated Residual Block\n",
    "        self.gated_res = GatedResidualBlock(in_channels, 512, kernel_size=3, strides=2)\n",
    "        in_channels = 512\n",
    "\n",
    "        # Attention Block\n",
    "        self.attention3 = AttentionBlock(in_channels, 256)\n",
    "        in_channels = 256\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_channels, 4096)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.bn_fc3 = nn.BatchNorm1d(1024)\n",
    "        self.fc4 = nn.Linear(1024, 256)\n",
    "        self.bn_fc4 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.metadata_fc1 = nn.Linear(41, 4096)\n",
    "        self.metadata_bn1 = nn.BatchNorm1d(4096)\n",
    "        self.metadata_fc2 = nn.Linear(4096, 1024)\n",
    "        self.metadata_bn2 = nn.BatchNorm1d(1024)\n",
    "        self.metadata_fc3 = nn.Linear(1024, 512)\n",
    "        self.metadata_bn3 = nn.BatchNorm1d(512)\n",
    "        self.metadata_fc4 = nn.Linear(512, 256)\n",
    "        self.metadata_bn4 = nn.BatchNorm1d(256)\n",
    "        self.final_fc = nn.Linear(256 + 256, classes)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        self.scaler = GradScaler()\n",
    "        self.loss = self.loss = nn.CrossEntropyLoss()\n",
    "        self.auroc = PartialAUROC(min_tpr=0.8)\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        x = self.activation(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Inverted Residual Blocks\n",
    "        for block in self.inv_res_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Dense Block\n",
    "        x = self.dense_block(x)\n",
    "\n",
    "        # Transition Layer\n",
    "        x = self.transition(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Average Pooling\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # Inception Block\n",
    "        x = self.inception(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        # Gated Residual Block\n",
    "        x = self.gated_res(x)\n",
    "\n",
    "        # Attention Block\n",
    "        x = self.attention3(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn_fc4(self.fc4(x)))\n",
    "\n",
    "        metadata = self.activation(self.metadata_bn1(self.metadata_fc1(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn2(self.metadata_fc2(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn3(self.metadata_fc3(metadata)))\n",
    "        metadata = self.dropout(metadata)\n",
    "        metadata = self.activation(self.metadata_bn4(self.metadata_fc4(metadata)))\n",
    "\n",
    "        x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.final_fc(x)\n",
    "        # Apply sigmoid to ensure output is between 0 and 1\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)  # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        # Use class 1 probability\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (images, metadata), targets = batch\n",
    "        outputs = self(images, metadata)\n",
    "        loss = self.loss(outputs, targets)  # targets is already one-hot encoded\n",
    "\n",
    "        # Get the probability of the positive class\n",
    "        pos_probs = outputs[:, 1].float().cpu()\n",
    "\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        targets_binary = targets[:, 1].int().cpu()\n",
    "        rocauc = self.auroc(pos_probs, targets_binary)\n",
    "\n",
    "        self.log(\n",
    "            \"test_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_pAUC\",\n",
    "            rocauc,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.NAdam(\n",
    "            self.parameters(), lr=0.001, momentum_decay=0.5, weight_decay=1e-5\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def prepare_df(\n",
    "    df: pd.DataFrame,\n",
    "    is_training=True,\n",
    "):\n",
    "    print(\"Preparing DataFrame...\")\n",
    "    df_hash = hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()\n",
    "    cache_dir = \"./cache\"\n",
    "    param_string = f\"{is_training}\"\n",
    "    cache_file = os.path.join(cache_dir, f\"prepared_df_{df_hash}_{param_string}.joblib\")\n",
    "\n",
    "    # Check if cached version exists\n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading cached prepared DataFrame...\")\n",
    "        return joblib.load(cache_file)\n",
    "    start_time = time.time()\n",
    "\n",
    "    drop_columns_train = [\n",
    "        \"lesion_id\",\n",
    "        \"iddx_full\",\n",
    "        \"iddx_1\",\n",
    "        \"iddx_2\",\n",
    "        \"iddx_3\",\n",
    "        \"iddx_4\",\n",
    "        \"iddx_5\",\n",
    "        \"mel_mitotic_index\",\n",
    "        \"mel_thick_mm\",\n",
    "        \"tbp_lv_dnn_lesion_confidence\",\n",
    "    ]\n",
    "    drop_columns_test = [\"attribution\", \"copyright_license\"]\n",
    "\n",
    "    if is_training:\n",
    "        df.drop(drop_columns_train, axis=1, inplace=True)\n",
    "    df.drop(drop_columns_test, axis=1, inplace=True)\n",
    "    target_columns = [\"target\"] if is_training else []\n",
    "    X = df.drop(target_columns + [\"isic_id\"], axis=1)\n",
    "    y = torch.tensor(df[\"target\"].values, dtype=torch.int8) if is_training else None\n",
    "\n",
    "    # Separate features by type\n",
    "    integer_features = X.select_dtypes(include=[\"int64\", \"int32\", \"int16\"]).columns\n",
    "    float_features = X.select_dtypes(include=[\"float64\", \"float32\", \"float16\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Handle NaN values and type conversions\n",
    "    for feature in float_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].mean()).astype(\"float32\")\n",
    "\n",
    "    for feature in integer_features:\n",
    "        X[feature] = X[feature].fillna(X[feature].median()).astype(\"int32\")\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        X[feature] = X[feature].astype(str).fillna(\"Unknown\")\n",
    "        X[feature] = pd.Categorical(X[feature]).codes\n",
    "\n",
    "    # Standardize all numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "    X_final = X_scaled\n",
    "\n",
    "    # Final check for any remaining NaN values\n",
    "    assert (\n",
    "        not X_final.isnull().any().any()\n",
    "    ), \"There are still NaN values in the processed data\"\n",
    "\n",
    "    print(\"Data shape after preprocessing:\", X_final.shape)\n",
    "    print(\"Number of NaN values after preprocessing:\", X_final.isnull().sum().sum())\n",
    "\n",
    "    if is_training:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "    print(f\"DataFrame prepared in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Metadata Shape: {X_final.shape}\")\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    joblib.dump((X_final, y, df[\"isic_id\"]), cache_file)\n",
    "    return X_final, y, df[\"isic_id\"]\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, metadata_df, is_training=True, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.metadata_df = metadata_df\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.X, self.y, self.image_names = prepare_df(metadata_df, is_training)\n",
    "        self.metadata_shape = self.X.shape\n",
    "        self.train_transform = get_transforms(is_training=True)\n",
    "        self.test_transform = get_transforms(is_training=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, tuple):\n",
    "            idx, augment = idx\n",
    "        else:\n",
    "            augment = False\n",
    "\n",
    "        isic_id = self.image_names[idx]\n",
    "        metadata = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf:\n",
    "            image_data = hdf[str(isic_id)][()]\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        if self.is_training and augment:\n",
    "            image = self.train_transform(image)\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            target = self.y[idx]\n",
    "            target_long = target.long()\n",
    "            del target\n",
    "            target_one_hot = nn.functional.one_hot(target_long, num_classes=2).float()\n",
    "            return (image, metadata), target_one_hot\n",
    "        else:\n",
    "            return (image, metadata), isic_id\n",
    "\n",
    "\n",
    "# Create separate transforms for training and validation\n",
    "def get_transforms(is_training=True):\n",
    "    # Define augmentation parameters\n",
    "    ROTATION_RANGE = 180\n",
    "    BRIGHTNESS_RANGE = (0.99, 1.01)\n",
    "    CONTRAST_RANGE = (0.99, 1.01)\n",
    "    SATURATION_RANGE = (0.9, 1.1)\n",
    "    HUE_RANGE = (-0.001, 0.001)\n",
    "    base_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((139, 139), antialias=True),\n",
    "    ]\n",
    "\n",
    "    if is_training:\n",
    "        train_transforms = [\n",
    "            transforms.RandomRotation(\n",
    "                degrees=ROTATION_RANGE,\n",
    "                interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "            ),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=BRIGHTNESS_RANGE,\n",
    "                contrast=CONTRAST_RANGE,\n",
    "                saturation=SATURATION_RANGE,\n",
    "                hue=HUE_RANGE,\n",
    "            ),\n",
    "        ]\n",
    "        return transforms.Compose(train_transforms + base_transforms)\n",
    "    else:\n",
    "        return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_hdf5_path: str,\n",
    "        test_hdf5_path: str,\n",
    "        train_metadata_df: pd.DataFrame,\n",
    "        test_metadata_df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_hdf5_path = train_hdf5_path\n",
    "        self.test_hdf5_path = test_hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_metadata_df = train_metadata_df\n",
    "        self.test_metadata_df = test_metadata_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ISICDataset(\n",
    "            self.train_hdf5_path,\n",
    "            self.train_metadata_df,\n",
    "            True,\n",
    "            transform=get_transforms(is_training=True),\n",
    "        )\n",
    "        self.metadata_shape = full_dataset.metadata_shape\n",
    "        \n",
    "        # Get targets for stratification\n",
    "        targets = self.train_metadata_df[\"target\"].values\n",
    "        balanced_indices = self.balance_dataset(np.arange(len(full_dataset)), targets)\n",
    "        \n",
    "        # Extract actual indices and augmentation flags\n",
    "        balanced_indices, augmentation_flags = zip(*balanced_indices)\n",
    "        balanced_indices = np.array(balanced_indices)\n",
    "        \n",
    "        augmentation_flags = np.array(augmentation_flags)\n",
    "        balanced_targets = targets[balanced_indices]\n",
    "\n",
    "        print(f\"Unique indices: {np.unique(balanced_indices)}\")\n",
    "        print(f\"Unique targets: {np.unique(balanced_targets)}\")\n",
    "        \n",
    "        print(len(balanced_indices))\n",
    "        print(len(balanced_targets))\n",
    "        \n",
    "        unique, counts = np.unique(balanced_targets, return_counts=True)\n",
    "        print(f\"Setup Count: {dict(zip(unique, counts))}\")\n",
    "        # Perform stratified split\n",
    "        train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "            balanced_indices,\n",
    "            balanced_targets,\n",
    "            test_size=0.2,\n",
    "            # stratify=balanced_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        val_indices, test_indices, val_targets, test_targets = train_test_split(\n",
    "            temp_indices,\n",
    "            temp_targets,\n",
    "            test_size=0.5,\n",
    "            # stratify=temp_targets,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Create subset datasets\n",
    "        if stage in [\"fit\", \"validate\", \"test\"]:\n",
    "            self.train_dataset = Subset(full_dataset, train_indices)\n",
    "            self.val_dataset = Subset(full_dataset, val_indices)\n",
    "            self.test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "        # Check for class balance\n",
    "        self._check_class_balance(train_targets.flatten(), \"Train\")\n",
    "        self._check_class_balance(val_targets.flatten(), \"Validation\")\n",
    "        self._check_class_balance(test_targets.flatten(), \"Test\")\n",
    "\n",
    "        print(f\"Length of full_dataset: {len(full_dataset)}\")\n",
    "        print(\n",
    "            f\"Length of train_indices: {len(train_indices)}, max index: {max(train_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of val_indices: {len(val_indices)}, max index: {max(val_indices)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of test_indices: {len(test_indices)}, max index: {max(test_indices)}\"\n",
    "        )\n",
    "\n",
    "    def balance_dataset(self, indices, targets):\n",
    "        np.random.seed(42)\n",
    "        positive_indices = indices[targets == 1]\n",
    "        negative_indices = indices[targets == 0]\n",
    "\n",
    "        num_positive_samples = len(positive_indices)\n",
    "        num_negative_samples = len(negative_indices)\n",
    "\n",
    "        print(f\"Number of Positive Samples: {num_positive_samples}\")\n",
    "        print(f\"Number of Negative Samples: {num_negative_samples}\")\n",
    "\n",
    "        # Determine the number of samples for each class\n",
    "        num_samples = int(np.mean([num_positive_samples, num_negative_samples]))//50\n",
    "\n",
    "        # Upsample positive indices\n",
    "        upsampled_positive_indices = np.random.choice(\n",
    "            positive_indices, size=num_samples // 2, replace=True\n",
    "        )\n",
    "\n",
    "        # Downsample negative indices\n",
    "        downsampled_negative_indices = np.random.choice(\n",
    "            negative_indices, size=num_samples // 2, replace=False\n",
    "        )\n",
    "\n",
    "        # Add augmentation flag\n",
    "        balanced_indices = [(idx, True) for idx in upsampled_positive_indices] + [\n",
    "            (idx, False) for idx in downsampled_negative_indices\n",
    "        ]\n",
    "\n",
    "        np.random.shuffle(balanced_indices)\n",
    "\n",
    "        print(f\"Length of Balanced Positive Indices: {len(upsampled_positive_indices)}\")\n",
    "        print(\n",
    "            f\"Length of Balanced Negative Indices: {len(downsampled_negative_indices)}\"\n",
    "        )\n",
    "\n",
    "        return balanced_indices\n",
    "\n",
    "    def _check_class_balance(self, targets, split_name):\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(\n",
    "            f\"{split_name} class distribution: {class_counts / len(targets)}, {len(targets)}\"\n",
    "        )\n",
    "        if len(class_counts) < 2 or min(class_counts) == 0:\n",
    "            raise ValueError(f\"Imbalanced classes in {split_name} split\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in train_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in val_loader: {len(data_loader)}\")\n",
    "        return data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        print(f\"Number of batches in test_loader: {len(data_loader)}\")\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_518395/2539756025.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "img_height, img_width = 139, 139\n",
    "\n",
    "# Load metadata\n",
    "train_metadata_df = pd.read_csv(\"train-metadata.csv\")\n",
    "test_metadata_df = pd.read_csv(\"test-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/miniconda3/envs/isic/lib/python3.12/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Loading cached prepared DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/miniconda3/envs/isic/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 2005\n",
      "Length of Balanced Negative Indices: 2005\n",
      "Unique indices: [   302    305    387 ... 400775 400922 400991]\n",
      "Unique targets: [0 1]\n",
      "4010\n",
      "4010\n",
      "Setup Count: {np.int64(0): np.int64(2005), np.int64(1): np.int64(2005)}\n",
      "Train class distribution: [0.50187032 0.49812968], 3208\n",
      "Validation class distribution: [0.52119701 0.47880299], 401\n",
      "Test class distribution: [0.4638404 0.5361596], 401\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 3208, max index: 400991\n",
      "Length of val_indices: 401, max index: 400922\n",
      "Length of test_indices: 401, max index: 400922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/miniconda3/envs/isic/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "   | Name             | Type               | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | conv1            | Conv2d             | 19.5 K | train\n",
      "1  | bn1              | BatchNorm2d        | 512    | train\n",
      "2  | activation       | Hardswish          | 0      | train\n",
      "3  | inv_res_blocks   | ModuleList         | 2.2 M  | train\n",
      "4  | dense_block      | DenseBlock         | 14.6 M | train\n",
      "5  | transition       | TransitionLayer    | 2.5 M  | train\n",
      "6  | attention        | AttentionBlock     | 2.3 M  | train\n",
      "7  | avg_pool         | AvgPool2d          | 0      | train\n",
      "8  | inception        | InceptionBlock     | 726 K  | train\n",
      "9  | inception2       | InceptionBlock     | 526 K  | train\n",
      "10 | attention2       | AttentionBlock     | 560 K  | train\n",
      "11 | gated_res        | GatedResidualBlock | 3.2 M  | train\n",
      "12 | attention3       | AttentionBlock     | 263 K  | train\n",
      "13 | global_avg_pool  | AdaptiveAvgPool2d  | 0      | train\n",
      "14 | flatten          | Flatten            | 0      | train\n",
      "15 | fc1              | Linear             | 1.1 M  | train\n",
      "16 | bn_fc1           | BatchNorm1d        | 8.2 K  | train\n",
      "17 | fc2              | Linear             | 8.4 M  | train\n",
      "18 | bn_fc2           | BatchNorm1d        | 4.1 K  | train\n",
      "19 | fc3              | Linear             | 2.1 M  | train\n",
      "20 | bn_fc3           | BatchNorm1d        | 2.0 K  | train\n",
      "21 | fc4              | Linear             | 262 K  | train\n",
      "22 | bn_fc4           | BatchNorm1d        | 512    | train\n",
      "23 | dropout          | Dropout            | 0      | train\n",
      "24 | metadata_fc1     | Linear             | 172 K  | train\n",
      "25 | metadata_bn1     | BatchNorm1d        | 8.2 K  | train\n",
      "26 | metadata_fc2     | Linear             | 4.2 M  | train\n",
      "27 | metadata_bn2     | BatchNorm1d        | 2.0 K  | train\n",
      "28 | metadata_fc3     | Linear             | 524 K  | train\n",
      "29 | metadata_bn3     | BatchNorm1d        | 1.0 K  | train\n",
      "30 | metadata_fc4     | Linear             | 131 K  | train\n",
      "31 | metadata_bn4     | BatchNorm1d        | 512    | train\n",
      "32 | final_fc         | Linear             | 1.0 K  | train\n",
      "33 | final_activation | Sigmoid            | 0      | train\n",
      "34 | loss             | CrossEntropyLoss   | 0      | train\n",
      "35 | auroc            | PartialAUROC       | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "43.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.7 M    Total params\n",
      "174.626   Total estimated model params size (MB)\n",
      "573       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Number of batches in val_loader: 7\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Warning: All TPR values are zero. AUC is undefined.\n",
      "Number of batches in train_loader: 51                                      \n",
      "Epoch 0: 100%|██████████| 51/51 [00:29<00:00,  1.73it/s, v_num=145, train_loss_step=0.537, train_pAUC_step=0.150, val_loss=0.476, val_pAUC=0.149, train_loss_epoch=0.554, train_pAUC_epoch=0.110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 9: 'val_pAUC' reached 0.14868 (best 0.14868), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=00-val_pAUC=0.14868.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 51/51 [00:26<00:00,  1.90it/s, v_num=145, train_loss_step=0.343, train_pAUC_step=0.200, val_loss=0.450, val_pAUC=0.154, train_loss_epoch=0.470, train_pAUC_epoch=0.149] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 18: 'val_pAUC' reached 0.15366 (best 0.15366), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=01-val_pAUC=0.15366.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 51/51 [00:27<00:00,  1.87it/s, v_num=145, train_loss_step=0.331, train_pAUC_step=0.200, val_loss=0.442, val_pAUC=0.162, train_loss_epoch=0.444, train_pAUC_epoch=0.161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 27: 'val_pAUC' reached 0.16240 (best 0.16240), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=02-val_pAUC=0.16240.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 51/51 [00:27<00:00,  1.84it/s, v_num=145, train_loss_step=0.615, train_pAUC_step=0.200, val_loss=0.422, val_pAUC=0.170, train_loss_epoch=0.425, train_pAUC_epoch=0.167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 36: 'val_pAUC' reached 0.17036 (best 0.17036), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=03-val_pAUC=0.17036.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 51/51 [00:24<00:00,  2.05it/s, v_num=145, train_loss_step=0.570, train_pAUC_step=0.200, val_loss=0.417, val_pAUC=0.172, train_loss_epoch=0.423, train_pAUC_epoch=0.168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 45: 'val_pAUC' reached 0.17171 (best 0.17171), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=04-val_pAUC=0.17171.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 51/51 [00:19<00:00,  2.55it/s, v_num=145, train_loss_step=0.414, train_pAUC_step=0.200, val_loss=0.421, val_pAUC=0.165, train_loss_epoch=0.419, train_pAUC_epoch=0.168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 54: 'val_pAUC' reached 0.16500 (best 0.17171), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=05-val_pAUC=0.16500.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 51/51 [00:19<00:00,  2.58it/s, v_num=145, train_loss_step=0.326, train_pAUC_step=0.200, val_loss=0.419, val_pAUC=0.165, train_loss_epoch=0.409, train_pAUC_epoch=0.172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 63: 'val_pAUC' reached 0.16507 (best 0.17171), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=06-val_pAUC=0.16507.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 51/51 [00:32<00:00,  1.55it/s, v_num=145, train_loss_step=0.606, train_pAUC_step=0.100, val_loss=0.412, val_pAUC=0.164, train_loss_epoch=0.413, train_pAUC_epoch=0.171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 72: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 51/51 [00:30<00:00,  1.66it/s, v_num=145, train_loss_step=0.318, train_pAUC_step=0.200, val_loss=0.412, val_pAUC=0.171, train_loss_epoch=0.405, train_pAUC_epoch=0.171] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 81: 'val_pAUC' reached 0.17070 (best 0.17171), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=08-val_pAUC=0.17070.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:27<00:00,  1.85it/s, v_num=145, train_loss_step=0.417, train_pAUC_step=0.200, val_loss=0.400, val_pAUC=0.171, train_loss_epoch=0.403, train_pAUC_epoch=0.174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 90: 'val_pAUC' reached 0.17094 (best 0.17171), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=09-val_pAUC=0.17094.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 51/51 [00:27<00:00,  1.82it/s, v_num=145, train_loss_step=0.429, train_pAUC_step=0.200, val_loss=0.398, val_pAUC=0.172, train_loss_epoch=0.401, train_pAUC_epoch=0.174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 99: 'val_pAUC' reached 0.17245 (best 0.17245), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=10-val_pAUC=0.17245.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 51/51 [00:25<00:00,  2.01it/s, v_num=145, train_loss_step=0.581, train_pAUC_step=0.100, val_loss=0.399, val_pAUC=0.174, train_loss_epoch=0.398, train_pAUC_epoch=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 108: 'val_pAUC' reached 0.17398 (best 0.17398), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=11-val_pAUC=0.17398.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 51/51 [00:20<00:00,  2.45it/s, v_num=145, train_loss_step=0.563, train_pAUC_step=0.133, val_loss=0.394, val_pAUC=0.177, train_loss_epoch=0.400, train_pAUC_epoch=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 117: 'val_pAUC' reached 0.17693 (best 0.17693), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=12-val_pAUC=0.17693.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 51/51 [00:20<00:00,  2.48it/s, v_num=145, train_loss_step=0.709, train_pAUC_step=0.100, val_loss=0.394, val_pAUC=0.172, train_loss_epoch=0.394, train_pAUC_epoch=0.178]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 126: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 51/51 [00:20<00:00,  2.50it/s, v_num=145, train_loss_step=0.324, train_pAUC_step=0.200, val_loss=0.400, val_pAUC=0.175, train_loss_epoch=0.399, train_pAUC_epoch=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 135: 'val_pAUC' reached 0.17477 (best 0.17693), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=14-val_pAUC=0.17477.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 51/51 [00:20<00:00,  2.44it/s, v_num=145, train_loss_step=0.440, train_pAUC_step=0.200, val_loss=0.391, val_pAUC=0.175, train_loss_epoch=0.390, train_pAUC_epoch=0.179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 144: 'val_pAUC' reached 0.17460 (best 0.17693), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=15-val_pAUC=0.17460.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|          | 0/51 [00:00<?, ?it/s, v_num=145, train_loss_step=0.440, train_pAUC_step=0.200, val_loss=0.391, val_pAUC=0.175, train_loss_epoch=0.390, train_pAUC_epoch=0.179]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but the required `min_epochs=100` or `min_steps=None` has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  98%|█████████▊| 50/51 [00:18<00:00,  2.65it/s, v_num=145, train_loss_step=0.370, train_pAUC_step=0.173, val_loss=0.391, val_pAUC=0.175, train_loss_epoch=0.390, train_pAUC_epoch=0.179]Warning: All TPR values are zero. AUC is undefined.\n",
      "Epoch 16: 100%|██████████| 51/51 [00:20<00:00,  2.49it/s, v_num=145, train_loss_step=0.890, train_pAUC_step=0.000, val_loss=0.396, val_pAUC=0.177, train_loss_epoch=0.389, train_pAUC_epoch=0.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 153: 'val_pAUC' reached 0.17688 (best 0.17693), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=16-val_pAUC=0.17688.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 51/51 [00:21<00:00,  2.42it/s, v_num=145, train_loss_step=0.396, train_pAUC_step=0.200, val_loss=0.396, val_pAUC=0.172, train_loss_epoch=0.399, train_pAUC_epoch=0.175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 162: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 51/51 [00:21<00:00,  2.39it/s, v_num=145, train_loss_step=0.317, train_pAUC_step=0.200, val_loss=0.392, val_pAUC=0.175, train_loss_epoch=0.384, train_pAUC_epoch=0.179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 171: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 51/51 [00:20<00:00,  2.47it/s, v_num=145, train_loss_step=0.413, train_pAUC_step=0.200, val_loss=0.386, val_pAUC=0.180, train_loss_epoch=0.393, train_pAUC_epoch=0.177]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 180: 'val_pAUC' reached 0.17989 (best 0.17989), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=19-val_pAUC=0.17989.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 51/51 [00:20<00:00,  2.44it/s, v_num=145, train_loss_step=0.569, train_pAUC_step=0.120, val_loss=0.398, val_pAUC=0.175, train_loss_epoch=0.387, train_pAUC_epoch=0.180]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 189: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 51/51 [00:20<00:00,  2.44it/s, v_num=145, train_loss_step=0.567, train_pAUC_step=0.0167, val_loss=0.380, val_pAUC=0.178, train_loss_epoch=0.390, train_pAUC_epoch=0.178]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 198: 'val_pAUC' reached 0.17782 (best 0.17989), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=21-val_pAUC=0.17782.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  98%|█████████▊| 50/51 [00:19<00:00,  2.59it/s, v_num=145, train_loss_step=0.386, train_pAUC_step=0.158, val_loss=0.380, val_pAUC=0.178, train_loss_epoch=0.390, train_pAUC_epoch=0.178] Warning: All TPR values are zero. AUC is undefined.\n",
      "Epoch 22: 100%|██████████| 51/51 [00:20<00:00,  2.43it/s, v_num=145, train_loss_step=0.676, train_pAUC_step=0.000, val_loss=0.377, val_pAUC=0.180, train_loss_epoch=0.381, train_pAUC_epoch=0.181]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 207: 'val_pAUC' reached 0.17968 (best 0.17989), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=22-val_pAUC=0.17968.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 51/51 [00:21<00:00,  2.41it/s, v_num=145, train_loss_step=0.507, train_pAUC_step=0.133, val_loss=0.385, val_pAUC=0.180, train_loss_epoch=0.377, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 216: 'val_pAUC' reached 0.18023 (best 0.18023), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=23-val_pAUC=0.18023.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 51/51 [00:21<00:00,  2.42it/s, v_num=145, train_loss_step=0.438, train_pAUC_step=0.133, val_loss=0.379, val_pAUC=0.182, train_loss_epoch=0.378, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 225: 'val_pAUC' reached 0.18247 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=24-val_pAUC=0.18247.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 51/51 [00:21<00:00,  2.41it/s, v_num=145, train_loss_step=0.554, train_pAUC_step=0.200, val_loss=0.384, val_pAUC=0.181, train_loss_epoch=0.376, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 234: 'val_pAUC' reached 0.18085 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=25-val_pAUC=0.18085.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 51/51 [00:21<00:00,  2.41it/s, v_num=145, train_loss_step=0.418, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 243: 'val_pAUC' reached 0.18106 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=26-val_pAUC=0.18106.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 51/51 [00:21<00:00,  2.38it/s, v_num=145, train_loss_step=0.318, train_pAUC_step=0.200, val_loss=0.379, val_pAUC=0.180, train_loss_epoch=0.378, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 252: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 51/51 [00:21<00:00,  2.40it/s, v_num=145, train_loss_step=0.314, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.180, train_loss_epoch=0.379, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 261: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 51/51 [00:21<00:00,  2.40it/s, v_num=145, train_loss_step=0.542, train_pAUC_step=0.200, val_loss=0.383, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 270: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 51/51 [00:21<00:00,  2.41it/s, v_num=145, train_loss_step=0.365, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.181, train_loss_epoch=0.377, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 279: 'val_pAUC' reached 0.18117 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=30-val_pAUC=0.18117.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 51/51 [00:21<00:00,  2.40it/s, v_num=145, train_loss_step=0.317, train_pAUC_step=0.200, val_loss=0.386, val_pAUC=0.179, train_loss_epoch=0.376, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 288: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 51/51 [00:21<00:00,  2.40it/s, v_num=145, train_loss_step=0.328, train_pAUC_step=0.200, val_loss=0.383, val_pAUC=0.179, train_loss_epoch=0.373, train_pAUC_epoch=0.185]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 297: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.181, train_loss_epoch=0.369, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 306: 'val_pAUC' reached 0.18117 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=33-val_pAUC=0.18117.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.449, train_pAUC_step=0.200, val_loss=0.383, val_pAUC=0.181, train_loss_epoch=0.380, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 315: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 51/51 [00:21<00:00,  2.38it/s, v_num=145, train_loss_step=0.433, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 324: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 51/51 [00:21<00:00,  2.40it/s, v_num=145, train_loss_step=0.601, train_pAUC_step=0.100, val_loss=0.382, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 333: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.497, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.377, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 342: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 51/51 [00:21<00:00,  2.37it/s, v_num=145, train_loss_step=0.673, train_pAUC_step=0.120, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.185]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 351: 'val_pAUC' reached 0.18132 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=38-val_pAUC=0.18132.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.410, train_pAUC_step=0.200, val_loss=0.379, val_pAUC=0.181, train_loss_epoch=0.375, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 360: 'val_pAUC' reached 0.18120 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=39-val_pAUC=0.18120.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.334, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.372, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 369: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 51/51 [00:21<00:00,  2.34it/s, v_num=145, train_loss_step=0.558, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 378: 'val_pAUC' reached 0.18138 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=41-val_pAUC=0.18138.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 51/51 [00:21<00:00,  2.35it/s, v_num=145, train_loss_step=0.474, train_pAUC_step=0.200, val_loss=0.379, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 387: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 51/51 [00:22<00:00,  2.28it/s, v_num=145, train_loss_step=0.316, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.180, train_loss_epoch=0.373, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 396: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 51/51 [00:21<00:00,  2.38it/s, v_num=145, train_loss_step=0.323, train_pAUC_step=0.200, val_loss=0.378, val_pAUC=0.182, train_loss_epoch=0.370, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 405: 'val_pAUC' reached 0.18183 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=44-val_pAUC=0.18183.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 51/51 [00:21<00:00,  2.35it/s, v_num=145, train_loss_step=0.469, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.181, train_loss_epoch=0.371, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 414: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 51/51 [00:21<00:00,  2.35it/s, v_num=145, train_loss_step=0.348, train_pAUC_step=0.200, val_loss=0.379, val_pAUC=0.181, train_loss_epoch=0.371, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 423: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 51/51 [00:21<00:00,  2.37it/s, v_num=145, train_loss_step=0.318, train_pAUC_step=0.200, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.375, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 432: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 51/51 [00:21<00:00,  2.33it/s, v_num=145, train_loss_step=0.584, train_pAUC_step=0.100, val_loss=0.378, val_pAUC=0.180, train_loss_epoch=0.373, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 441: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 51/51 [00:22<00:00,  2.32it/s, v_num=145, train_loss_step=0.358, train_pAUC_step=0.200, val_loss=0.383, val_pAUC=0.180, train_loss_epoch=0.373, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 450: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 51/51 [00:22<00:00,  2.27it/s, v_num=145, train_loss_step=0.434, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.180, train_loss_epoch=0.370, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, global step 459: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 51/51 [00:21<00:00,  2.34it/s, v_num=145, train_loss_step=0.458, train_pAUC_step=0.150, val_loss=0.379, val_pAUC=0.182, train_loss_epoch=0.379, train_pAUC_epoch=0.181]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, global step 468: 'val_pAUC' reached 0.18150 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=51-val_pAUC=0.18150.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 51/51 [00:21<00:00,  2.34it/s, v_num=145, train_loss_step=0.335, train_pAUC_step=0.200, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52, global step 477: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 51/51 [00:22<00:00,  2.23it/s, v_num=145, train_loss_step=0.634, train_pAUC_step=0.133, val_loss=0.383, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 486: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 51/51 [00:21<00:00,  2.36it/s, v_num=145, train_loss_step=0.422, train_pAUC_step=0.200, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.377, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 495: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 51/51 [00:22<00:00,  2.30it/s, v_num=145, train_loss_step=0.439, train_pAUC_step=0.133, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.372, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 504: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 51/51 [00:21<00:00,  2.34it/s, v_num=145, train_loss_step=0.491, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.377, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56, global step 513: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 51/51 [00:22<00:00,  2.30it/s, v_num=145, train_loss_step=0.806, train_pAUC_step=0.100, val_loss=0.378, val_pAUC=0.181, train_loss_epoch=0.376, train_pAUC_epoch=0.182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 522: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 51/51 [00:22<00:00,  2.28it/s, v_num=145, train_loss_step=0.332, train_pAUC_step=0.200, val_loss=0.382, val_pAUC=0.180, train_loss_epoch=0.376, train_pAUC_epoch=0.181]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 531: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 51/51 [00:22<00:00,  2.29it/s, v_num=145, train_loss_step=0.443, train_pAUC_step=0.0667, val_loss=0.381, val_pAUC=0.180, train_loss_epoch=0.374, train_pAUC_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59, global step 540: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 51/51 [00:22<00:00,  2.30it/s, v_num=145, train_loss_step=0.335, train_pAUC_step=0.200, val_loss=0.380, val_pAUC=0.182, train_loss_epoch=0.373, train_pAUC_epoch=0.183] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 549: 'val_pAUC' reached 0.18152 (best 0.18247), saving model to '/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_145/gurunet-epoch=60-val_pAUC=0.18152.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 51/51 [00:22<00:00,  2.27it/s, v_num=145, train_loss_step=0.324, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 558: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 51/51 [00:22<00:00,  2.27it/s, v_num=145, train_loss_step=0.451, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.376, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 567: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 51/51 [00:22<00:00,  2.27it/s, v_num=145, train_loss_step=0.356, train_pAUC_step=0.200, val_loss=0.381, val_pAUC=0.181, train_loss_epoch=0.374, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 576: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 51/51 [00:22<00:00,  2.29it/s, v_num=145, train_loss_step=0.369, train_pAUC_step=0.200, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, global step 585: 'val_pAUC' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65:  98%|█████████▊| 50/51 [00:20<00:00,  2.43it/s, v_num=145, train_loss_step=0.377, train_pAUC_step=0.187, val_loss=0.380, val_pAUC=0.181, train_loss_epoch=0.373, train_pAUC_epoch=0.183]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m skip_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_training:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/amp.py:78\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m skip_unscaling \u001b[38;5;241m=\u001b[39m closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 337\u001b[0m, in \u001b[0;36mGuruNet.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Convert one-hot encoded targets to binary labels\u001b[39;00m\n\u001b[1;32m    336\u001b[0m targets_binary \u001b[38;5;241m=\u001b[39m targets[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m--> 337\u001b[0m rocauc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauroc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_binary\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use class 1 probability\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    341\u001b[0m     loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    348\u001b[0m     rocauc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    352\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torchmetrics/metric.py:310\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Metric shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be synced when performing ``forward``. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_update \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_state_update \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_on_step:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_full_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torchmetrics/metric.py:342\u001b[0m, in \u001b[0;36mMetric._forward_full_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 342\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# restore context\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/isic/lib/python3.12/site-packages/torchmetrics/metric.py:633\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[1;32m    629\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    630\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[1;32m    631\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[1;32m    632\u001b[0m ):\n\u001b[0;32m--> 633\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;66;03m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     value \u001b[38;5;241m=\u001b[39m apply_to_collection(value, Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mclone())\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mPartialAUROC.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreds)\n\u001b[1;32m     38\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_auroc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_tpr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m, in \u001b[0;36mPartialAUROC._partial_auroc\u001b[0;34m(self, y_true, y_score, min_tpr)\u001b[0m\n\u001b[1;32m     66\u001b[0m     interp_tpr \u001b[38;5;241m=\u001b[39m y_interp[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     interp_tpr \u001b[38;5;241m=\u001b[39m \u001b[43my_interp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m (max_fpr \u001b[38;5;241m-\u001b[39m x_interp[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     69\u001b[0m         y_interp[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y_interp[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     70\u001b[0m     ) \u001b[38;5;241m/\u001b[39m (x_interp[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x_interp[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     72\u001b[0m tpr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([tpr[:stop], torch\u001b[38;5;241m.\u001b[39mtensor([interp_tpr])])\n\u001b[1;32m     73\u001b[0m fpr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fpr[:stop], torch\u001b[38;5;241m.\u001b[39mtensor([max_fpr])])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "torch.cuda.memory.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"gurunet_model\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/version_{logger.version}\",\n",
    "    filename=\"gurunet-{epoch:02d}-{val_pAUC:.5f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_pAUC\",\n",
    "    mode=\"max\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_pAUC\", patience=15, mode=\"min\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\", log_momentum=True)\n",
    "\n",
    "# Initialize your data module\n",
    "data_module = ISICDataModule(\n",
    "    \"train-image.hdf5\",\n",
    "    \"test-image.hdf5\",\n",
    "    train_metadata_df,\n",
    "    test_metadata_df,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your model\n",
    "model = GuruNet(\n",
    "    input_shape=(139, 139, 3),\n",
    "    metadata_shape=(None, 37),\n",
    "    classes=2,\n",
    ")\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stop_callback,\n",
    "        lr_monitor,\n",
    "    ],\n",
    "    logger=logger,\n",
    "    precision=\"16\",\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=True,\n",
    "    accumulate_grad_batches=6,\n",
    "    profiler=\"simple\",\n",
    "    deterministic=True,\n",
    "    min_epochs=100\n",
    ")\n",
    "skip_training = False\n",
    "if not skip_training:\n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataFrame...\n",
      "Loading cached prepared DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/miniconda3/envs/isic/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "Restoring states from the checkpoint path at /home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_144/gurunet-epoch=44-val_pAUC=0.17860.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Samples: 393\n",
      "Number of Negative Samples: 400666\n",
      "Length of Balanced Positive Indices: 2005\n",
      "Length of Balanced Negative Indices: 2005\n",
      "Unique indices: [   302    305    387 ... 400775 400922 400991]\n",
      "Unique targets: [0 1]\n",
      "4010\n",
      "4010\n",
      "Setup Count: {np.int64(0): np.int64(2005), np.int64(1): np.int64(2005)}\n",
      "Train class distribution: [0.50187032 0.49812968], 3208\n",
      "Validation class distribution: [0.52119701 0.47880299], 401\n",
      "Test class distribution: [0.4638404 0.5361596], 401\n",
      "Length of full_dataset: 401059\n",
      "Length of train_indices: 3208, max index: 400991\n",
      "Length of val_indices: 401, max index: 400922\n",
      "Length of test_indices: 401, max index: 400922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_144/gurunet-epoch=44-val_pAUC=0.17860.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in test_loader: 7\n",
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:02<00:00,  3.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3789622485637665     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_pAUC_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.18669907748699188    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3789622485637665    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_pAUC_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18669907748699188   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST Profiler Report\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                               \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                                \t|  -              \t|  115564         \t|  1742.5         \t|  100 %          \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                                                                                                                                   \t|  27.761         \t|  50             \t|  1388.1         \t|  79.659         \t|\n",
      "|  run_training_batch                                                                                                                                                   \t|  0.41684        \t|  2550           \t|  1062.9         \t|  61.0           \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                              \t|  0.22236        \t|  2550           \t|  567.03         \t|  32.541         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                         \t|  0.18139        \t|  2550           \t|  462.54         \t|  26.545         \t|\n",
      "|  [LightningModule]GuruNet.optimizer_step                                                                                                                              \t|  0.41604        \t|  650            \t|  270.43         \t|  15.519         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                           \t|  0.066024       \t|  2550           \t|  168.36         \t|  9.662          \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                       \t|  0.13827        \t|  352            \t|  48.672         \t|  2.7932         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                                           \t|  0.13145        \t|  352            \t|  46.269         \t|  2.6553         \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.56884        \t|  50             \t|  28.442         \t|  1.6323         \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.49416        \t|  50             \t|  24.708         \t|  1.4179         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                       \t|  0.0019126      \t|  2909           \t|  5.5637         \t|  0.31929        \t|\n",
      "|  [LightningModule]GuruNet.transfer_batch_to_device                                                                                                                    \t|  0.0018416      \t|  2909           \t|  5.3573         \t|  0.30745        \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                         \t|  0.0017849      \t|  2550           \t|  4.5514         \t|  0.2612         \t|\n",
      "|  [LightningDataModule]ISICDataModule.setup                                                                                                                            \t|  0.71337        \t|  3              \t|  2.1401         \t|  0.12282        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.test_step                                                                                                                             \t|  0.29729        \t|  7              \t|  2.0811         \t|  0.11943        \t|\n",
      "|  [_EvaluationLoop].test_next                                                                                                                                          \t|  0.14203        \t|  7              \t|  0.99419        \t|  0.057056       \t|\n",
      "|  [LightningModule]GuruNet.optimizer_zero_grad                                                                                                                         \t|  0.0013407      \t|  650            \t|  0.87144        \t|  0.050011       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                                    \t|  0.0012303      \t|  352            \t|  0.43306        \t|  0.024853       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                        \t|  0.0048444      \t|  51             \t|  0.24707        \t|  0.014179       \t|\n",
      "|  [LightningModule]GuruNet.on_validation_model_eval                                                                                                                    \t|  0.0033624      \t|  51             \t|  0.17148        \t|  0.0098411      \t|\n",
      "|  [LightningModule]GuruNet.on_validation_model_zero_grad                                                                                                               \t|  0.002994       \t|  50             \t|  0.1497         \t|  0.0085912      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                                  \t|  0.00042211     \t|  352            \t|  0.14858        \t|  0.008527       \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  2.3977e-05     \t|  2550           \t|  0.061141       \t|  0.0035088      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                          \t|  0.0011857      \t|  51             \t|  0.06047        \t|  0.0034703      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                       \t|  0.0010283      \t|  50             \t|  0.051414       \t|  0.0029506      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                         \t|  0.00086331     \t|  50             \t|  0.043166       \t|  0.0024772      \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_epoch_end                                                                                     \t|  0.00077954     \t|  50             \t|  0.038977       \t|  0.0022368      \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_batch_start                                                                                                                   \t|  9.1363e-06     \t|  2550           \t|  0.023298       \t|  0.001337       \t|\n",
      "|  [LightningModule]GuruNet.configure_gradient_clipping                                                                                                                 \t|  3.4629e-05     \t|  650            \t|  0.022509       \t|  0.0012918      \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_epoch_start                                                                                                                   \t|  0.00031977     \t|  50             \t|  0.015989       \t|  0.00091756     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  5.7445e-06     \t|  2550           \t|  0.014648       \t|  0.00084066     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_after_backward                                                                                      \t|  5.7065e-06     \t|  2550           \t|  0.014552       \t|  0.0008351      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  3.3802e-06     \t|  2550           \t|  0.0086195      \t|  0.00049466     \t|\n",
      "|  [LightningModule]GuruNet.on_before_batch_transfer                                                                                                                    \t|  2.6833e-06     \t|  2909           \t|  0.0078058      \t|  0.00044797     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_batch_end                                                                                     \t|  3.0147e-06     \t|  2550           \t|  0.0076875      \t|  0.00044117     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start                                                                                                                          \t|  2.9248e-06     \t|  2550           \t|  0.0074583      \t|  0.00042802     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start                                                                                                                             \t|  0.0074439      \t|  1              \t|  0.0074439      \t|  0.0004272      \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end                                                                                                                            \t|  2.8296e-06     \t|  2550           \t|  0.0072156      \t|  0.00041409     \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                                                                                                                                  \t|  0.0067063      \t|  1              \t|  0.0067063      \t|  0.00038487     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_batch_start                                                                                   \t|  2.5514e-06     \t|  2550           \t|  0.0065061      \t|  0.00037338     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_before_backward                                                                                     \t|  2.3743e-06     \t|  2550           \t|  0.0060545      \t|  0.00034746     \t|\n",
      "|  [Callback]LearningRateMonitor.on_after_backward                                                                                                                      \t|  2.1453e-06     \t|  2550           \t|  0.0054704      \t|  0.00031394     \t|\n",
      "|  [LightningModule]GuruNet.on_after_batch_transfer                                                                                                                     \t|  1.6921e-06     \t|  2909           \t|  0.0049225      \t|  0.00028249     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_batch_end                                                                                                                     \t|  1.7917e-06     \t|  2550           \t|  0.0045688      \t|  0.0002622      \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_batch_end                                                                                                                          \t|  0.00056086     \t|  7              \t|  0.003926       \t|  0.00022531     \t|\n",
      "|  [LightningModule]GuruNet.on_train_batch_start                                                                                                                        \t|  1.4696e-06     \t|  2550           \t|  0.0037476      \t|  0.00021507     \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                          \t|  1.4578e-06     \t|  2550           \t|  0.0037175      \t|  0.00021334     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                                  \t|  1.3777e-06     \t|  2550           \t|  0.0035132      \t|  0.00020162     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  1.2128e-06     \t|  2550           \t|  0.0030927      \t|  0.00017749     \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_batch_start                                                                                                                        \t|  0.00042716     \t|  7              \t|  0.0029901      \t|  0.0001716      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                       \t|  1.1357e-06     \t|  2550           \t|  0.0028961      \t|  0.0001662      \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_start                                                                                                                              \t|  0.0028508      \t|  1              \t|  0.0028508      \t|  0.0001636      \t|\n",
      "|  [LightningModule]GuruNet.on_test_model_eval                                                                                                                          \t|  0.0027272      \t|  1              \t|  0.0027272      \t|  0.00015651     \t|\n",
      "|  [LightningModule]GuruNet.on_train_batch_end                                                                                                                          \t|  1.064e-06      \t|  2550           \t|  0.0027133      \t|  0.00015571     \t|\n",
      "|  [LightningModule]GuruNet.on_after_backward                                                                                                                           \t|  1.0055e-06     \t|  2550           \t|  0.0025641      \t|  0.00014715     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_before_zero_grad                                                                                    \t|  3.9133e-06     \t|  650            \t|  0.0025436      \t|  0.00014597     \t|\n",
      "|  [LightningModule]GuruNet.on_before_backward                                                                                                                          \t|  9.3499e-07     \t|  2550           \t|  0.0023842      \t|  0.00013683     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  4.5981e-05     \t|  51             \t|  0.002345       \t|  0.00013458     \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_backward                                                                                                                     \t|  9.0538e-07     \t|  2550           \t|  0.0023087      \t|  0.00013249     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  8.6673e-07     \t|  2550           \t|  0.0022102      \t|  0.00012684     \t|\n",
      "|  [LightningModule]GuruNet.lr_scheduler_step                                                                                                                           \t|  4.4069e-05     \t|  50             \t|  0.0022034      \t|  0.00012645     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  8.4475e-07     \t|  2550           \t|  0.0021541      \t|  0.00012362     \t|\n",
      "|  [Callback]ModelSummary.on_after_backward                                                                                                                             \t|  7.7301e-07     \t|  2550           \t|  0.0019712      \t|  0.00011312     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  7.5302e-07     \t|  2550           \t|  0.0019202      \t|  0.0001102      \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_before_optimizer_step                                                                               \t|  2.7883e-06     \t|  650            \t|  0.0018124      \t|  0.00010401     \t|\n",
      "|  [Callback]ModelSummary.on_before_backward                                                                                                                            \t|  7.1061e-07     \t|  2550           \t|  0.001812       \t|  0.00010399     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  6.9153e-07     \t|  2550           \t|  0.0017634      \t|  0.0001012      \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                         \t|  6.4606e-07     \t|  2550           \t|  0.0016475      \t|  9.4546e-05     \t|\n",
      "|  [LightningModule]GuruNet.configure_optimizers                                                                                                                        \t|  0.0016219      \t|  1              \t|  0.0016219      \t|  9.3079e-05     \t|\n",
      "|  [LightningModule]GuruNet.on_before_zero_grad                                                                                                                         \t|  2.3876e-06     \t|  650            \t|  0.0015519      \t|  8.9062e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                      \t|  0.0012032      \t|  1              \t|  0.0012032      \t|  6.9047e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                       \t|  2.2478e-06     \t|  352            \t|  0.00079123     \t|  4.5408e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_batch_end                                                                                \t|  2.2432e-06     \t|  352            \t|  0.00078962     \t|  4.5315e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  1.0773e-06     \t|  650            \t|  0.00070025     \t|  4.0186e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_start                                                                                                                         \t|  0.00065591     \t|  1              \t|  0.00065591     \t|  3.7642e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                      \t|  9.5768e-07     \t|  650            \t|  0.00062249     \t|  3.5724e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_zero_grad                                                                                                                    \t|  9.4e-07        \t|  650            \t|  0.000611       \t|  3.5065e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_start                                                                                    \t|  1.1921e-05     \t|  51             \t|  0.00060799     \t|  3.4892e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_batch_start                                                                              \t|  1.5357e-06     \t|  352            \t|  0.00054057     \t|  3.1022e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_optimizer_step                                                                                                               \t|  8.214e-07      \t|  650            \t|  0.00053391     \t|  3.064e-05      \t|\n",
      "|  [LightningModule]GuruNet.on_before_optimizer_step                                                                                                                    \t|  7.757e-07      \t|  650            \t|  0.00050421     \t|  2.8936e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  6.5952e-07     \t|  650            \t|  0.00042869     \t|  2.4602e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                        \t|  6.3357e-07     \t|  650            \t|  0.00041182     \t|  2.3634e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_start                                                                                                                     \t|  1.1594e-06     \t|  352            \t|  0.00040812     \t|  2.3422e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_epoch_start                                                                              \t|  7.5625e-06     \t|  51             \t|  0.00038569     \t|  2.2134e-05     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_batch_end                                                                                                                     \t|  1.0941e-06     \t|  352            \t|  0.00038511     \t|  2.2101e-05     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_batch_start                                                                                                                   \t|  1.0608e-06     \t|  352            \t|  0.0003734      \t|  2.1429e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                           \t|  5.7313e-07     \t|  650            \t|  0.00037254     \t|  2.1379e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end                                                                                                                               \t|  0.00035273     \t|  1              \t|  0.00035273     \t|  2.0243e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                                   \t|  5.2952e-07     \t|  650            \t|  0.00034419     \t|  1.9752e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  5.2197e-07     \t|  650            \t|  0.00033928     \t|  1.9471e-05     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                                   \t|  6.3748e-06     \t|  51             \t|  0.00032512     \t|  1.8658e-05     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_epoch_start                                                                                                                   \t|  6.3615e-06     \t|  51             \t|  0.00032443     \t|  1.8619e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_end                                                                                                                                \t|  0.00031885     \t|  1              \t|  0.00031885     \t|  1.8299e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  4.7444e-07     \t|  650            \t|  0.00030838     \t|  1.7698e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  8.4059e-07     \t|  352            \t|  0.00029589     \t|  1.6981e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_batch_end                                                                                                                \t|  7.9808e-07     \t|  352            \t|  0.00028092     \t|  1.6122e-05     \t|\n",
      "|  [LightningModule]GuruNet.on_train_start                                                                                                                              \t|  0.0002734      \t|  1              \t|  0.0002734      \t|  1.569e-05      \t|\n",
      "|  [LightningDataModule]ISICDataModule.test_dataloader                                                                                                                  \t|  0.00026746     \t|  1              \t|  0.00026746     \t|  1.5349e-05     \t|\n",
      "|  [LightningDataModule]ISICDataModule.val_dataloader                                                                                                                   \t|  0.00026081     \t|  1              \t|  0.00026081     \t|  1.4968e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  5.0354e-06     \t|  51             \t|  0.0002568      \t|  1.4738e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  7.0405e-07     \t|  352            \t|  0.00024782     \t|  1.4222e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  6.691e-07      \t|  352            \t|  0.00023552     \t|  1.3516e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_end                                                                                      \t|  4.5081e-06     \t|  51             \t|  0.00022992     \t|  1.3195e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  7.6281e-05     \t|  3              \t|  0.00022884     \t|  1.3133e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_batch_start                                                                                                              \t|  6.4091e-07     \t|  352            \t|  0.0002256      \t|  1.2947e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_start                                                                                                                           \t|  4.3944e-06     \t|  51             \t|  0.00022412     \t|  1.2862e-05     \t|\n",
      "|  [LightningDataModule]ISICDataModule.train_dataloader                                                                                                                 \t|  0.00021635     \t|  1              \t|  0.00021635     \t|  1.2416e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  5.6772e-07     \t|  352            \t|  0.00019984     \t|  1.1468e-05     \t|\n",
      "|  [LightningDataModule]ISICDataModule.state_dict                                                                                                                       \t|  2.77e-06       \t|  53             \t|  0.00014681     \t|  8.4254e-06     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_start                                                                                         \t|  0.00014028     \t|  1              \t|  0.00014028     \t|  8.0507e-06     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_epoch_start                                                                                   \t|  2.1796e-06     \t|  50             \t|  0.00010898     \t|  6.2543e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                          \t|  1.7298e-06     \t|  50             \t|  8.6491e-05     \t|  4.9636e-06     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_validation_epoch_end                                                                                \t|  1.4469e-06     \t|  51             \t|  7.3792e-05     \t|  4.2348e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_end                                                                                                                             \t|  1.4071e-06     \t|  51             \t|  7.1761e-05     \t|  4.1183e-06     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_epoch_end                                                                                                                     \t|  1.4256e-06     \t|  50             \t|  7.1281e-05     \t|  4.0907e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                            \t|  1.2902e-06     \t|  50             \t|  6.451e-05      \t|  3.7021e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_start                                                                                                                         \t|  1.2131e-06     \t|  51             \t|  6.187e-05      \t|  3.5506e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_train_epoch_start                                                                                                                        \t|  1.183e-06      \t|  50             \t|  5.9152e-05     \t|  3.3947e-06     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_save_checkpoint                                                                                     \t|  1.0985e-06     \t|  53             \t|  5.8222e-05     \t|  3.3413e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  1.0716e-06     \t|  51             \t|  5.4652e-05     \t|  3.1364e-06     \t|\n",
      "|  [Callback]LearningRateMonitor.on_save_checkpoint                                                                                                                     \t|  1.0108e-06     \t|  53             \t|  5.3572e-05     \t|  3.0744e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  1.7367e-05     \t|  3              \t|  5.21e-05       \t|  2.99e-06       \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_end                                                                                                                      \t|  9.4529e-07     \t|  51             \t|  4.821e-05      \t|  2.7667e-06     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_epoch_start                                                                                                              \t|  9.1669e-07     \t|  51             \t|  4.6751e-05     \t|  2.683e-06      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  8.1353e-07     \t|  51             \t|  4.149e-05      \t|  2.3811e-06     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                                     \t|  8.1275e-07     \t|  51             \t|  4.145e-05      \t|  2.3788e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_end                                                                                                                           \t|  7.6298e-07     \t|  51             \t|  3.8912e-05     \t|  2.2331e-06     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_start                                                                                                                    \t|  7.5004e-07     \t|  51             \t|  3.8252e-05     \t|  2.1952e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_validation_epoch_end                                                                                                                     \t|  7.2178e-07     \t|  51             \t|  3.6811e-05     \t|  2.1125e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                         \t|  6.9302e-07     \t|  53             \t|  3.673e-05      \t|  2.1079e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_train_epoch_end                                                                                                                          \t|  7.114e-07      \t|  50             \t|  3.557e-05      \t|  2.0413e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  6.6451e-07     \t|  51             \t|  3.389e-05      \t|  1.9449e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  6.49e-07       \t|  50             \t|  3.245e-05      \t|  1.8623e-06     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_epoch_end                                                                                                                \t|  6.0904e-07     \t|  51             \t|  3.1061e-05     \t|  1.7826e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                                  \t|  5.9865e-07     \t|  51             \t|  3.0531e-05     \t|  1.7521e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  5.8218e-07     \t|  51             \t|  2.9691e-05     \t|  1.7039e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  2.9221e-05     \t|  1              \t|  2.9221e-05     \t|  1.677e-06      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  5.7682e-07     \t|  50             \t|  2.8841e-05     \t|  1.6551e-06     \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                            \t|  5.3453e-07     \t|  53             \t|  2.833e-05      \t|  1.6258e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  5.451e-07      \t|  51             \t|  2.78e-05       \t|  1.5954e-06     \t|\n",
      "|  [LightningModule]GuruNet.on_save_checkpoint                                                                                                                          \t|  4.906e-07      \t|  53             \t|  2.6002e-05     \t|  1.4922e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  4.6434e-07     \t|  53             \t|  2.461e-05      \t|  1.4123e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                                     \t|  4.7961e-07     \t|  51             \t|  2.446e-05      \t|  1.4037e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                                    \t|  4.7825e-07     \t|  51             \t|  2.4391e-05     \t|  1.3998e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  4.6335e-07     \t|  51             \t|  2.3631e-05     \t|  1.3562e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                       \t|  4.1886e-07     \t|  51             \t|  2.1362e-05     \t|  1.2259e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  3.2094e-07     \t|  53             \t|  1.701e-05      \t|  9.7618e-07     \t|\n",
      "|  [Callback]ModelSummary.on_train_start                                                                                                                                \t|  1.697e-05      \t|  1              \t|  1.697e-05      \t|  9.7389e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_batch_end                                                                                      \t|  1.9657e-06     \t|  7              \t|  1.376e-05      \t|  7.8967e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                                      \t|  4.5733e-06     \t|  3              \t|  1.372e-05      \t|  7.8737e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_batch_start                                                                                    \t|  1.7886e-06     \t|  7              \t|  1.252e-05      \t|  7.1851e-07     \t|\n",
      "|  [Callback]ModelSummary.on_test_batch_end                                                                                                                             \t|  1.4143e-06     \t|  7              \t|  9.9e-06        \t|  5.6815e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.setup                                                                                                  \t|  2.8767e-06     \t|  3              \t|  8.63e-06       \t|  4.9527e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                        \t|  8.28e-06       \t|  1              \t|  8.28e-06       \t|  4.7518e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  8.07e-06       \t|  1              \t|  8.07e-06       \t|  4.6313e-07     \t|\n",
      "|  [LightningModule]GuruNet.on_test_batch_start                                                                                                                         \t|  1.0486e-06     \t|  7              \t|  7.34e-06       \t|  4.2123e-07     \t|\n",
      "|  [LightningModule]GuruNet.on_test_batch_end                                                                                                                           \t|  9.5286e-07     \t|  7              \t|  6.67e-06       \t|  3.8278e-07     \t|\n",
      "|  [Callback]ModelSummary.on_test_batch_start                                                                                                                           \t|  8.9143e-07     \t|  7              \t|  6.24e-06       \t|  3.5811e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_test_start                                                                                                                         \t|  6.14e-06       \t|  1              \t|  6.14e-06       \t|  3.5237e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                        \t|  5.851e-06      \t|  1              \t|  5.851e-06      \t|  3.3578e-07     \t|\n",
      "|  [LightningModule]GuruNet.on_test_epoch_start                                                                                                                         \t|  5.79e-06       \t|  1              \t|  5.79e-06       \t|  3.3228e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_epoch_start                                                                                    \t|  5.68e-06       \t|  1              \t|  5.68e-06       \t|  3.2597e-07     \t|\n",
      "|  [LightningModule]GuruNet.configure_callbacks                                                                                                                         \t|  1.7667e-06     \t|  3              \t|  5.3e-06        \t|  3.0416e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_sanity_check_end                                                                                    \t|  5.26e-06       \t|  1              \t|  5.26e-06       \t|  3.0186e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_batch_end        \t|  7.3e-07        \t|  7              \t|  5.11e-06       \t|  2.9326e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_start                                                                                          \t|  5.06e-06       \t|  1              \t|  5.06e-06       \t|  2.9039e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.teardown                                                                                               \t|  2.48e-06       \t|  2              \t|  4.96e-06       \t|  2.8465e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_batch_end                                                                                                                      \t|  7.0714e-07     \t|  7              \t|  4.95e-06       \t|  2.8407e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_batch_start                                                                                                                    \t|  7.0143e-07     \t|  7              \t|  4.91e-06       \t|  2.8178e-07     \t|\n",
      "|  [LightningModule]GuruNet.on_load_checkpoint                                                                                                                          \t|  2.2905e-06     \t|  2              \t|  4.581e-06      \t|  2.629e-07      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_batch_start      \t|  6.5e-07        \t|  7              \t|  4.55e-06       \t|  2.6112e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_fit_end                                                                                             \t|  4.25e-06       \t|  1              \t|  4.25e-06       \t|  2.439e-07      \t|\n",
      "|  [Callback]LearningRateMonitor.setup                                                                                                                                  \t|  1.32e-06       \t|  3              \t|  3.96e-06       \t|  2.2726e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_exception                                                                                           \t|  3.83e-06       \t|  1              \t|  3.83e-06       \t|  2.198e-07      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_batch_end        \t|  5.3857e-07     \t|  7              \t|  3.77e-06       \t|  2.1636e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_batch_start      \t|  5.2571e-07     \t|  7              \t|  3.68e-06       \t|  2.1119e-07     \t|\n",
      "|  [LightningModule]GuruNet.setup                                                                                                                                       \t|  1.15e-06       \t|  3              \t|  3.45e-06       \t|  1.9799e-07     \t|\n",
      "|  [LightningDataModule]ISICDataModule.teardown                                                                                                                         \t|  1.505e-06      \t|  2              \t|  3.01e-06       \t|  1.7274e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_train_end                                                                                           \t|  2.47e-06       \t|  1              \t|  2.47e-06       \t|  1.4175e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_end                                                                                            \t|  2.18e-06       \t|  1              \t|  2.18e-06       \t|  1.2511e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_fit_start                                                                                           \t|  2.06e-06       \t|  1              \t|  2.06e-06       \t|  1.1822e-07     \t|\n",
      "|  [Callback]ModelSummary.on_test_start                                                                                                                                 \t|  2.04e-06       \t|  1              \t|  2.04e-06       \t|  1.1707e-07     \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                                         \t|  6.4667e-07     \t|  3              \t|  1.94e-06       \t|  1.1133e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_sanity_check_start                                                                                  \t|  1.8e-06        \t|  1              \t|  1.8e-06        \t|  1.033e-07      \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'val_pAUC', 'mode': 'min'}.on_test_epoch_end                                                                                      \t|  1.73e-06       \t|  1              \t|  1.73e-06       \t|  9.9283e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_test_start                                                                                                                               \t|  1.67e-06       \t|  1              \t|  1.67e-06       \t|  9.5839e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  1.6e-06        \t|  1              \t|  1.6e-06        \t|  9.1822e-08     \t|\n",
      "|  [Callback]ModelSummary.on_train_end                                                                                                                                  \t|  1.54e-06       \t|  1              \t|  1.54e-06       \t|  8.8379e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.teardown                                                                                                                               \t|  7.6e-07        \t|  2              \t|  1.52e-06       \t|  8.7231e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_train_end                                                                                                                                \t|  1.32e-06       \t|  1              \t|  1.32e-06       \t|  7.5753e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_fit_end                                                                                                                                  \t|  1.29e-06       \t|  1              \t|  1.29e-06       \t|  7.4032e-08     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                         \t|  1.18e-06       \t|  1              \t|  1.18e-06       \t|  6.7719e-08     \t|\n",
      "|  [LightningModule]GuruNet.teardown                                                                                                                                    \t|  5.6e-07        \t|  2              \t|  1.12e-06       \t|  6.4275e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_test_end                                                                                                                                 \t|  1.12e-06       \t|  1              \t|  1.12e-06       \t|  6.4275e-08     \t|\n",
      "|  [LightningDataModule]ISICDataModule.on_exception                                                                                                                     \t|  1.1e-06        \t|  1              \t|  1.1e-06        \t|  6.3128e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_test_epoch_end                                                                                                                           \t|  1.07e-06       \t|  1              \t|  1.07e-06       \t|  6.1406e-08     \t|\n",
      "|  [Callback]ModelSummary.on_test_end                                                                                                                                   \t|  1.06e-06       \t|  1              \t|  1.06e-06       \t|  6.0832e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  5e-07          \t|  2              \t|  1e-06          \t|  5.7389e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                                                                                                                                   \t|  5e-07          \t|  2              \t|  1e-06          \t|  5.7389e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  4.9e-07        \t|  2              \t|  9.8e-07        \t|  5.6241e-08     \t|\n",
      "|  [LightningModule]GuruNet.on_fit_start                                                                                                                                \t|  9.2e-07        \t|  1              \t|  9.2e-07        \t|  5.2798e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_exception                                                                                                                               \t|  8.9e-07        \t|  1              \t|  8.9e-07        \t|  5.1076e-08     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                          \t|  8.6e-07        \t|  1              \t|  8.6e-07        \t|  4.9354e-08     \t|\n",
      "|  [Callback]ModelSummary.teardown                                                                                                                                      \t|  4.25e-07       \t|  2              \t|  8.5e-07        \t|  4.878e-08      \t|\n",
      "|  [Callback]LearningRateMonitor.on_exception                                                                                                                           \t|  8.3e-07        \t|  1              \t|  8.3e-07        \t|  4.7633e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_end                                                                                                                           \t|  8e-07          \t|  1              \t|  8e-07          \t|  4.5911e-08     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_test_end                                                                                                                           \t|  7.9e-07        \t|  1              \t|  7.9e-07        \t|  4.5337e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_start            \t|  6.9e-07        \t|  1              \t|  6.9e-07        \t|  3.9598e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_start                                                                                                                          \t|  6.5e-07        \t|  1              \t|  6.5e-07        \t|  3.7303e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  6.5e-07        \t|  1              \t|  6.5e-07        \t|  3.7303e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_fit_start                                                                                                                           \t|  5.9e-07        \t|  1              \t|  5.9e-07        \t|  3.3859e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_sanity_check_start                                                                                                                  \t|  5.9e-07        \t|  1              \t|  5.9e-07        \t|  3.3859e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_epoch_end                                                                                                                      \t|  5.8e-07        \t|  1              \t|  5.8e-07        \t|  3.3285e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_epoch_end                                                                                                                          \t|  5.8e-07        \t|  1              \t|  5.8e-07        \t|  3.3285e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_exception             \t|  5.8e-07        \t|  1              \t|  5.8e-07        \t|  3.3285e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  5.7e-07        \t|  1              \t|  5.7e-07        \t|  3.2712e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_fit_end                                                                                                                             \t|  5.7e-07        \t|  1              \t|  5.7e-07        \t|  3.2712e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_epoch_start                                                                                                                    \t|  5.6e-07        \t|  1              \t|  5.6e-07        \t|  3.2138e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_end              \t|  5.6e-07        \t|  1              \t|  5.6e-07        \t|  3.2138e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                                 \t|  5.5e-07        \t|  1              \t|  5.5e-07        \t|  3.1564e-08     \t|\n",
      "|  [Callback]ModelSummary.on_exception                                                                                                                                  \t|  5.5e-07        \t|  1              \t|  5.5e-07        \t|  3.1564e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_test_end                                                                                                                            \t|  5.4e-07        \t|  1              \t|  5.4e-07        \t|  3.099e-08      \t|\n",
      "|  [Callback]LearningRateMonitor.on_sanity_check_end                                                                                                                    \t|  5.4e-07        \t|  1              \t|  5.4e-07        \t|  3.099e-08      \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                           \t|  5.3e-07        \t|  1              \t|  5.3e-07        \t|  3.0416e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  5.3e-07        \t|  1              \t|  5.3e-07        \t|  3.0416e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  5.2e-07        \t|  1              \t|  5.2e-07        \t|  2.9842e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  5e-07          \t|  1              \t|  5e-07          \t|  2.8694e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_epoch_start      \t|  5e-07          \t|  1              \t|  5e-07          \t|  2.8694e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_epoch_start      \t|  5e-07          \t|  1              \t|  5e-07          \t|  2.8694e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_exception             \t|  4.9e-07        \t|  1              \t|  4.9e-07        \t|  2.8121e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_end              \t|  4.9e-07        \t|  1              \t|  4.9e-07        \t|  2.812e-08      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_start            \t|  4.6e-07        \t|  1              \t|  4.6e-07        \t|  2.6399e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                               \t|  4.5e-07        \t|  1              \t|  4.5e-07        \t|  2.5825e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  4.5e-07        \t|  1              \t|  4.5e-07        \t|  2.5825e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  4.3e-07        \t|  1              \t|  4.3e-07        \t|  2.4677e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  4.3e-07        \t|  1              \t|  4.3e-07        \t|  2.4677e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_epoch_end        \t|  4.1e-07        \t|  1              \t|  4.1e-07        \t|  2.3529e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_test_epoch_end        \t|  4.1e-07        \t|  1              \t|  4.1e-07        \t|  2.3529e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'val_pAUC', 'mode': 'max', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  4.1e-07        \t|  1              \t|  4.1e-07        \t|  2.3529e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_test_epoch_start                                                                                                                        \t|  4.1e-07        \t|  1              \t|  4.1e-07        \t|  2.3529e-08     \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                                                                                                                                    \t|  3.9e-07        \t|  1              \t|  3.9e-07        \t|  2.2382e-08     \t|\n",
      "|  [Callback]ModelSummary.on_test_epoch_start                                                                                                                           \t|  3.9e-07        \t|  1              \t|  3.9e-07        \t|  2.2382e-08     \t|\n",
      "|  [Callback]ModelSummary.on_test_epoch_end                                                                                                                             \t|  3.9e-07        \t|  1              \t|  3.9e-07        \t|  2.2382e-08     \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.3789622485637665,\n",
       "  'test_pAUC_epoch': 0.18669907748699188}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.test(\n",
    "#     model=model,\n",
    "#     ckpt_path=\"/home/pupperemeritus/DL/isic-2024-challenge/checkpoints/version_144/gurunet-epoch=44-val_pAUC=0.17860.ckpt\",\n",
    "#     datamodule=data_module,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
